{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTM350 Final Project\n",
    "Xinyue (Cindy) Zhang, Jiayin Li, Honggang (Peter) Min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Lighting Talk: https://drive.google.com/file/d/148A_IiZdJEtvBMISvdI50mLJ9ClsUh99/view?usp=sharing\n",
    "\n",
    "##  ML Problem Framing\n",
    "Explains a thorough, appropriate, and comprehensive definition of the problem in a machine learning context.\n",
    "Incentives:\n",
    "As a New York Yellow Taxi Driver, how can you maximize the tip you get?\n",
    "\n",
    "It is not possible for you to decide 'where the consutomer to go', 'how far the ride is', or 'the number passengers you will pick up next'. However, you do have the right to pick what time you want your shift to be and what day of the week you want to work.\n",
    "\n",
    "Based on this idea, we created two new binary attributes with the AWS open dataset of NYC Taxi: 'isNight' and 'isWeekend'. \n",
    "\n",
    "For the 'isNight' attribute, we classified shift time into \"day\" and \"night\".\n",
    "For the 'isWeekend' attribute, we we classified shift day of the week into \"weekend\" and \"weekday\".\n",
    "Both variables will be futher expained later.\n",
    "\n",
    "To make it clear, we define the dependent variable to be whether \"the driver’s tip is higher(equal)or lower than the monthly tip average\".\n",
    "If the trip's tip is higher than monthly tip, then Y = 1, else, Y = 0.\n",
    "\n",
    "\n",
    "Goal: predict the mean for tip of one shift base on time(day/night & weekend/weekday). Classified those who earn above the average 1, otherwise 0.\n",
    "\n",
    "Final Objective: An analysis of how yellow taxi drivers can maximize their total payout (tip amount) based on the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<botocore.client.S3 object at 0x7fdfcae5a128>\n"
     ]
    }
   ],
   "source": [
    "# Setup Notebook Environment\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "print(s3)\n",
    "s3_resource = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA\n",
    "\n",
    "Our data is about the trips taken by yellow taxis in New York City. Official data info is on the AWS open data page: https://registry.opendata.aws/nyc-tlc-trip-records-pds/\n",
    "We get the data by downloading the dataset from official AWS open data set's s3 bucket to our own s3 bucket follow the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Created or already exists open-data-analytics-taxi-trips-zyl bucket.'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat Bucket\n",
    "\n",
    "def create_bucket(bucket):\n",
    "    import logging\n",
    "\n",
    "    try:\n",
    "        s3.create_bucket(Bucket=bucket)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        logging.error(e)\n",
    "        return 'Bucket ' + bucket + ' could not be created.'\n",
    "    return 'Created or already exists ' + bucket + ' bucket.'\n",
    "\n",
    "create_bucket('open-data-analytics-taxi-trips-zyl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing buckets containing \"open\" string:\n",
      "  open-data-analytics-taxi-trips-zyl\n"
     ]
    }
   ],
   "source": [
    "# List Bucket\n",
    "\n",
    "def list_buckets(match=''):\n",
    "    response = s3.list_buckets()\n",
    "    if match:\n",
    "        print(f'Existing buckets containing \"{match}\" string:')\n",
    "    else:\n",
    "        print('All existing buckets:')\n",
    "    for bucket in response['Buckets']:\n",
    "        if match:\n",
    "            if match in bucket[\"Name\"]:\n",
    "                print(f'  {bucket[\"Name\"]}')\n",
    "                      \n",
    "list_buckets(match='open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip data/green_tripdata_2018-01.csv ( 68MB)\n",
      "trip data/green_tripdata_2018-02.csv ( 66MB)\n",
      "trip data/green_tripdata_2018-03.csv ( 71MB)\n",
      "trip data/green_tripdata_2018-04.csv ( 68MB)\n",
      "trip data/green_tripdata_2018-05.csv ( 68MB)\n",
      "trip data/green_tripdata_2018-06.csv ( 63MB)\n",
      "trip data/green_tripdata_2018-07.csv ( 58MB)\n",
      "trip data/green_tripdata_2018-08.csv ( 57MB)\n",
      "trip data/green_tripdata_2018-09.csv ( 57MB)\n",
      "trip data/green_tripdata_2018-10.csv ( 61MB)\n",
      "trip data/green_tripdata_2018-11.csv ( 56MB)\n",
      "trip data/green_tripdata_2018-12.csv ( 59MB)\n",
      "Matched file size is 0.7GB with 12 files\n",
      "Bucket nyc-tlc total size is 285.5GB with 274 files\n"
     ]
    }
   ],
   "source": [
    "# List Bucket Contents\n",
    "\n",
    "def list_bucket_contents(bucket, match='', size_mb=0):\n",
    "    bucket_resource = s3_resource.Bucket(bucket)\n",
    "    total_size_gb = 0\n",
    "    total_files = 0\n",
    "    match_size_gb = 0\n",
    "    match_files = 0\n",
    "    for key in bucket_resource.objects.all():\n",
    "        key_size_mb = key.size/1024/1024\n",
    "        total_size_gb += key_size_mb\n",
    "        total_files += 1\n",
    "        list_check = False\n",
    "        if not match:\n",
    "            list_check = True\n",
    "        elif match in key.key:\n",
    "            list_check = True\n",
    "        if list_check and not size_mb:\n",
    "            match_files += 1\n",
    "            match_size_gb += key_size_mb\n",
    "            print(f'{key.key} ({key_size_mb:3.0f}MB)')\n",
    "        elif list_check and key_size_mb <= size_mb:\n",
    "            match_files += 1\n",
    "            match_size_gb += key_size_mb\n",
    "            print(f'{key.key} ({key_size_mb:3.0f}MB)')\n",
    "\n",
    "    if match:\n",
    "        print(f'Matched file size is {match_size_gb/1024:3.1f}GB with {match_files} files')            \n",
    "    \n",
    "    print(f'Bucket {bucket} total size is {total_size_gb/1024:3.1f}GB with {total_files} files')\n",
    "  \n",
    "    \n",
    "list_bucket_contents(bucket='nyc-tlc', match='2018', size_mb=250)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2018-01.csv already exists in S3 bucket fp-zyl\n",
      "File yellow_tripdata_2018-02.csv already exists in S3 bucket fp-zyl\n",
      "File yellow_tripdata_2018-03.csv already exists in S3 bucket fp-zyl\n",
      "File yellow_tripdata_2018-04.csv already exists in S3 bucket fp-zyl\n",
      "File yellow_tripdata_2018-05.csv already exists in S3 bucket fp-zyl\n",
      "File yellow_tripdata_2018-06.csv already exists in S3 bucket fp-zyl\n",
      "File yellow_tripdata_2018-07.csv already exists in S3 bucket fp-zyl\n",
      "File yellow_tripdata_2018-08.csv already exists in S3 bucket fp-zyl\n",
      "File yellow_tripdata_2018-09.csv already exists in S3 bucket fp-zyl\n",
      "File yellow_tripdata_2018-10.csv already exists in S3 bucket fp-zyl\n",
      "File yellow_tripdata_2018-11.csv already exists in S3 bucket fp-zyl\n",
      "File yellow_tripdata_2018-12.csv already exists in S3 bucket fp-zyl\n"
     ]
    }
   ],
   "source": [
    "#  Copy Among Buckets\n",
    "\n",
    "def key_exists(bucket, key):\n",
    "    try:\n",
    "        s3_resource.Object(bucket, key).load()\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            # The key does not exist.\n",
    "            return(False)\n",
    "        else:\n",
    "            # Something else has gone wrong.\n",
    "            raise\n",
    "    else:\n",
    "        # The key does exist.\n",
    "        return(True)\n",
    "\n",
    "def copy_among_buckets(from_bucket, from_key, to_bucket, to_key):\n",
    "    if not key_exists(to_bucket, to_key):\n",
    "        s3_resource.meta.client.copy({'Bucket': from_bucket, 'Key': from_key}, \n",
    "                                        to_bucket, to_key)        \n",
    "        print(f'File {to_key} saved to S3 bucket {to_bucket}')\n",
    "    else:\n",
    "        print(f'File {to_key} already exists in S3 bucket {to_bucket}') \n",
    "\n",
    "        \n",
    "for i in range(1,13): \n",
    "    if i<10:\n",
    "        fromkey='trip data/yellow_tripdata_2018-0'+str(i)+'.csv'\n",
    "        tokey='yellow_tripdata_2018-0'+str(i)+'.csv'\n",
    "    else:\n",
    "        fromkey='trip data/yellow_tripdata_2018-'+str(i)+'.csv'\n",
    "        tokey='yellow_tripdata_2018-'+str(i)+'.csv'\n",
    "    copy_among_buckets(from_bucket='nyc-tlc', from_key=fromkey,\n",
    "                      to_bucket='fp-zyl', to_key=tokey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 17 columns):\n",
      "VendorID                 100 non-null int64\n",
      "tpep_pickup_datetime     100 non-null object\n",
      "tpep_dropoff_datetime    100 non-null object\n",
      "passenger_count          100 non-null int64\n",
      "trip_distance            100 non-null float64\n",
      "RatecodeID               100 non-null int64\n",
      "store_and_fwd_flag       100 non-null object\n",
      "PULocationID             100 non-null int64\n",
      "DOLocationID             100 non-null int64\n",
      "payment_type             100 non-null int64\n",
      "fare_amount              100 non-null float64\n",
      "extra                    100 non-null float64\n",
      "mta_tax                  100 non-null float64\n",
      "tip_amount               100 non-null float64\n",
      "tolls_amount             100 non-null float64\n",
      "improvement_surcharge    100 non-null float64\n",
      "total_amount             100 non-null float64\n",
      "dtypes: float64(8), int64(6), object(3)\n",
      "memory usage: 13.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.420000</td>\n",
       "      <td>1.78000</td>\n",
       "      <td>2.335000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>169.310000</td>\n",
       "      <td>163.510000</td>\n",
       "      <td>1.48000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.333100</td>\n",
       "      <td>0.0576</td>\n",
       "      <td>0.294</td>\n",
       "      <td>13.769700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496045</td>\n",
       "      <td>1.33772</td>\n",
       "      <td>2.209747</td>\n",
       "      <td>0.10</td>\n",
       "      <td>73.621156</td>\n",
       "      <td>74.873698</td>\n",
       "      <td>0.55922</td>\n",
       "      <td>8.320165</td>\n",
       "      <td>0.111351</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.687668</td>\n",
       "      <td>0.5760</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9.144411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-4.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>1.00</td>\n",
       "      <td>140.750000</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>7.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>11.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.087500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>17.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>264.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.080000</td>\n",
       "      <td>5.7600</td>\n",
       "      <td>0.300</td>\n",
       "      <td>52.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         VendorID  passenger_count  trip_distance  RatecodeID  PULocationID  \\\n",
       "count  100.000000        100.00000     100.000000      100.00    100.000000   \n",
       "mean     1.420000          1.78000       2.335000        1.01    169.310000   \n",
       "std      0.496045          1.33772       2.209747        0.10     73.621156   \n",
       "min      1.000000          1.00000       0.000000        1.00      4.000000   \n",
       "25%      1.000000          1.00000       0.797500        1.00    140.750000   \n",
       "50%      1.000000          1.00000       1.650000        1.00    164.000000   \n",
       "75%      2.000000          2.00000       3.000000        1.00    236.000000   \n",
       "max      2.000000          6.00000      10.900000        2.00    263.000000   \n",
       "\n",
       "       DOLocationID  payment_type  fare_amount       extra  mta_tax  \\\n",
       "count    100.000000     100.00000   100.000000  100.000000   100.00   \n",
       "mean     163.510000       1.48000    11.110000    0.485000     0.49   \n",
       "std       74.873698       0.55922     8.320165    0.111351     0.10   \n",
       "min        4.000000       1.00000    -3.000000   -0.500000    -0.50   \n",
       "25%       97.500000       1.00000     5.500000    0.500000     0.50   \n",
       "50%      164.000000       1.00000     8.750000    0.500000     0.50   \n",
       "75%      234.000000       2.00000    14.500000    0.500000     0.50   \n",
       "max      264.000000       4.00000    52.000000    0.500000     0.50   \n",
       "\n",
       "       tip_amount  tolls_amount  improvement_surcharge  total_amount  \n",
       "count  100.000000      100.0000                100.000    100.000000  \n",
       "mean     1.333100        0.0576                  0.294     13.769700  \n",
       "std      1.687668        0.5760                  0.060      9.144411  \n",
       "min      0.000000        0.0000                 -0.300     -4.300000  \n",
       "25%      0.000000        0.0000                  0.300      7.450000  \n",
       "50%      1.000000        0.0000                  0.300     11.230000  \n",
       "75%      2.087500        0.0000                  0.300     17.152500  \n",
       "max      9.080000        5.7600                  0.300     52.800000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview CSV Dataset\n",
    "\n",
    "def preview_csv_dataset(bucket, key, rows=10):\n",
    "    data_source = {\n",
    "            'Bucket': bucket,\n",
    "            'Key': key\n",
    "        }\n",
    "    # Generate the URL to get Key from Bucket\n",
    "    url = s3.generate_presigned_url(\n",
    "        ClientMethod = 'get_object',\n",
    "        Params = data_source\n",
    "    )\n",
    "\n",
    "    data = pd.read_csv(url, nrows=rows)\n",
    "    return data\n",
    "\n",
    "\n",
    "df = preview_csv_dataset(bucket='nyc-tlc', key='trip data/yellow_tripdata_2018-01.csv', rows=100)\n",
    "\n",
    "df.head()\n",
    "df.shape\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:28:22</td>\n",
       "      <td>2018-12-01 00:44:07</td>\n",
       "      <td>2</td>\n",
       "      <td>2.500</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>148</td>\n",
       "      <td>234</td>\n",
       "      <td>1</td>\n",
       "      <td>12.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>3.950</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>17.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:52:29</td>\n",
       "      <td>2018-12-01 01:11:37</td>\n",
       "      <td>3</td>\n",
       "      <td>2.300</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>170</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>13.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2.850</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>17.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-01 00:12:52</td>\n",
       "      <td>2018-12-01 00:36:23</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>113</td>\n",
       "      <td>193</td>\n",
       "      <td>2</td>\n",
       "      <td>2.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:35:08</td>\n",
       "      <td>2018-12-01 00:43:11</td>\n",
       "      <td>1</td>\n",
       "      <td>3.900</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>95</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>12.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>16.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:21:54</td>\n",
       "      <td>2018-12-01 01:15:13</td>\n",
       "      <td>1</td>\n",
       "      <td>12.800</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>163</td>\n",
       "      <td>228</td>\n",
       "      <td>1</td>\n",
       "      <td>45.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>9.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>55.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:00:38</td>\n",
       "      <td>2018-12-01 00:29:26</td>\n",
       "      <td>1</td>\n",
       "      <td>18.800</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>132</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>50.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>10.350</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>62.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:59:39</td>\n",
       "      <td>2018-12-01 01:09:07</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>246</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>7.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>9.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:19:19</td>\n",
       "      <td>2018-12-01 00:22:19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.300</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>163</td>\n",
       "      <td>4</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>5.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:41:41</td>\n",
       "      <td>2018-12-01 01:09:02</td>\n",
       "      <td>1</td>\n",
       "      <td>3.300</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>43</td>\n",
       "      <td>146</td>\n",
       "      <td>2</td>\n",
       "      <td>17.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>18.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:16:03</td>\n",
       "      <td>2018-12-01 00:52:42</td>\n",
       "      <td>1</td>\n",
       "      <td>5.700</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>223</td>\n",
       "      <td>1</td>\n",
       "      <td>26.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>5.550</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>33.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:56:42</td>\n",
       "      <td>2018-12-01 01:22:35</td>\n",
       "      <td>1</td>\n",
       "      <td>17.300</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>132</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>52.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>10.000</td>\n",
       "      <td>5.760</td>\n",
       "      <td>0.300</td>\n",
       "      <td>68.560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:19:36</td>\n",
       "      <td>2018-12-01 00:24:58</td>\n",
       "      <td>1</td>\n",
       "      <td>0.300</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>114</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>7.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:27:51</td>\n",
       "      <td>2018-12-01 00:33:33</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>5.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>6.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:44:02</td>\n",
       "      <td>2018-12-01 01:13:33</td>\n",
       "      <td>2</td>\n",
       "      <td>2.600</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>79</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>19.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>4.050</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>24.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:30:12</td>\n",
       "      <td>2018-12-01 00:39:09</td>\n",
       "      <td>4</td>\n",
       "      <td>1.800</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>261</td>\n",
       "      <td>114</td>\n",
       "      <td>3</td>\n",
       "      <td>8.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>9.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:45:34</td>\n",
       "      <td>2018-12-01 00:54:36</td>\n",
       "      <td>2</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>249</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>8.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.950</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>11.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:58:18</td>\n",
       "      <td>2018-12-01 01:09:27</td>\n",
       "      <td>2</td>\n",
       "      <td>2.200</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>246</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>3.350</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>14.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:42:46</td>\n",
       "      <td>2018-12-01 00:55:13</td>\n",
       "      <td>1</td>\n",
       "      <td>3.900</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>148</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>13.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2.850</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>17.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:04:27</td>\n",
       "      <td>2018-12-01 00:18:59</td>\n",
       "      <td>3</td>\n",
       "      <td>1.600</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>249</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>11.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:20:12</td>\n",
       "      <td>2018-12-01 00:29:25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>7.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>10.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:31:19</td>\n",
       "      <td>2018-12-01 01:07:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>144</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>5.450</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>32.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:07:32</td>\n",
       "      <td>2018-12-01 00:09:21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>3.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>4.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:03:46</td>\n",
       "      <td>2018-12-01 00:15:47</td>\n",
       "      <td>2</td>\n",
       "      <td>1.700</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>148</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>9.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>12.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:18:07</td>\n",
       "      <td>2018-12-01 00:29:11</td>\n",
       "      <td>2</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>90</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>8.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.950</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>11.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:31:36</td>\n",
       "      <td>2018-12-01 00:41:42</td>\n",
       "      <td>2</td>\n",
       "      <td>1.400</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>107</td>\n",
       "      <td>249</td>\n",
       "      <td>1</td>\n",
       "      <td>8.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.950</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>11.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:49:10</td>\n",
       "      <td>2018-12-01 00:55:31</td>\n",
       "      <td>2</td>\n",
       "      <td>1.300</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>158</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>7.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>9.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-01 00:11:42</td>\n",
       "      <td>2018-12-01 00:45:39</td>\n",
       "      <td>1</td>\n",
       "      <td>7.700</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>48</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>28.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.760</td>\n",
       "      <td>0.300</td>\n",
       "      <td>40.560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-01 00:56:20</td>\n",
       "      <td>2018-12-01 01:18:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3.940</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>181</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>17.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>3.660</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>21.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:09:53</td>\n",
       "      <td>2018-12-01 00:32:44</td>\n",
       "      <td>3</td>\n",
       "      <td>2.200</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>1</td>\n",
       "      <td>15.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>3.350</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>20.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-01 00:34:01</td>\n",
       "      <td>2018-12-01 00:54:11</td>\n",
       "      <td>2</td>\n",
       "      <td>4.100</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>2</td>\n",
       "      <td>16.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>17.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173201</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-31 23:36:05</td>\n",
       "      <td>2018-12-31 23:54:08</td>\n",
       "      <td>2</td>\n",
       "      <td>2.100</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>164</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>12.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>16.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173202</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-31 23:15:29</td>\n",
       "      <td>2018-12-31 23:29:42</td>\n",
       "      <td>1</td>\n",
       "      <td>2.100</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>186</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>11.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>14.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173203</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-31 23:28:39</td>\n",
       "      <td>2018-12-31 23:35:25</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>234</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>6.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>7.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173204</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:30:07</td>\n",
       "      <td>2018-12-31 23:42:38</td>\n",
       "      <td>1</td>\n",
       "      <td>2.140</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>90</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2.260</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>13.560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173205</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-31 23:08:36</td>\n",
       "      <td>2018-12-31 23:35:21</td>\n",
       "      <td>2</td>\n",
       "      <td>5.200</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>48</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>20.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>3.270</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>25.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173206</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-31 23:43:54</td>\n",
       "      <td>2019-01-01 00:14:59</td>\n",
       "      <td>3</td>\n",
       "      <td>17.400</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>132</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>48.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>49.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173207</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:20:29</td>\n",
       "      <td>2018-12-31 23:34:54</td>\n",
       "      <td>1</td>\n",
       "      <td>2.390</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>234</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>10.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>11.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173208</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:20:37</td>\n",
       "      <td>2018-12-31 23:40:04</td>\n",
       "      <td>1</td>\n",
       "      <td>2.960</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>263</td>\n",
       "      <td>239</td>\n",
       "      <td>2</td>\n",
       "      <td>14.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>15.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173209</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:47:30</td>\n",
       "      <td>2019-01-01 00:11:28</td>\n",
       "      <td>1</td>\n",
       "      <td>4.770</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>142</td>\n",
       "      <td>231</td>\n",
       "      <td>1</td>\n",
       "      <td>18.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>3.960</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>23.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173210</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-31 23:00:17</td>\n",
       "      <td>2018-12-31 23:16:57</td>\n",
       "      <td>2</td>\n",
       "      <td>3.600</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>1</td>\n",
       "      <td>14.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>3.800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>19.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173211</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-31 23:26:33</td>\n",
       "      <td>2018-12-31 23:33:22</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>2</td>\n",
       "      <td>6.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>7.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173212</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:00:21</td>\n",
       "      <td>2018-12-31 23:03:03</td>\n",
       "      <td>1</td>\n",
       "      <td>0.890</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>170</td>\n",
       "      <td>229</td>\n",
       "      <td>1</td>\n",
       "      <td>4.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.740</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>7.540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173213</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:05:53</td>\n",
       "      <td>2018-12-31 23:12:13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.930</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>6.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.080</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>8.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173214</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:24:03</td>\n",
       "      <td>2018-12-31 23:39:17</td>\n",
       "      <td>1</td>\n",
       "      <td>3.550</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>151</td>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "      <td>14.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>4.590</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>19.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173215</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:56:05</td>\n",
       "      <td>2018-12-31 23:57:51</td>\n",
       "      <td>1</td>\n",
       "      <td>0.410</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>3.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>3.300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>8.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173216</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-31 23:09:16</td>\n",
       "      <td>2018-12-31 23:21:13</td>\n",
       "      <td>1</td>\n",
       "      <td>2.200</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>237</td>\n",
       "      <td>170</td>\n",
       "      <td>2</td>\n",
       "      <td>10.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>11.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173217</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-31 23:33:23</td>\n",
       "      <td>2018-12-31 23:54:43</td>\n",
       "      <td>1</td>\n",
       "      <td>3.600</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>164</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>15.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>21.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173218</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:04:04</td>\n",
       "      <td>2018-12-31 23:07:14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.510</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>237</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.320</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>6.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173219</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:24:35</td>\n",
       "      <td>2018-12-31 23:34:35</td>\n",
       "      <td>1</td>\n",
       "      <td>1.550</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>239</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>8.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>9.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173220</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:36:41</td>\n",
       "      <td>2018-12-31 23:39:35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.370</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.320</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>6.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173221</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 22:57:26</td>\n",
       "      <td>2018-12-31 23:27:04</td>\n",
       "      <td>1</td>\n",
       "      <td>7.090</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>43</td>\n",
       "      <td>231</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>5.460</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>32.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173222</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:23:45</td>\n",
       "      <td>2018-12-31 23:41:44</td>\n",
       "      <td>1</td>\n",
       "      <td>5.830</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>114</td>\n",
       "      <td>263</td>\n",
       "      <td>1</td>\n",
       "      <td>19.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>4.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>24.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173223</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 19:32:08</td>\n",
       "      <td>2018-12-31 19:35:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.400</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>5.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173224</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 20:05:51</td>\n",
       "      <td>2019-01-01 19:13:26</td>\n",
       "      <td>1</td>\n",
       "      <td>1.510</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>243</td>\n",
       "      <td>119</td>\n",
       "      <td>2</td>\n",
       "      <td>7.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>8.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173225</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:45:18</td>\n",
       "      <td>2018-12-31 23:51:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.980</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>132</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2.320</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>11.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173226</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:40:45</td>\n",
       "      <td>2018-12-31 23:49:52</td>\n",
       "      <td>3</td>\n",
       "      <td>1.080</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>237</td>\n",
       "      <td>2</td>\n",
       "      <td>7.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>8.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173227</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:16:46</td>\n",
       "      <td>2018-12-31 23:28:09</td>\n",
       "      <td>2</td>\n",
       "      <td>2.680</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>263</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>11.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>4.920</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>17.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173228</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:29:01</td>\n",
       "      <td>2018-12-31 23:33:39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>6.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173229</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:55:39</td>\n",
       "      <td>2018-12-31 23:59:15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.920</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>263</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.260</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>7.560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173230</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-31 23:13:26</td>\n",
       "      <td>2018-12-31 23:13:31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>264</td>\n",
       "      <td>193</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8173231 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0               1  2018-12-01 00:28:22   2018-12-01 00:44:07                2   \n",
       "1               1  2018-12-01 00:52:29   2018-12-01 01:11:37                3   \n",
       "2               2  2018-12-01 00:12:52   2018-12-01 00:36:23                1   \n",
       "3               1  2018-12-01 00:35:08   2018-12-01 00:43:11                1   \n",
       "4               1  2018-12-01 00:21:54   2018-12-01 01:15:13                1   \n",
       "5               1  2018-12-01 00:00:38   2018-12-01 00:29:26                1   \n",
       "6               1  2018-12-01 00:59:39   2018-12-01 01:09:07                1   \n",
       "7               1  2018-12-01 00:19:19   2018-12-01 00:22:19                1   \n",
       "8               1  2018-12-01 00:41:41   2018-12-01 01:09:02                1   \n",
       "9               1  2018-12-01 00:16:03   2018-12-01 00:52:42                1   \n",
       "10              1  2018-12-01 00:56:42   2018-12-01 01:22:35                1   \n",
       "11              1  2018-12-01 00:19:36   2018-12-01 00:24:58                1   \n",
       "12              1  2018-12-01 00:27:51   2018-12-01 00:33:33                1   \n",
       "13              1  2018-12-01 00:44:02   2018-12-01 01:13:33                2   \n",
       "14              1  2018-12-01 00:30:12   2018-12-01 00:39:09                4   \n",
       "15              1  2018-12-01 00:45:34   2018-12-01 00:54:36                2   \n",
       "16              1  2018-12-01 00:58:18   2018-12-01 01:09:27                2   \n",
       "17              1  2018-12-01 00:42:46   2018-12-01 00:55:13                1   \n",
       "18              1  2018-12-01 00:04:27   2018-12-01 00:18:59                3   \n",
       "19              1  2018-12-01 00:20:12   2018-12-01 00:29:25                3   \n",
       "20              1  2018-12-01 00:31:19   2018-12-01 01:07:00                1   \n",
       "21              1  2018-12-01 00:07:32   2018-12-01 00:09:21                2   \n",
       "22              1  2018-12-01 00:03:46   2018-12-01 00:15:47                2   \n",
       "23              1  2018-12-01 00:18:07   2018-12-01 00:29:11                2   \n",
       "24              1  2018-12-01 00:31:36   2018-12-01 00:41:42                2   \n",
       "25              1  2018-12-01 00:49:10   2018-12-01 00:55:31                2   \n",
       "26              2  2018-12-01 00:11:42   2018-12-01 00:45:39                1   \n",
       "27              2  2018-12-01 00:56:20   2018-12-01 01:18:00                1   \n",
       "28              1  2018-12-01 00:09:53   2018-12-01 00:32:44                3   \n",
       "29              1  2018-12-01 00:34:01   2018-12-01 00:54:11                2   \n",
       "...           ...                  ...                   ...              ...   \n",
       "8173201         1  2018-12-31 23:36:05   2018-12-31 23:54:08                2   \n",
       "8173202         1  2018-12-31 23:15:29   2018-12-31 23:29:42                1   \n",
       "8173203         1  2018-12-31 23:28:39   2018-12-31 23:35:25                1   \n",
       "8173204         2  2018-12-31 23:30:07   2018-12-31 23:42:38                1   \n",
       "8173205         1  2018-12-31 23:08:36   2018-12-31 23:35:21                2   \n",
       "8173206         1  2018-12-31 23:43:54   2019-01-01 00:14:59                3   \n",
       "8173207         2  2018-12-31 23:20:29   2018-12-31 23:34:54                1   \n",
       "8173208         2  2018-12-31 23:20:37   2018-12-31 23:40:04                1   \n",
       "8173209         2  2018-12-31 23:47:30   2019-01-01 00:11:28                1   \n",
       "8173210         1  2018-12-31 23:00:17   2018-12-31 23:16:57                2   \n",
       "8173211         1  2018-12-31 23:26:33   2018-12-31 23:33:22                1   \n",
       "8173212         2  2018-12-31 23:00:21   2018-12-31 23:03:03                1   \n",
       "8173213         2  2018-12-31 23:05:53   2018-12-31 23:12:13                1   \n",
       "8173214         2  2018-12-31 23:24:03   2018-12-31 23:39:17                1   \n",
       "8173215         2  2018-12-31 23:56:05   2018-12-31 23:57:51                1   \n",
       "8173216         1  2018-12-31 23:09:16   2018-12-31 23:21:13                1   \n",
       "8173217         1  2018-12-31 23:33:23   2018-12-31 23:54:43                1   \n",
       "8173218         2  2018-12-31 23:04:04   2018-12-31 23:07:14                1   \n",
       "8173219         2  2018-12-31 23:24:35   2018-12-31 23:34:35                1   \n",
       "8173220         2  2018-12-31 23:36:41   2018-12-31 23:39:35                1   \n",
       "8173221         2  2018-12-31 22:57:26   2018-12-31 23:27:04                1   \n",
       "8173222         2  2018-12-31 23:23:45   2018-12-31 23:41:44                1   \n",
       "8173223         2  2018-12-31 19:32:08   2018-12-31 19:35:00                1   \n",
       "8173224         2  2018-12-31 20:05:51   2019-01-01 19:13:26                1   \n",
       "8173225         2  2018-12-31 23:45:18   2018-12-31 23:51:59                1   \n",
       "8173226         2  2018-12-31 23:40:45   2018-12-31 23:49:52                3   \n",
       "8173227         2  2018-12-31 23:16:46   2018-12-31 23:28:09                2   \n",
       "8173228         2  2018-12-31 23:29:01   2018-12-31 23:33:39                1   \n",
       "8173229         2  2018-12-31 23:55:39   2018-12-31 23:59:15                1   \n",
       "8173230         2  2018-12-31 23:13:26   2018-12-31 23:13:31                1   \n",
       "\n",
       "         trip_distance  RatecodeID store_and_fwd_flag  PULocationID  \\\n",
       "0                2.500           1                  N           148   \n",
       "1                2.300           1                  N           170   \n",
       "2                0.000           1                  N           113   \n",
       "3                3.900           1                  N            95   \n",
       "4               12.800           1                  N           163   \n",
       "5               18.800           1                  N           132   \n",
       "6                1.000           1                  N           246   \n",
       "7                0.300           1                  N           161   \n",
       "8                3.300           1                  N            43   \n",
       "9                5.700           1                  N           161   \n",
       "10              17.300           2                  N           132   \n",
       "11               0.300           1                  N           114   \n",
       "12               0.800           1                  N            79   \n",
       "13               2.600           1                  N            79   \n",
       "14               1.800           1                  N           261   \n",
       "15               2.000           1                  N           249   \n",
       "16               2.200           1                  N           246   \n",
       "17               3.900           1                  N           148   \n",
       "18               1.600           1                  N           249   \n",
       "19               1.200           1                  N             4   \n",
       "20               4.000           1                  N           144   \n",
       "21               0.500           1                  N            74   \n",
       "22               1.700           1                  N           148   \n",
       "23               1.200           1                  N            90   \n",
       "24               1.400           1                  N           107   \n",
       "25               1.300           1                  N           158   \n",
       "26               7.700           1                  N            48   \n",
       "27               3.940           1                  N           181   \n",
       "28               2.200           1                  N           264   \n",
       "29               4.100           1                  N           264   \n",
       "...                ...         ...                ...           ...   \n",
       "8173201          2.100           1                  N           164   \n",
       "8173202          2.100           1                  N           186   \n",
       "8173203          1.000           1                  N           234   \n",
       "8173204          2.140           1                  N            90   \n",
       "8173205          5.200           1                  N            48   \n",
       "8173206         17.400           1                  N           132   \n",
       "8173207          2.390           1                  N           234   \n",
       "8173208          2.960           1                  N           263   \n",
       "8173209          4.770           1                  N           142   \n",
       "8173210          3.600           1                  N           264   \n",
       "8173211          0.700           1                  N           264   \n",
       "8173212          0.890           1                  N           170   \n",
       "8173213          0.930           1                  N           229   \n",
       "8173214          3.550           1                  N           151   \n",
       "8173215          0.410           1                  N            41   \n",
       "8173216          2.200           1                  N           237   \n",
       "8173217          3.600           1                  N           164   \n",
       "8173218          0.510           1                  N           237   \n",
       "8173219          1.550           1                  N           239   \n",
       "8173220          0.370           1                  N            48   \n",
       "8173221          7.090           1                  N            43   \n",
       "8173222          5.830           1                  N           114   \n",
       "8173223          0.400           1                  N            41   \n",
       "8173224          1.510           1                  N           243   \n",
       "8173225          1.980           1                  N           132   \n",
       "8173226          1.080           1                  N           161   \n",
       "8173227          2.680           1                  N           263   \n",
       "8173228          0.750           1                  N            41   \n",
       "8173229          0.920           1                  N           263   \n",
       "8173230          0.000           1                  N           264   \n",
       "\n",
       "         DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  \\\n",
       "0                 234             1       12.000  0.500    0.500       3.950   \n",
       "1                 144             1       13.000  0.500    0.500       2.850   \n",
       "2                 193             2        2.500  0.500    0.500       0.000   \n",
       "3                  92             1       12.500  0.500    0.500       2.750   \n",
       "4                 228             1       45.000  0.500    0.500       9.250   \n",
       "5                  97             1       50.500  0.500    0.500      10.350   \n",
       "6                 164             1        7.500  0.500    0.500       0.440   \n",
       "7                 163             4        4.000  0.500    0.500       0.000   \n",
       "8                 146             2       17.500  0.500    0.500       0.000   \n",
       "9                 223             1       26.500  0.500    0.500       5.550   \n",
       "10                170             1       52.000  0.000    0.500      10.000   \n",
       "11                 79             1        5.000  0.500    0.500       1.250   \n",
       "12                 79             2        5.500  0.500    0.500       0.000   \n",
       "13                 65             1       19.000  0.500    0.500       4.050   \n",
       "14                114             3        8.500  0.500    0.500       0.000   \n",
       "15                246             1        8.500  0.500    0.500       1.950   \n",
       "16                163             1       10.000  0.500    0.500       3.350   \n",
       "17                140             1       13.000  0.500    0.500       2.850   \n",
       "18                  4             2       10.000  0.500    0.500       0.000   \n",
       "19                144             1        7.500  0.500    0.500       1.750   \n",
       "20                112             1       26.000  0.500    0.500       5.450   \n",
       "21                 74             1        3.500  0.500    0.500       0.000   \n",
       "22                 90             1        9.500  0.500    0.500       2.150   \n",
       "23                107             1        8.500  0.500    0.500       1.950   \n",
       "24                249             1        8.500  0.500    0.500       1.950   \n",
       "25                246             1        7.000  0.500    0.500       1.000   \n",
       "26                106             1       28.500  0.500    0.500       5.000   \n",
       "27                 37             1       17.000  0.500    0.500       3.660   \n",
       "28                264             1       15.500  0.500    0.500       3.350   \n",
       "29                264             2       16.500  0.500    0.500       0.000   \n",
       "...               ...           ...          ...    ...      ...         ...   \n",
       "8173201            68             1       12.500  0.500    0.500       2.750   \n",
       "8173202           143             1       11.000  0.500    0.500       2.000   \n",
       "8173203            68             2        6.000  0.500    0.500       0.000   \n",
       "8173204            48             1       10.000  0.500    0.500       2.260   \n",
       "8173205            87             1       20.500  0.500    0.500       3.270   \n",
       "8173206           265             2       48.000  0.500    0.500       0.000   \n",
       "8173207            48             2       10.500  0.500    0.500       0.000   \n",
       "8173208           239             2       14.500  0.500    0.500       0.000   \n",
       "8173209           231             1       18.500  0.500    0.500       3.960   \n",
       "8173210           264             1       14.000  0.500    0.500       3.800   \n",
       "8173211           264             2        6.000  0.500    0.500       0.000   \n",
       "8173212           229             1        4.500  0.500    0.500       1.740   \n",
       "8173213           163             1        6.000  0.500    0.500       1.080   \n",
       "8173214           168             1       14.000  0.500    0.500       4.590   \n",
       "8173215            42             1        3.500  0.500    0.500       3.300   \n",
       "8173216           170             2       10.500  0.500    0.500       0.000   \n",
       "8173217           239             1       15.500  0.500    0.500       5.000   \n",
       "8173218           163             1        4.000  0.500    0.500       1.320   \n",
       "8173219            48             2        8.500  0.500    0.500       0.000   \n",
       "8173220            48             1        4.000  0.500    0.500       1.320   \n",
       "8173221           231             1       26.000  0.500    0.500       5.460   \n",
       "8173222           263             1       19.500  0.500    0.500       4.160   \n",
       "8173223            41             2        4.000  0.500    0.500       0.000   \n",
       "8173224           119             2        7.500  0.500    0.500       0.000   \n",
       "8173225            10             1        8.000  0.500    0.500       2.320   \n",
       "8173226           237             2        7.500  0.500    0.500       0.000   \n",
       "8173227            24             1       11.000  0.500    0.500       4.920   \n",
       "8173228            43             2        5.000  0.500    0.500       0.000   \n",
       "8173229            75             1        5.000  0.500    0.500       1.260   \n",
       "8173230           193             2        0.000  0.000    0.000       0.000   \n",
       "\n",
       "         tolls_amount  improvement_surcharge  total_amount  \n",
       "0               0.000                  0.300        17.250  \n",
       "1               0.000                  0.300        17.150  \n",
       "2               0.000                  0.300         3.800  \n",
       "3               0.000                  0.300        16.550  \n",
       "4               0.000                  0.300        55.550  \n",
       "5               0.000                  0.300        62.150  \n",
       "6               0.000                  0.300         9.240  \n",
       "7               0.000                  0.300         5.300  \n",
       "8               0.000                  0.300        18.800  \n",
       "9               0.000                  0.300        33.350  \n",
       "10              5.760                  0.300        68.560  \n",
       "11              0.000                  0.300         7.550  \n",
       "12              0.000                  0.300         6.800  \n",
       "13              0.000                  0.300        24.350  \n",
       "14              0.000                  0.300         9.800  \n",
       "15              0.000                  0.300        11.750  \n",
       "16              0.000                  0.300        14.650  \n",
       "17              0.000                  0.300        17.150  \n",
       "18              0.000                  0.300        11.300  \n",
       "19              0.000                  0.300        10.550  \n",
       "20              0.000                  0.300        32.750  \n",
       "21              0.000                  0.300         4.800  \n",
       "22              0.000                  0.300        12.950  \n",
       "23              0.000                  0.300        11.750  \n",
       "24              0.000                  0.300        11.750  \n",
       "25              0.000                  0.300         9.300  \n",
       "26              5.760                  0.300        40.560  \n",
       "27              0.000                  0.300        21.960  \n",
       "28              0.000                  0.300        20.150  \n",
       "29              0.000                  0.300        17.800  \n",
       "...               ...                    ...           ...  \n",
       "8173201         0.000                  0.300        16.550  \n",
       "8173202         0.000                  0.300        14.300  \n",
       "8173203         0.000                  0.300         7.300  \n",
       "8173204         0.000                  0.300        13.560  \n",
       "8173205         0.000                  0.300        25.070  \n",
       "8173206         0.000                  0.300        49.300  \n",
       "8173207         0.000                  0.300        11.800  \n",
       "8173208         0.000                  0.300        15.800  \n",
       "8173209         0.000                  0.300        23.760  \n",
       "8173210         0.000                  0.300        19.100  \n",
       "8173211         0.000                  0.300         7.300  \n",
       "8173212         0.000                  0.300         7.540  \n",
       "8173213         0.000                  0.300         8.380  \n",
       "8173214         0.000                  0.300        19.890  \n",
       "8173215         0.000                  0.300         8.100  \n",
       "8173216         0.000                  0.300        11.800  \n",
       "8173217         0.000                  0.300        21.800  \n",
       "8173218         0.000                  0.300         6.620  \n",
       "8173219         0.000                  0.300         9.800  \n",
       "8173220         0.000                  0.300         6.620  \n",
       "8173221         0.000                  0.300        32.760  \n",
       "8173222         0.000                  0.300        24.960  \n",
       "8173223         0.000                  0.300         5.300  \n",
       "8173224         0.000                  0.300         8.800  \n",
       "8173225         0.000                  0.300        11.620  \n",
       "8173226         0.000                  0.300         8.800  \n",
       "8173227         0.000                  0.300        17.220  \n",
       "8173228         0.000                  0.300         6.300  \n",
       "8173229         0.000                  0.300         7.560  \n",
       "8173230         0.000                  0.000         0.000  \n",
       "\n",
       "[8173231 rows x 17 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download data to the notebook instance\n",
    "for i in range(1,13):\n",
    "    if i<10:\n",
    "        raw_data_filename = 'trip data/yellow_tripdata_2018-0'+str(i)+'.csv'\n",
    "    else:\n",
    "        raw_data_filename = 'trip data/yellow_tripdata_2018-'+str(i)+'.csv'\n",
    "    data_bucket = 'nyc-tlc'\n",
    "    filename='raw_data2018'+str(i)+'.csv'\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(data_bucket).download_file(raw_data_filename, filename)\n",
    "\n",
    "    data = pd.read_csv('./'+filename)\n",
    "#pd.set_option('display.max_rows', 20) \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data1 = pd.read_csv('./raw_data20181.csv',usecols=['tpep_pickup_datetime','passenger_count','trip_distance','fare_amount','tip_amount','total_amount','payment_type'])\n",
    "# data2 = pd.read_csv('./raw_data20182.csv',usecols=['passenger_count','trip_distance','fare_amount','tip_amount','total_amount'])\n",
    "# data3 = pd.read_csv('./raw_data20183.csv',usecols=['passenger_count','trip_distance','fare_amount','tip_amount','total_amount'])\n",
    "# data4 = pd.read_csv('./raw_data20184.csv',usecols=['passenger_count','trip_distance','fare_amount','tip_amount','total_amount'])\n",
    "# data5 = pd.read_csv('./raw_data20185.csv',usecols=['passenger_count','trip_distance','fare_amount','tip_amount','total_amount'])\n",
    "# data6 = pd.read_csv('./raw_data20186.csv',usecols=['passenger_count','trip_distance','fare_amount','tip_amount','total_amount'])\n",
    "# data7 = pd.read_csv('./raw_data20187.csv',usecols=['passenger_count','trip_distance','fare_amount','tip_amount','total_amount'])\n",
    "# data8 = pd.read_csv('./raw_data20188.csv',usecols=['passenger_count','trip_distance','fare_amount','tip_amount','total_amount'])\n",
    "# data9 = pd.read_csv('./raw_data20189.csv',usecols=['passenger_count','trip_distance','fare_amount','tip_amount','total_amount'])\n",
    "# data10 = pd.read_csv('./raw_data201810.csv',usecols=['passenger_count','trip_distance','fare_amount','tip_amount','total_amount'])\n",
    "# data11 = pd.read_csv('./raw_data201811.csv',usecols=['passenger_count','trip_distance','fare_amount','tip_amount','total_amount'])\n",
    "# data12 = pd.read_csv('./raw_data201812.csv',usecols=['passenger_count','trip_distance','fare_amount','tip_amount','total_amount'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>tip_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:21:05</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256908</th>\n",
       "      <td>2018-01-16 23:05:13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256910</th>\n",
       "      <td>2018-01-16 23:48:44</td>\n",
       "      <td>6</td>\n",
       "      <td>1.19</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256914</th>\n",
       "      <td>2018-01-16 23:55:54</td>\n",
       "      <td>1</td>\n",
       "      <td>1.25</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256918</th>\n",
       "      <td>2018-01-16 23:16:20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.39</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256919</th>\n",
       "      <td>2018-01-16 23:21:50</td>\n",
       "      <td>5</td>\n",
       "      <td>1.23</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256925</th>\n",
       "      <td>2018-01-16 23:13:51</td>\n",
       "      <td>1</td>\n",
       "      <td>9.52</td>\n",
       "      <td>27.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256930</th>\n",
       "      <td>2018-01-16 23:48:26</td>\n",
       "      <td>1</td>\n",
       "      <td>3.80</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256932</th>\n",
       "      <td>2018-01-16 23:10:33</td>\n",
       "      <td>1</td>\n",
       "      <td>0.41</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256938</th>\n",
       "      <td>2018-01-16 23:34:16</td>\n",
       "      <td>1</td>\n",
       "      <td>2.10</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256942</th>\n",
       "      <td>2018-01-16 23:31:14</td>\n",
       "      <td>1</td>\n",
       "      <td>3.55</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256945</th>\n",
       "      <td>2018-01-16 23:47:19</td>\n",
       "      <td>1</td>\n",
       "      <td>1.12</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256947</th>\n",
       "      <td>2018-01-16 23:37:17</td>\n",
       "      <td>2</td>\n",
       "      <td>4.68</td>\n",
       "      <td>16.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256950</th>\n",
       "      <td>2018-01-16 23:04:18</td>\n",
       "      <td>5</td>\n",
       "      <td>3.61</td>\n",
       "      <td>12.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256957</th>\n",
       "      <td>2018-01-16 23:46:37</td>\n",
       "      <td>2</td>\n",
       "      <td>2.61</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256960</th>\n",
       "      <td>2018-01-16 23:50:31</td>\n",
       "      <td>1</td>\n",
       "      <td>2.60</td>\n",
       "      <td>10.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256962</th>\n",
       "      <td>2018-01-16 23:27:07</td>\n",
       "      <td>1</td>\n",
       "      <td>1.20</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256963</th>\n",
       "      <td>2018-01-16 23:09:10</td>\n",
       "      <td>1</td>\n",
       "      <td>12.90</td>\n",
       "      <td>38.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256972</th>\n",
       "      <td>2018-01-16 23:32:33</td>\n",
       "      <td>3</td>\n",
       "      <td>0.88</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256985</th>\n",
       "      <td>2018-01-16 23:22:55</td>\n",
       "      <td>1</td>\n",
       "      <td>1.70</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256991</th>\n",
       "      <td>2018-01-16 23:23:15</td>\n",
       "      <td>2</td>\n",
       "      <td>1.12</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256993</th>\n",
       "      <td>2018-01-16 23:02:46</td>\n",
       "      <td>5</td>\n",
       "      <td>1.61</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256998</th>\n",
       "      <td>2018-01-16 23:26:39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.59</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256999</th>\n",
       "      <td>2018-01-16 23:35:44</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4257001</th>\n",
       "      <td>2018-01-16 23:37:20</td>\n",
       "      <td>1</td>\n",
       "      <td>1.29</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4257004</th>\n",
       "      <td>2018-01-16 23:24:23</td>\n",
       "      <td>1</td>\n",
       "      <td>1.18</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4257009</th>\n",
       "      <td>2018-01-16 23:31:38</td>\n",
       "      <td>1</td>\n",
       "      <td>7.10</td>\n",
       "      <td>25.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4257016</th>\n",
       "      <td>2018-01-16 23:29:22</td>\n",
       "      <td>1</td>\n",
       "      <td>3.77</td>\n",
       "      <td>14.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256906</th>\n",
       "      <td>2018-01-16 23:47:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4.20</td>\n",
       "      <td>13.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4257020</th>\n",
       "      <td>2018-01-16 23:09:03</td>\n",
       "      <td>3</td>\n",
       "      <td>7.99</td>\n",
       "      <td>28.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407978</th>\n",
       "      <td>2018-01-06 23:10:11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>3.50</td>\n",
       "      <td>91.29</td>\n",
       "      <td>96.09</td>\n",
       "      <td>26.082857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11372</th>\n",
       "      <td>2018-01-01 00:03:01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>84.30</td>\n",
       "      <td>26.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5888144</th>\n",
       "      <td>2018-01-22 14:42:58</td>\n",
       "      <td>1</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.50</td>\n",
       "      <td>67.00</td>\n",
       "      <td>70.30</td>\n",
       "      <td>26.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160850</th>\n",
       "      <td>2018-01-06 03:53:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.50</td>\n",
       "      <td>70.00</td>\n",
       "      <td>73.80</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6650153</th>\n",
       "      <td>2018-01-25 04:47:58</td>\n",
       "      <td>1</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.50</td>\n",
       "      <td>71.20</td>\n",
       "      <td>75.00</td>\n",
       "      <td>28.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306170</th>\n",
       "      <td>2018-01-02 11:14:17</td>\n",
       "      <td>1</td>\n",
       "      <td>9.80</td>\n",
       "      <td>3.00</td>\n",
       "      <td>96.20</td>\n",
       "      <td>100.00</td>\n",
       "      <td>32.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5236217</th>\n",
       "      <td>2018-01-20 00:53:17</td>\n",
       "      <td>1</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.50</td>\n",
       "      <td>82.00</td>\n",
       "      <td>85.80</td>\n",
       "      <td>32.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3958719</th>\n",
       "      <td>2018-01-15 23:08:32</td>\n",
       "      <td>1</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2.50</td>\n",
       "      <td>85.00</td>\n",
       "      <td>88.80</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6619652</th>\n",
       "      <td>2018-01-24 22:00:03</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.50</td>\n",
       "      <td>88.00</td>\n",
       "      <td>91.80</td>\n",
       "      <td>35.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5928537</th>\n",
       "      <td>2018-01-22 16:56:47</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>95.00</td>\n",
       "      <td>99.30</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4201517</th>\n",
       "      <td>2018-01-16 19:56:58</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>95.70</td>\n",
       "      <td>100.00</td>\n",
       "      <td>38.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998826</th>\n",
       "      <td>2018-01-05 15:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.50</td>\n",
       "      <td>96.70</td>\n",
       "      <td>100.00</td>\n",
       "      <td>38.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3523314</th>\n",
       "      <td>2018-01-14 11:27:45</td>\n",
       "      <td>3</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2.50</td>\n",
       "      <td>125.28</td>\n",
       "      <td>128.58</td>\n",
       "      <td>50.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7134586</th>\n",
       "      <td>2018-01-26 15:17:43</td>\n",
       "      <td>1</td>\n",
       "      <td>13.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>127.00</td>\n",
       "      <td>130.30</td>\n",
       "      <td>50.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182921</th>\n",
       "      <td>2018-01-09 22:43:54</td>\n",
       "      <td>1</td>\n",
       "      <td>47.40</td>\n",
       "      <td>2.50</td>\n",
       "      <td>141.20</td>\n",
       "      <td>145.00</td>\n",
       "      <td>56.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5452107</th>\n",
       "      <td>2018-01-20 19:27:09</td>\n",
       "      <td>6</td>\n",
       "      <td>1.35</td>\n",
       "      <td>7.00</td>\n",
       "      <td>411.00</td>\n",
       "      <td>418.80</td>\n",
       "      <td>58.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4544998</th>\n",
       "      <td>2018-01-17 22:52:21</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13</td>\n",
       "      <td>6.00</td>\n",
       "      <td>441.71</td>\n",
       "      <td>449.01</td>\n",
       "      <td>73.618333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8146086</th>\n",
       "      <td>2018-01-30 02:25:55</td>\n",
       "      <td>1</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.00</td>\n",
       "      <td>220.00</td>\n",
       "      <td>222.30</td>\n",
       "      <td>110.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5099442</th>\n",
       "      <td>2018-01-19 17:44:15</td>\n",
       "      <td>1</td>\n",
       "      <td>17.63</td>\n",
       "      <td>0.45</td>\n",
       "      <td>65.00</td>\n",
       "      <td>85.45</td>\n",
       "      <td>144.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5392233</th>\n",
       "      <td>2018-01-20 16:14:23</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.31</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5204923</th>\n",
       "      <td>2018-01-19 22:53:14</td>\n",
       "      <td>1</td>\n",
       "      <td>51.90</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.35</td>\n",
       "      <td>27.66</td>\n",
       "      <td>635.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8533870</th>\n",
       "      <td>2018-01-31 11:12:52</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>11.80</td>\n",
       "      <td>70.87</td>\n",
       "      <td>1180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8005115</th>\n",
       "      <td>2018-01-29 14:07:29</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>17.08</td>\n",
       "      <td>17.39</td>\n",
       "      <td>1708.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8563722</th>\n",
       "      <td>2018-01-31 12:00:13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.01</td>\n",
       "      <td>21.00</td>\n",
       "      <td>21.31</td>\n",
       "      <td>2100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8142992</th>\n",
       "      <td>2018-01-30 00:16:22</td>\n",
       "      <td>1</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.31</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711170</th>\n",
       "      <td>2018-01-08 09:40:36</td>\n",
       "      <td>1</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>55.00</td>\n",
       "      <td>67.81</td>\n",
       "      <td>5500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699997</th>\n",
       "      <td>2018-01-08 08:58:37</td>\n",
       "      <td>1</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>66.66</td>\n",
       "      <td>79.47</td>\n",
       "      <td>6666.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874116</th>\n",
       "      <td>2018-01-12 10:43:56</td>\n",
       "      <td>1</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.01</td>\n",
       "      <td>80.00</td>\n",
       "      <td>80.31</td>\n",
       "      <td>8000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971319</th>\n",
       "      <td>2018-01-05 14:33:52</td>\n",
       "      <td>2</td>\n",
       "      <td>11.40</td>\n",
       "      <td>0.01</td>\n",
       "      <td>97.20</td>\n",
       "      <td>108.01</td>\n",
       "      <td>9720.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3839994</th>\n",
       "      <td>2018-01-15 14:43:18</td>\n",
       "      <td>2</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.31</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8641672 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tpep_pickup_datetime  passenger_count  trip_distance  fare_amount  \\\n",
       "0        2018-01-01 00:21:05                1           0.50         4.50   \n",
       "4256908  2018-01-16 23:05:13                1           0.84         4.50   \n",
       "4256910  2018-01-16 23:48:44                6           1.19         6.50   \n",
       "4256914  2018-01-16 23:55:54                1           1.25         6.50   \n",
       "4256918  2018-01-16 23:16:20                5           0.39         4.00   \n",
       "4256919  2018-01-16 23:21:50                5           1.23         6.00   \n",
       "4256925  2018-01-16 23:13:51                1           9.52        27.00   \n",
       "4256930  2018-01-16 23:48:26                1           3.80        15.00   \n",
       "4256932  2018-01-16 23:10:33                1           0.41         4.00   \n",
       "4256938  2018-01-16 23:34:16                1           2.10         9.00   \n",
       "4256942  2018-01-16 23:31:14                1           3.55        13.00   \n",
       "4256945  2018-01-16 23:47:19                1           1.12         6.00   \n",
       "4256947  2018-01-16 23:37:17                2           4.68        16.50   \n",
       "4256950  2018-01-16 23:04:18                5           3.61        12.50   \n",
       "4256957  2018-01-16 23:46:37                2           2.61        13.00   \n",
       "4256960  2018-01-16 23:50:31                1           2.60        10.50   \n",
       "4256962  2018-01-16 23:27:07                1           1.20         6.00   \n",
       "4256963  2018-01-16 23:09:10                1          12.90        38.00   \n",
       "4256972  2018-01-16 23:32:33                3           0.88         6.50   \n",
       "4256985  2018-01-16 23:22:55                1           1.70         8.50   \n",
       "4256991  2018-01-16 23:23:15                2           1.12         7.50   \n",
       "4256993  2018-01-16 23:02:46                5           1.61         7.50   \n",
       "4256998  2018-01-16 23:26:39                1           0.59         4.00   \n",
       "4256999  2018-01-16 23:35:44                1           0.78         5.00   \n",
       "4257001  2018-01-16 23:37:20                1           1.29         7.00   \n",
       "4257004  2018-01-16 23:24:23                1           1.18         6.00   \n",
       "4257009  2018-01-16 23:31:38                1           7.10        25.50   \n",
       "4257016  2018-01-16 23:29:22                1           3.77        14.50   \n",
       "4256906  2018-01-16 23:47:00                1           4.20        13.50   \n",
       "4257020  2018-01-16 23:09:03                3           7.99        28.00   \n",
       "...                      ...              ...            ...          ...   \n",
       "1407978  2018-01-06 23:10:11                1           0.30         3.50   \n",
       "11372    2018-01-01 00:03:01                1           0.01         3.00   \n",
       "5888144  2018-01-22 14:42:58                1           2.80         2.50   \n",
       "1160850  2018-01-06 03:53:00                1           2.90         2.50   \n",
       "6650153  2018-01-25 04:47:58                1           0.60         2.50   \n",
       "306170   2018-01-02 11:14:17                1           9.80         3.00   \n",
       "5236217  2018-01-20 00:53:17                1           2.10         2.50   \n",
       "3958719  2018-01-15 23:08:32                1           1.20         2.50   \n",
       "6619652  2018-01-24 22:00:03                1           0.80         2.50   \n",
       "5928537  2018-01-22 16:56:47                1           1.00         2.50   \n",
       "4201517  2018-01-16 19:56:58                1           2.50         2.50   \n",
       "998826   2018-01-05 15:59:59                1           0.60         2.50   \n",
       "3523314  2018-01-14 11:27:45                3           0.15         2.50   \n",
       "7134586  2018-01-26 15:17:43                1          13.00         2.50   \n",
       "2182921  2018-01-09 22:43:54                1          47.40         2.50   \n",
       "5452107  2018-01-20 19:27:09                6           1.35         7.00   \n",
       "4544998  2018-01-17 22:52:21                1           1.13         6.00   \n",
       "8146086  2018-01-30 02:25:55                1           2.78         2.00   \n",
       "5099442  2018-01-19 17:44:15                1          17.63         0.45   \n",
       "5392233  2018-01-20 16:14:23                1           0.10         0.01   \n",
       "5204923  2018-01-19 22:53:14                1          51.90         0.01   \n",
       "8533870  2018-01-31 11:12:52                1           2.50         0.01   \n",
       "8005115  2018-01-29 14:07:29                2           0.50         0.01   \n",
       "8563722  2018-01-31 12:00:13                1           0.20         0.01   \n",
       "8142992  2018-01-30 00:16:22                1           3.80         0.01   \n",
       "1711170  2018-01-08 09:40:36                1           3.80         0.01   \n",
       "1699997  2018-01-08 08:58:37                1          11.80         0.01   \n",
       "2874116  2018-01-12 10:43:56                1          10.20         0.01   \n",
       "971319   2018-01-05 14:33:52                2          11.40         0.01   \n",
       "3839994  2018-01-15 14:43:18                2           1.30         0.01   \n",
       "\n",
       "         tip_amount  total_amount   tip_percent  \n",
       "0              0.00          5.80      0.000000  \n",
       "4256908        0.00          5.80      0.000000  \n",
       "4256910        0.00          7.80      0.000000  \n",
       "4256914        0.00          7.80      0.000000  \n",
       "4256918        0.00          5.30      0.000000  \n",
       "4256919        0.00          7.30      0.000000  \n",
       "4256925        0.00         28.30      0.000000  \n",
       "4256930        0.00         16.30      0.000000  \n",
       "4256932        0.00          5.30      0.000000  \n",
       "4256938        0.00         10.30      0.000000  \n",
       "4256942        0.00         14.30      0.000000  \n",
       "4256945        0.00          7.30      0.000000  \n",
       "4256947        0.00         17.80      0.000000  \n",
       "4256950        0.00         13.80      0.000000  \n",
       "4256957        0.00         14.30      0.000000  \n",
       "4256960        0.00         11.80      0.000000  \n",
       "4256962        0.00          7.30      0.000000  \n",
       "4256963        0.00         39.30      0.000000  \n",
       "4256972        0.00          7.80      0.000000  \n",
       "4256985        0.00          9.80      0.000000  \n",
       "4256991        0.00          8.80      0.000000  \n",
       "4256993        0.00          8.80      0.000000  \n",
       "4256998        0.00          5.30      0.000000  \n",
       "4256999        0.00          6.30      0.000000  \n",
       "4257001        0.00          8.30      0.000000  \n",
       "4257004        0.00          7.30      0.000000  \n",
       "4257009        0.00         26.80      0.000000  \n",
       "4257016        0.00         15.80      0.000000  \n",
       "4256906        0.00         14.80      0.000000  \n",
       "4257020        0.00         29.30      0.000000  \n",
       "...             ...           ...           ...  \n",
       "1407978       91.29         96.09     26.082857  \n",
       "11372         80.00         84.30     26.666667  \n",
       "5888144       67.00         70.30     26.800000  \n",
       "1160850       70.00         73.80     28.000000  \n",
       "6650153       71.20         75.00     28.480000  \n",
       "306170        96.20        100.00     32.066667  \n",
       "5236217       82.00         85.80     32.800000  \n",
       "3958719       85.00         88.80     34.000000  \n",
       "6619652       88.00         91.80     35.200000  \n",
       "5928537       95.00         99.30     38.000000  \n",
       "4201517       95.70        100.00     38.280000  \n",
       "998826        96.70        100.00     38.680000  \n",
       "3523314      125.28        128.58     50.112000  \n",
       "7134586      127.00        130.30     50.800000  \n",
       "2182921      141.20        145.00     56.480000  \n",
       "5452107      411.00        418.80     58.714286  \n",
       "4544998      441.71        449.01     73.618333  \n",
       "8146086      220.00        222.30    110.000000  \n",
       "5099442       65.00         85.45    144.444444  \n",
       "5392233        4.00          4.31    400.000000  \n",
       "5204923        6.35         27.66    635.000000  \n",
       "8533870       11.80         70.87   1180.000000  \n",
       "8005115       17.08         17.39   1708.000000  \n",
       "8563722       21.00         21.31   2100.000000  \n",
       "8142992       50.00         50.31   5000.000000  \n",
       "1711170       55.00         67.81   5500.000000  \n",
       "1699997       66.66         79.47   6666.000000  \n",
       "2874116       80.00         80.31   8000.000000  \n",
       "971319        97.20        108.01   9720.000000  \n",
       "3839994      100.00        100.31  10000.000000  \n",
       "\n",
       "[8641672 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.759874e+06</td>\n",
       "      <td>8.759874e+06</td>\n",
       "      <td>8.759874e+06</td>\n",
       "      <td>8.759874e+06</td>\n",
       "      <td>8.759874e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.606855e+00</td>\n",
       "      <td>2.804001e+00</td>\n",
       "      <td>1.224434e+01</td>\n",
       "      <td>1.818745e+00</td>\n",
       "      <td>1.549098e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.258464e+00</td>\n",
       "      <td>6.412346e+01</td>\n",
       "      <td>1.168321e+01</td>\n",
       "      <td>2.486341e+00</td>\n",
       "      <td>1.419540e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.500000e+02</td>\n",
       "      <td>-8.880000e+01</td>\n",
       "      <td>-4.503000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.100000e-01</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.300000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.550000e+00</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>1.360000e+00</td>\n",
       "      <td>1.130000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.840000e+00</td>\n",
       "      <td>1.350000e+01</td>\n",
       "      <td>2.350000e+00</td>\n",
       "      <td>1.662000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>1.894838e+05</td>\n",
       "      <td>8.016000e+03</td>\n",
       "      <td>4.417100e+02</td>\n",
       "      <td>8.016800e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       passenger_count  trip_distance   fare_amount    tip_amount  \\\n",
       "count     8.759874e+06   8.759874e+06  8.759874e+06  8.759874e+06   \n",
       "mean      1.606855e+00   2.804001e+00  1.224434e+01  1.818745e+00   \n",
       "std       1.258464e+00   6.412346e+01  1.168321e+01  2.486341e+00   \n",
       "min       0.000000e+00   0.000000e+00 -4.500000e+02 -8.880000e+01   \n",
       "25%       1.000000e+00   9.100000e-01  6.000000e+00  0.000000e+00   \n",
       "50%       1.000000e+00   1.550000e+00  9.000000e+00  1.360000e+00   \n",
       "75%       2.000000e+00   2.840000e+00  1.350000e+01  2.350000e+00   \n",
       "max       9.000000e+00   1.894838e+05  8.016000e+03  4.417100e+02   \n",
       "\n",
       "       total_amount  \n",
       "count  8.759874e+06  \n",
       "mean   1.549098e+01  \n",
       "std    1.419540e+01  \n",
       "min   -4.503000e+02  \n",
       "25%    8.300000e+00  \n",
       "50%    1.130000e+01  \n",
       "75%    1.662000e+01  \n",
       "max    8.016800e+03  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get a general idea about the data distribution\n",
    "data1[['passenger_count','trip_distance','fare_amount','tip_amount','total_amount']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We here used January data as an example to rank the dataframe base on tip amount, and we found that there are negative numbers, which does not make sense since drivers would never tip back to their customers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5300576</th>\n",
       "      <td>2018-01-20 10:18:32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.81</td>\n",
       "      <td>3</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-88.80</td>\n",
       "      <td>-94.10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7849715</th>\n",
       "      <td>2018-01-28 21:34:35</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-13.06</td>\n",
       "      <td>-78.36</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6210621</th>\n",
       "      <td>2018-01-23 17:18:30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.36</td>\n",
       "      <td>4</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-11.00</td>\n",
       "      <td>-16.80</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3575694</th>\n",
       "      <td>2018-01-14 14:25:45</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>4</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-10.00</td>\n",
       "      <td>-15.30</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562900</th>\n",
       "      <td>2018-01-11 10:21:16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>-7.92</td>\n",
       "      <td>-60.72</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555849</th>\n",
       "      <td>2018-01-21 02:01:48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>-3.32</td>\n",
       "      <td>-16.62</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210176</th>\n",
       "      <td>2018-01-10 06:59:44</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>-6.30</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3912075</th>\n",
       "      <td>2018-01-15 19:43:33</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>-5.60</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5153327</th>\n",
       "      <td>2018-01-19 20:06:53</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>-5.80</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001180</th>\n",
       "      <td>2018-01-05 16:40:16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>-6.30</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2501492</th>\n",
       "      <td>2018-01-11 05:48:23</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78</td>\n",
       "      <td>3</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>-9.96</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6976476</th>\n",
       "      <td>2018-01-26 01:31:18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.31</td>\n",
       "      <td>4</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-1.56</td>\n",
       "      <td>-9.36</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7213616</th>\n",
       "      <td>2018-01-26 19:49:32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-6.96</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2170556</th>\n",
       "      <td>2018-01-09 22:50:10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-4.94</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4538276</th>\n",
       "      <td>2018-01-17 22:09:09</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-4.94</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2170554</th>\n",
       "      <td>2018-01-09 22:42:11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-4.94</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899912</th>\n",
       "      <td>2018-01-08 22:19:26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48</td>\n",
       "      <td>3</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>-6.36</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118009</th>\n",
       "      <td>2018-01-01 12:23:32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48</td>\n",
       "      <td>3</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-6.30</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4411982</th>\n",
       "      <td>2018-01-17 15:46:31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-4.29</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442944</th>\n",
       "      <td>2018-01-10 21:52:55</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-4.75</td>\n",
       "      <td>21.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108933</th>\n",
       "      <td>2018-01-05 22:14:05</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-4.75</td>\n",
       "      <td>21.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8712023</th>\n",
       "      <td>2018-01-31 21:15:51</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-4.75</td>\n",
       "      <td>21.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8745102</th>\n",
       "      <td>2018-01-31 22:06:23</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-4.75</td>\n",
       "      <td>21.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6691110</th>\n",
       "      <td>2018-01-25 08:57:41</td>\n",
       "      <td>2</td>\n",
       "      <td>0.85</td>\n",
       "      <td>3</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-9.19</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990485</th>\n",
       "      <td>2018-01-26 04:25:38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>-7.11</td>\n",
       "      <td>26.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835199</th>\n",
       "      <td>2018-01-08 18:53:06</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>-7.11</td>\n",
       "      <td>26.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8314157</th>\n",
       "      <td>2018-01-30 17:56:14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>-5.16</td>\n",
       "      <td>26.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750438</th>\n",
       "      <td>2018-01-18 16:35:04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>-7.11</td>\n",
       "      <td>26.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5240311</th>\n",
       "      <td>2018-01-20 01:07:12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.32</td>\n",
       "      <td>3</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>-6.10</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920250</th>\n",
       "      <td>2018-01-19 04:28:28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-4.56</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738533</th>\n",
       "      <td>2018-01-08 11:00:35</td>\n",
       "      <td>1</td>\n",
       "      <td>24.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155.00</td>\n",
       "      <td>161.06</td>\n",
       "      <td>8759845.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4013056</th>\n",
       "      <td>2018-01-16 08:03:27</td>\n",
       "      <td>2</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>155.00</td>\n",
       "      <td>166.80</td>\n",
       "      <td>8759845.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8602534</th>\n",
       "      <td>2018-01-31 15:24:21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>156.08</td>\n",
       "      <td>159.38</td>\n",
       "      <td>8759847.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6863262</th>\n",
       "      <td>2018-01-25 18:48:37</td>\n",
       "      <td>1</td>\n",
       "      <td>23.30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>158.00</td>\n",
       "      <td>164.06</td>\n",
       "      <td>8759848.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7631492</th>\n",
       "      <td>2018-01-28 01:05:38</td>\n",
       "      <td>6</td>\n",
       "      <td>16.52</td>\n",
       "      <td>1</td>\n",
       "      <td>51.0</td>\n",
       "      <td>159.00</td>\n",
       "      <td>211.30</td>\n",
       "      <td>8759849.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804957</th>\n",
       "      <td>2018-01-04 13:36:54</td>\n",
       "      <td>2</td>\n",
       "      <td>34.40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.00</td>\n",
       "      <td>160.30</td>\n",
       "      <td>8759850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867102</th>\n",
       "      <td>2018-01-05 00:03:49</td>\n",
       "      <td>2</td>\n",
       "      <td>37.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.00</td>\n",
       "      <td>196.56</td>\n",
       "      <td>8759852.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3786461</th>\n",
       "      <td>2018-01-15 11:29:02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>180.00</td>\n",
       "      <td>183.30</td>\n",
       "      <td>8759852.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2913546</th>\n",
       "      <td>2018-01-12 13:24:06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>180.00</td>\n",
       "      <td>183.30</td>\n",
       "      <td>8759852.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6178664</th>\n",
       "      <td>2018-01-23 15:09:32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>188.00</td>\n",
       "      <td>191.30</td>\n",
       "      <td>8759854.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745839</th>\n",
       "      <td>2018-01-04 00:27:03</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>190.00</td>\n",
       "      <td>242.80</td>\n",
       "      <td>8759855.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55838</th>\n",
       "      <td>2018-01-01 03:36:54</td>\n",
       "      <td>1</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>191.70</td>\n",
       "      <td>205.50</td>\n",
       "      <td>8759856.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8431606</th>\n",
       "      <td>2018-01-30 23:32:52</td>\n",
       "      <td>4</td>\n",
       "      <td>6.56</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>200.00</td>\n",
       "      <td>223.30</td>\n",
       "      <td>8759859.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3404844</th>\n",
       "      <td>2018-01-13 22:13:06</td>\n",
       "      <td>1</td>\n",
       "      <td>2.72</td>\n",
       "      <td>1</td>\n",
       "      <td>15.5</td>\n",
       "      <td>200.00</td>\n",
       "      <td>216.80</td>\n",
       "      <td>8759859.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320814</th>\n",
       "      <td>2018-01-02 12:23:46</td>\n",
       "      <td>2</td>\n",
       "      <td>2.73</td>\n",
       "      <td>1</td>\n",
       "      <td>11.5</td>\n",
       "      <td>200.00</td>\n",
       "      <td>212.30</td>\n",
       "      <td>8759859.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787384</th>\n",
       "      <td>2018-01-04 10:21:04</td>\n",
       "      <td>3</td>\n",
       "      <td>18.48</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>200.00</td>\n",
       "      <td>258.56</td>\n",
       "      <td>8759859.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4130895</th>\n",
       "      <td>2018-01-16 15:20:43</td>\n",
       "      <td>2</td>\n",
       "      <td>42.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.00</td>\n",
       "      <td>227.32</td>\n",
       "      <td>8759859.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135399</th>\n",
       "      <td>2018-01-13 02:56:07</td>\n",
       "      <td>1</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>200.00</td>\n",
       "      <td>209.80</td>\n",
       "      <td>8759859.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8146086</th>\n",
       "      <td>2018-01-30 02:25:55</td>\n",
       "      <td>1</td>\n",
       "      <td>2.78</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>220.00</td>\n",
       "      <td>222.30</td>\n",
       "      <td>8759863.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511358</th>\n",
       "      <td>2018-01-11 07:55:52</td>\n",
       "      <td>1</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>220.00</td>\n",
       "      <td>230.30</td>\n",
       "      <td>8759863.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166456</th>\n",
       "      <td>2018-01-06 05:27:39</td>\n",
       "      <td>1</td>\n",
       "      <td>4.40</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>230.00</td>\n",
       "      <td>282.80</td>\n",
       "      <td>8759865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3911171</th>\n",
       "      <td>2018-01-15 19:49:21</td>\n",
       "      <td>2</td>\n",
       "      <td>2.84</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>250.00</td>\n",
       "      <td>263.80</td>\n",
       "      <td>8759866.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012265</th>\n",
       "      <td>2018-01-05 16:57:55</td>\n",
       "      <td>1</td>\n",
       "      <td>57.30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>260.00</td>\n",
       "      <td>260.30</td>\n",
       "      <td>8759867.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3288331</th>\n",
       "      <td>2018-01-13 16:11:50</td>\n",
       "      <td>1</td>\n",
       "      <td>7.98</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>260.00</td>\n",
       "      <td>301.80</td>\n",
       "      <td>8759867.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301773</th>\n",
       "      <td>2018-01-02 10:30:23</td>\n",
       "      <td>1</td>\n",
       "      <td>53.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.00</td>\n",
       "      <td>320.80</td>\n",
       "      <td>8759869.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8368600</th>\n",
       "      <td>2018-01-30 19:08:57</td>\n",
       "      <td>5</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>330.00</td>\n",
       "      <td>346.80</td>\n",
       "      <td>8759870.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072357</th>\n",
       "      <td>2018-01-09 16:53:51</td>\n",
       "      <td>1</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>355.00</td>\n",
       "      <td>378.80</td>\n",
       "      <td>8759871.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5452107</th>\n",
       "      <td>2018-01-20 19:27:09</td>\n",
       "      <td>6</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>411.00</td>\n",
       "      <td>418.80</td>\n",
       "      <td>8759872.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4613248</th>\n",
       "      <td>2018-01-18 08:51:34</td>\n",
       "      <td>6</td>\n",
       "      <td>12.99</td>\n",
       "      <td>1</td>\n",
       "      <td>54.5</td>\n",
       "      <td>415.00</td>\n",
       "      <td>485.04</td>\n",
       "      <td>8759873.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4544998</th>\n",
       "      <td>2018-01-17 22:52:21</td>\n",
       "      <td>1</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>441.71</td>\n",
       "      <td>449.01</td>\n",
       "      <td>8759874.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8759874 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tpep_pickup_datetime  passenger_count  trip_distance  payment_type  \\\n",
       "5300576  2018-01-20 10:18:32                1           0.81             3   \n",
       "7849715  2018-01-28 21:34:35                2           0.00             3   \n",
       "6210621  2018-01-23 17:18:30                1           0.36             4   \n",
       "3575694  2018-01-14 14:25:45                5           0.75             4   \n",
       "2562900  2018-01-11 10:21:16                1           0.68             3   \n",
       "5555849  2018-01-21 02:01:48                1           0.00             3   \n",
       "2210176  2018-01-10 06:59:44                1           0.00             4   \n",
       "3912075  2018-01-15 19:43:33                1           0.00             3   \n",
       "5153327  2018-01-19 20:06:53                1           0.00             3   \n",
       "1001180  2018-01-05 16:40:16                1           0.00             3   \n",
       "2501492  2018-01-11 05:48:23                1           0.78             3   \n",
       "6976476  2018-01-26 01:31:18                2           0.31             4   \n",
       "7213616  2018-01-26 19:49:32                2           0.25             3   \n",
       "2170556  2018-01-09 22:50:10                1           0.00             3   \n",
       "4538276  2018-01-17 22:09:09                1           0.00             3   \n",
       "2170554  2018-01-09 22:42:11                1           0.00             4   \n",
       "1899912  2018-01-08 22:19:26                1           0.48             3   \n",
       "118009   2018-01-01 12:23:32                1           0.48             3   \n",
       "4411982  2018-01-17 15:46:31                1           0.00             3   \n",
       "2442944  2018-01-10 21:52:55                1           0.00             3   \n",
       "1108933  2018-01-05 22:14:05                1           0.00             3   \n",
       "8712023  2018-01-31 21:15:51                1           0.00             3   \n",
       "8745102  2018-01-31 22:06:23                1           0.00             3   \n",
       "6691110  2018-01-25 08:57:41                2           0.85             3   \n",
       "6990485  2018-01-26 04:25:38                1           0.00             3   \n",
       "1835199  2018-01-08 18:53:06                2           0.00             3   \n",
       "8314157  2018-01-30 17:56:14                1           0.00             3   \n",
       "4750438  2018-01-18 16:35:04                5           0.02             4   \n",
       "5240311  2018-01-20 01:07:12                1           0.32             3   \n",
       "4920250  2018-01-19 04:28:28                1           0.03             3   \n",
       "...                      ...              ...            ...           ...   \n",
       "1738533  2018-01-08 11:00:35                1          24.80             1   \n",
       "4013056  2018-01-16 08:03:27                2           1.60             1   \n",
       "8602534  2018-01-31 15:24:21                1           0.00             1   \n",
       "6863262  2018-01-25 18:48:37                1          23.30             1   \n",
       "7631492  2018-01-28 01:05:38                6          16.52             1   \n",
       "804957   2018-01-04 13:36:54                2          34.40             1   \n",
       "867102   2018-01-05 00:03:49                2          37.10             1   \n",
       "3786461  2018-01-15 11:29:02                1           0.00             1   \n",
       "2913546  2018-01-12 13:24:06                1           0.00             1   \n",
       "6178664  2018-01-23 15:09:32                1           0.00             1   \n",
       "745839   2018-01-04 00:27:03                1           0.00             1   \n",
       "55838    2018-01-01 03:36:54                1           3.25             1   \n",
       "8431606  2018-01-30 23:32:52                4           6.56             1   \n",
       "3404844  2018-01-13 22:13:06                1           2.72             1   \n",
       "320814   2018-01-02 12:23:46                2           2.73             1   \n",
       "787384   2018-01-04 10:21:04                3          18.48             1   \n",
       "4130895  2018-01-16 15:20:43                2          42.80             1   \n",
       "3135399  2018-01-13 02:56:07                1           1.39             1   \n",
       "8146086  2018-01-30 02:25:55                1           2.78             1   \n",
       "2511358  2018-01-11 07:55:52                1           1.61             1   \n",
       "1166456  2018-01-06 05:27:39                1           4.40             1   \n",
       "3911171  2018-01-15 19:49:21                2           2.84             1   \n",
       "1012265  2018-01-05 16:57:55                1          57.30             1   \n",
       "3288331  2018-01-13 16:11:50                1           7.98             1   \n",
       "301773   2018-01-02 10:30:23                1          53.80             1   \n",
       "8368600  2018-01-30 19:08:57                5           2.69             1   \n",
       "2072357  2018-01-09 16:53:51                1           6.25             1   \n",
       "5452107  2018-01-20 19:27:09                6           1.35             1   \n",
       "4613248  2018-01-18 08:51:34                6          12.99             1   \n",
       "4544998  2018-01-17 22:52:21                1           1.13             1   \n",
       "\n",
       "         fare_amount  tip_amount  total_amount       Rank  \n",
       "5300576         -4.5      -88.80        -94.10        1.0  \n",
       "7849715        -65.0      -13.06        -78.36        2.0  \n",
       "6210621         -4.0      -11.00        -16.80        3.0  \n",
       "3575694         -4.5      -10.00        -15.30        4.0  \n",
       "2562900        -52.0       -7.92        -60.72        5.0  \n",
       "5555849        -13.0       -3.32        -16.62        6.0  \n",
       "2210176         -2.5       -3.00         -6.30        7.0  \n",
       "3912075         -2.5       -2.30         -5.60        8.0  \n",
       "5153327         -2.5       -2.00         -5.80        9.5  \n",
       "1001180         -2.5       -2.00         -6.30        9.5  \n",
       "2501492         -7.0       -1.66         -9.96       11.0  \n",
       "6976476         -6.5       -1.56         -9.36       12.0  \n",
       "7213616         -4.0       -1.16         -6.96       13.0  \n",
       "2170556         -2.5       -1.14         -4.94       15.0  \n",
       "4538276         -2.5       -1.14         -4.94       15.0  \n",
       "2170554         -2.5       -1.14         -4.94       15.0  \n",
       "1899912         -4.0       -1.06         -6.36       17.0  \n",
       "118009          -4.5       -1.00         -6.30       18.0  \n",
       "4411982         -2.5       -0.99         -4.29       19.0  \n",
       "2442944         -2.5       -0.95         -4.75       21.5  \n",
       "1108933         -2.5       -0.95         -4.75       21.5  \n",
       "8712023         -2.5       -0.95         -4.75       21.5  \n",
       "8745102         -2.5       -0.95         -4.75       21.5  \n",
       "6691110         -5.5       -0.94         -9.19       24.0  \n",
       "6990485         -3.0       -0.86         -7.11       26.5  \n",
       "1835199         -2.5       -0.86         -7.11       26.5  \n",
       "8314157         -2.5       -0.86         -5.16       26.5  \n",
       "4750438         -2.5       -0.86         -7.11       26.5  \n",
       "5240311         -4.0       -0.80         -6.10       29.0  \n",
       "4920250         -2.5       -0.76         -4.56       31.0  \n",
       "...              ...         ...           ...        ...  \n",
       "1738533          0.0      155.00        161.06  8759845.5  \n",
       "4013056         11.0      155.00        166.80  8759845.5  \n",
       "8602534          2.5      156.08        159.38  8759847.0  \n",
       "6863262          0.0      158.00        164.06  8759848.0  \n",
       "7631492         51.0      159.00        211.30  8759849.0  \n",
       "804957           0.0      160.00        160.30  8759850.0  \n",
       "867102           0.0      180.00        196.56  8759852.0  \n",
       "3786461          2.5      180.00        183.30  8759852.0  \n",
       "2913546          2.5      180.00        183.30  8759852.0  \n",
       "6178664          2.5      188.00        191.30  8759854.0  \n",
       "745839          52.0      190.00        242.80  8759855.0  \n",
       "55838           12.5      191.70        205.50  8759856.0  \n",
       "8431606         22.0      200.00        223.30  8759859.5  \n",
       "3404844         15.5      200.00        216.80  8759859.5  \n",
       "320814          11.5      200.00        212.30  8759859.5  \n",
       "787384          52.0      200.00        258.56  8759859.5  \n",
       "4130895          0.0      200.00        227.32  8759859.5  \n",
       "3135399          8.5      200.00        209.80  8759859.5  \n",
       "8146086          2.0      220.00        222.30  8759863.5  \n",
       "2511358          9.5      220.00        230.30  8759863.5  \n",
       "1166456         52.0      230.00        282.80  8759865.0  \n",
       "3911171         13.0      250.00        263.80  8759866.0  \n",
       "1012265          0.0      260.00        260.30  8759867.5  \n",
       "3288331         41.0      260.00        301.80  8759867.5  \n",
       "301773           0.0      310.00        320.80  8759869.0  \n",
       "8368600         15.0      330.00        346.80  8759870.0  \n",
       "2072357         22.0      355.00        378.80  8759871.0  \n",
       "5452107          7.0      411.00        418.80  8759872.0  \n",
       "4613248         54.5      415.00        485.04  8759873.0  \n",
       "4544998          6.0      441.71        449.01  8759874.0  \n",
       "\n",
       "[8759874 rows x 8 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using January data as an example to rank the df base on tip amount, and we found that there are negative numbers, which does not make sense\n",
    "data1[\"Rank\"] = data1[\"tip_amount\"].rank()\n",
    "data1.sort_values(\"tip_amount\", inplace = True) #reordered: least on top\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we also deleted some data that does not make sense, including negative fare amount, total amount, the number of passengers and trip distance. In this way, our dataset became more reasonable and realistic, so that our model later could be more accurate and significant for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows w negative tip_amount\n",
    "\n",
    "data1 = data1[(data1['fare_amount']>0)]  #delete negative rows\n",
    "data1 = data1[(data1['tip_amount']>=0)] \n",
    "data1 = data1[(data1['total_amount']>0)] \n",
    "data1 = data1[(data1['passenger_count']>0)] \n",
    "data1 = data1[(data1['trip_distance']>0)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.641672e+06</td>\n",
       "      <td>8.641672e+06</td>\n",
       "      <td>8.641672e+06</td>\n",
       "      <td>8.641672e+06</td>\n",
       "      <td>8.641672e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.619278e+00</td>\n",
       "      <td>2.823534e+00</td>\n",
       "      <td>1.219387e+01</td>\n",
       "      <td>1.813055e+00</td>\n",
       "      <td>1.543502e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.257139e+00</td>\n",
       "      <td>6.455868e+01</td>\n",
       "      <td>1.108265e+01</td>\n",
       "      <td>2.420130e+00</td>\n",
       "      <td>1.363849e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.100000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.300000e-01</td>\n",
       "      <td>6.500000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.300000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.570000e+00</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>1.400000e+00</td>\n",
       "      <td>1.130000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.870000e+00</td>\n",
       "      <td>1.350000e+01</td>\n",
       "      <td>2.350000e+00</td>\n",
       "      <td>1.662000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>1.894838e+05</td>\n",
       "      <td>8.016000e+03</td>\n",
       "      <td>4.417100e+02</td>\n",
       "      <td>8.016800e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       passenger_count  trip_distance   fare_amount    tip_amount  \\\n",
       "count     8.641672e+06   8.641672e+06  8.641672e+06  8.641672e+06   \n",
       "mean      1.619278e+00   2.823534e+00  1.219387e+01  1.813055e+00   \n",
       "std       1.257139e+00   6.455868e+01  1.108265e+01  2.420130e+00   \n",
       "min       1.000000e+00   1.000000e-02  1.000000e-02  0.000000e+00   \n",
       "25%       1.000000e+00   9.300000e-01  6.500000e+00  0.000000e+00   \n",
       "50%       1.000000e+00   1.570000e+00  9.000000e+00  1.400000e+00   \n",
       "75%       2.000000e+00   2.870000e+00  1.350000e+01  2.350000e+00   \n",
       "max       9.000000e+00   1.894838e+05  8.016000e+03  4.417100e+02   \n",
       "\n",
       "       total_amount  \n",
       "count  8.641672e+06  \n",
       "mean   1.543502e+01  \n",
       "std    1.363849e+01  \n",
       "min    3.100000e-01  \n",
       "25%    8.300000e+00  \n",
       "50%    1.130000e+01  \n",
       "75%    1.662000e+01  \n",
       "max    8.016800e+03  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['tip_percent'] = data1['tip_amount']/data1['fare_amount']\n",
    "data1.sort_values(\"tip_percent\", inplace = True)\n",
    "data1[['passenger_count','trip_distance','fare_amount','tip_amount','total_amount']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>Rank</th>\n",
       "      <th>tip_percent</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2469978</th>\n",
       "      <td>2018-01-10 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>22.28</td>\n",
       "      <td>2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>58.56</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158535</th>\n",
       "      <td>2018-01-09 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>18.49</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7.70</td>\n",
       "      <td>59.00</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7623170</th>\n",
       "      <td>2018-01-28 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>15.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485944</th>\n",
       "      <td>2018-01-07 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789112</th>\n",
       "      <td>2018-01-25 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.22</td>\n",
       "      <td>17.02</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.158571</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182801</th>\n",
       "      <td>2018-01-06 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1.70</td>\n",
       "      <td>13.00</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.161905</td>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913056</th>\n",
       "      <td>2018-01-09 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.49</td>\n",
       "      <td>10.79</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.355714</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331252</th>\n",
       "      <td>2018-01-20 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5369427</th>\n",
       "      <td>2018-01-20 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744761</th>\n",
       "      <td>2018-01-04 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2.59</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.26</td>\n",
       "      <td>13.56</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3442553</th>\n",
       "      <td>2018-01-14 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719140</th>\n",
       "      <td>2018-01-15 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1.55</td>\n",
       "      <td>9.35</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443732</th>\n",
       "      <td>2018-01-31 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.16</td>\n",
       "      <td>12.96</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.227368</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356360</th>\n",
       "      <td>2018-01-02 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>14.57</td>\n",
       "      <td>2</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>54.56</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114303</th>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3.47</td>\n",
       "      <td>1</td>\n",
       "      <td>16.5</td>\n",
       "      <td>2.67</td>\n",
       "      <td>20.47</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.161818</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930677</th>\n",
       "      <td>2018-01-12 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2.97</td>\n",
       "      <td>1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>2.30</td>\n",
       "      <td>17.60</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.158621</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195509</th>\n",
       "      <td>2018-01-10 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622750</th>\n",
       "      <td>2018-01-21 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>7.42</td>\n",
       "      <td>1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5222268</th>\n",
       "      <td>2018-01-20 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1.09</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644820</th>\n",
       "      <td>2018-01-03 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5048098</th>\n",
       "      <td>2018-01-19 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>10.71</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9.71</td>\n",
       "      <td>58.27</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.231190</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7591736</th>\n",
       "      <td>2018-01-27 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>13.83</td>\n",
       "      <td>2</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-27</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896706</th>\n",
       "      <td>2018-01-19 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4.54</td>\n",
       "      <td>2</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860665</th>\n",
       "      <td>2018-01-12 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>11.49</td>\n",
       "      <td>2</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>45.06</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364403</th>\n",
       "      <td>2018-01-06 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>5.64</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191873</th>\n",
       "      <td>2018-01-09 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4966341</th>\n",
       "      <td>2018-01-19 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635147</th>\n",
       "      <td>2018-01-11 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969136</th>\n",
       "      <td>2018-01-12 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648097</th>\n",
       "      <td>2018-01-11 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>17.64</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475510</th>\n",
       "      <td>2018-01-02 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>2.96</td>\n",
       "      <td>17.76</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.219259</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865091</th>\n",
       "      <td>2018-01-04 23:59:59</td>\n",
       "      <td>3</td>\n",
       "      <td>8.57</td>\n",
       "      <td>1</td>\n",
       "      <td>29.5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>40.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3102731</th>\n",
       "      <td>2018-01-12 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>2.95</td>\n",
       "      <td>17.75</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.218519</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4885138</th>\n",
       "      <td>2018-01-18 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7314718</th>\n",
       "      <td>2018-01-26 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.81</td>\n",
       "      <td>1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>3.96</td>\n",
       "      <td>23.76</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.214054</td>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3723325</th>\n",
       "      <td>2018-01-14 23:59:59</td>\n",
       "      <td>2</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>13.11</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597246</th>\n",
       "      <td>2018-01-27 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>4.20</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-27</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527651</th>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>11.16</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6641081</th>\n",
       "      <td>2018-01-24 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.46</td>\n",
       "      <td>8.76</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.243333</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743708</th>\n",
       "      <td>2018-01-03 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.08</td>\n",
       "      <td>15.38</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525960</th>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>11.16</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6622950</th>\n",
       "      <td>2018-01-24 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.15</td>\n",
       "      <td>12.95</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.226316</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6975451</th>\n",
       "      <td>2018-01-25 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185037</th>\n",
       "      <td>2018-01-09 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1</td>\n",
       "      <td>19.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>24.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190264</th>\n",
       "      <td>2018-01-09 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>11.60</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5510839</th>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>4.41</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127189</th>\n",
       "      <td>2018-01-05 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1</td>\n",
       "      <td>10.5</td>\n",
       "      <td>2.35</td>\n",
       "      <td>14.15</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.223810</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963249</th>\n",
       "      <td>2018-01-15 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>16.55</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198236</th>\n",
       "      <td>2018-01-09 23:59:59</td>\n",
       "      <td>6</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472404</th>\n",
       "      <td>2018-01-02 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223740</th>\n",
       "      <td>2018-01-19 23:59:59</td>\n",
       "      <td>2</td>\n",
       "      <td>4.61</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.61</td>\n",
       "      <td>27.67</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.288125</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316979</th>\n",
       "      <td>2018-01-23 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>62.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4565660</th>\n",
       "      <td>2018-01-17 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>8.10</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867497</th>\n",
       "      <td>2018-01-04 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788713</th>\n",
       "      <td>2018-01-11 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214201</th>\n",
       "      <td>2018-01-19 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1.55</td>\n",
       "      <td>9.35</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6623570</th>\n",
       "      <td>2018-01-24 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>3.95</td>\n",
       "      <td>23.75</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.213514</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6973117</th>\n",
       "      <td>2018-01-25 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>14.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427267</th>\n",
       "      <td>2018-01-13 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504232</th>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>2</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>9.95</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.235714</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8641672 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tpep_pickup_datetime  passenger_count  trip_distance  payment_type  \\\n",
       "2469978  2018-01-10 00:00:00                1          22.28             2   \n",
       "2158535  2018-01-09 00:00:00                1          18.49             1   \n",
       "7623170  2018-01-28 00:00:00                1           1.91             1   \n",
       "1485944  2018-01-07 00:00:00                1           1.06             2   \n",
       "6789112  2018-01-25 00:00:00                5           2.80             1   \n",
       "1182801  2018-01-06 00:00:00                2           2.80             1   \n",
       "1913056  2018-01-09 00:00:00                1           1.30             1   \n",
       "5331252  2018-01-20 00:00:00                2           1.69             2   \n",
       "5369427  2018-01-20 00:00:00                1           6.50             2   \n",
       "744761   2018-01-04 00:00:00                2           2.59             1   \n",
       "3442553  2018-01-14 00:00:00                1           0.91             2   \n",
       "3719140  2018-01-15 00:00:00                1           0.80             1   \n",
       "8443732  2018-01-31 00:00:00                1           1.86             1   \n",
       "356360   2018-01-02 00:00:00                1          14.57             2   \n",
       "1114303  2018-01-05 00:00:00                1           3.47             1   \n",
       "2930677  2018-01-12 00:00:00                1           2.97             1   \n",
       "2195509  2018-01-10 00:00:00                3           3.59             2   \n",
       "5622750  2018-01-21 00:00:00                1           7.42             1   \n",
       "5222268  2018-01-20 00:00:00                5           1.09             2   \n",
       "644820   2018-01-03 00:00:00                2           1.53             2   \n",
       "5048098  2018-01-19 00:00:00                6          10.71             1   \n",
       "7591736  2018-01-27 00:00:00                1          13.83             2   \n",
       "4896706  2018-01-19 00:00:00                1           4.54             2   \n",
       "2860665  2018-01-12 00:00:00                1          11.49             2   \n",
       "1364403  2018-01-06 00:00:00                1           5.64             2   \n",
       "2191873  2018-01-09 00:00:00                5           1.25             2   \n",
       "4966341  2018-01-19 00:00:00                1           1.42             2   \n",
       "2635147  2018-01-11 00:00:00                1           3.45             2   \n",
       "2969136  2018-01-12 00:00:00                1           5.00             2   \n",
       "2648097  2018-01-11 00:00:00                6           2.00             1   \n",
       "...                      ...              ...            ...           ...   \n",
       "475510   2018-01-02 23:59:59                1           3.51             1   \n",
       "865091   2018-01-04 23:59:59                3           8.57             1   \n",
       "3102731  2018-01-12 23:59:59                1           2.80             1   \n",
       "4885138  2018-01-18 23:59:59                1           0.80             2   \n",
       "7314718  2018-01-26 23:59:59                1           3.81             1   \n",
       "3723325  2018-01-14 23:59:59                2           1.91             1   \n",
       "7597246  2018-01-27 23:59:59                1           4.20             2   \n",
       "5527651  2018-01-20 23:59:59                1           1.61             1   \n",
       "6641081  2018-01-24 23:59:59                1           1.38             1   \n",
       "743708   2018-01-03 23:59:59                1           3.25             1   \n",
       "5525960  2018-01-20 23:59:59                1           1.67             1   \n",
       "6622950  2018-01-24 23:59:59                1           2.00             1   \n",
       "6975451  2018-01-25 23:59:59                1           0.80             2   \n",
       "2185037  2018-01-09 23:59:59                1           4.70             1   \n",
       "2190264  2018-01-09 23:59:59                1           1.40             1   \n",
       "5510839  2018-01-20 23:59:59                1           4.41             1   \n",
       "1127189  2018-01-05 23:59:59                1           1.50             1   \n",
       "3963249  2018-01-15 23:59:59                1           3.20             1   \n",
       "2198236  2018-01-09 23:59:59                6           0.82             2   \n",
       "472404   2018-01-02 23:59:59                1           1.15             1   \n",
       "5223740  2018-01-19 23:59:59                2           4.61             1   \n",
       "6316979  2018-01-23 23:59:59                1          17.00             1   \n",
       "4565660  2018-01-17 23:59:59                1           8.10             2   \n",
       "867497   2018-01-04 23:59:59                1           2.08             2   \n",
       "2788713  2018-01-11 23:59:59                1           2.15             2   \n",
       "5214201  2018-01-19 23:59:59                1           1.00             1   \n",
       "6623570  2018-01-24 23:59:59                1           5.50             1   \n",
       "6973117  2018-01-25 23:59:59                1           1.53             1   \n",
       "3427267  2018-01-13 23:59:59                1           0.80             2   \n",
       "5504232  2018-01-20 23:59:59                2           1.30             1   \n",
       "\n",
       "         fare_amount  tip_amount  total_amount       Rank  tip_percent  \\\n",
       "2469978         52.0        0.00         58.56       63.5     0.000000   \n",
       "2158535         50.0        7.70         59.00       63.5     0.154000   \n",
       "7623170         11.0        3.00         15.30       63.5     0.272727   \n",
       "1485944          6.5        0.00          7.30       63.5     0.000000   \n",
       "6789112         14.0        2.22         17.02       63.5     0.158571   \n",
       "1182801         10.5        1.70         13.00       63.5     0.161905   \n",
       "1913056          7.0        2.49         10.79       63.5     0.355714   \n",
       "5331252          9.0        0.00          9.80       63.5     0.000000   \n",
       "5369427         19.5        0.00         20.30       63.5     0.000000   \n",
       "744761          10.0        2.26         13.56       63.5     0.226000   \n",
       "3442553          7.0        0.00          8.30       63.5     0.000000   \n",
       "3719140          6.5        1.55          9.35       63.5     0.238462   \n",
       "8443732          9.5        2.16         12.96       63.5     0.227368   \n",
       "356360          48.0        0.00         54.56       63.5     0.000000   \n",
       "1114303         16.5        2.67         20.47       63.5     0.161818   \n",
       "2930677         14.5        2.30         17.60       63.5     0.158621   \n",
       "2195509         15.0        0.00         16.30       63.5     0.000000   \n",
       "5622750         24.5        0.00         25.30       63.5     0.000000   \n",
       "5222268          6.0        0.00          7.30       63.5     0.000000   \n",
       "644820          10.5        0.00         12.30       63.5     0.000000   \n",
       "5048098         42.0        9.71         58.27       63.5     0.231190   \n",
       "7591736         38.5        0.00         39.80       63.5     0.000000   \n",
       "4896706         15.5        0.00         16.80       63.5     0.000000   \n",
       "2860665         38.5        0.00         45.06       63.5     0.000000   \n",
       "1364403         20.0        0.00         21.30       63.5     0.000000   \n",
       "2191873          8.5        0.00          9.80       63.5     0.000000   \n",
       "4966341         11.5        0.00         12.30       63.5     0.000000   \n",
       "2635147         12.0        0.00         12.80       63.5     0.000000   \n",
       "2969136         23.0        0.00         24.80       63.5     0.000000   \n",
       "2648097         15.0        0.84         17.64       63.5     0.056000   \n",
       "...              ...         ...           ...        ...          ...   \n",
       "475510          13.5        2.96         17.76  8641628.0     0.219259   \n",
       "865091          29.5       10.00         40.80  8641628.0     0.338983   \n",
       "3102731         13.5        2.95         17.75  8641628.0     0.218519   \n",
       "4885138          5.0        0.00          6.30  8641628.0     0.000000   \n",
       "7314718         18.5        3.96         23.76  8641628.0     0.214054   \n",
       "3723325          8.0        1.86         13.11  8641628.0     0.232500   \n",
       "7597246         17.0        0.00         18.30  8641628.0     0.000000   \n",
       "5527651          8.0        1.86         11.16  8641628.0     0.232500   \n",
       "6641081          6.0        1.46          8.76  8641628.0     0.243333   \n",
       "743708          11.0        3.08         15.38  8641628.0     0.280000   \n",
       "5525960          8.0        1.86         11.16  8641628.0     0.232500   \n",
       "6622950          9.5        2.15         12.95  8641628.0     0.226316   \n",
       "6975451          5.0        0.00          6.30  8641628.0     0.000000   \n",
       "2185037         19.5        4.00         24.80  8641628.0     0.205128   \n",
       "2190264          8.0        2.30         11.60  8641628.0     0.287500   \n",
       "5510839         17.0        0.00         18.30  8641628.0     0.000000   \n",
       "1127189         10.5        2.35         14.15  8641628.0     0.223810   \n",
       "3963249         12.5        2.75         16.55  8641628.0     0.220000   \n",
       "2198236          4.5        0.00          5.80  8641628.0     0.000000   \n",
       "472404           7.5        1.00          9.80  8641628.0     0.133333   \n",
       "5223740         16.0        4.61         27.67  8641628.0     0.288125   \n",
       "6316979         52.0       10.00         62.80  8641628.0     0.192308   \n",
       "4565660         25.0        0.00         26.30  8641628.0     0.000000   \n",
       "867497           9.0        0.00         10.30  8641628.0     0.000000   \n",
       "2788713         10.0        0.00         11.30  8641628.0     0.000000   \n",
       "5214201          6.5        1.55          9.35  8641628.0     0.238462   \n",
       "6623570         18.5        3.95         23.75  8641628.0     0.213514   \n",
       "6973117          8.0        5.00         14.30  8641628.0     0.625000   \n",
       "3427267          6.0        0.00          7.30  8641628.0     0.000000   \n",
       "5504232          7.0        1.65          9.95  8641628.0     0.235714   \n",
       "\n",
       "               date      time  \n",
       "2469978  2018-01-10  00:00:00  \n",
       "2158535  2018-01-09  00:00:00  \n",
       "7623170  2018-01-28  00:00:00  \n",
       "1485944  2018-01-07  00:00:00  \n",
       "6789112  2018-01-25  00:00:00  \n",
       "1182801  2018-01-06  00:00:00  \n",
       "1913056  2018-01-09  00:00:00  \n",
       "5331252  2018-01-20  00:00:00  \n",
       "5369427  2018-01-20  00:00:00  \n",
       "744761   2018-01-04  00:00:00  \n",
       "3442553  2018-01-14  00:00:00  \n",
       "3719140  2018-01-15  00:00:00  \n",
       "8443732  2018-01-31  00:00:00  \n",
       "356360   2018-01-02  00:00:00  \n",
       "1114303  2018-01-05  00:00:00  \n",
       "2930677  2018-01-12  00:00:00  \n",
       "2195509  2018-01-10  00:00:00  \n",
       "5622750  2018-01-21  00:00:00  \n",
       "5222268  2018-01-20  00:00:00  \n",
       "644820   2018-01-03  00:00:00  \n",
       "5048098  2018-01-19  00:00:00  \n",
       "7591736  2018-01-27  00:00:00  \n",
       "4896706  2018-01-19  00:00:00  \n",
       "2860665  2018-01-12  00:00:00  \n",
       "1364403  2018-01-06  00:00:00  \n",
       "2191873  2018-01-09  00:00:00  \n",
       "4966341  2018-01-19  00:00:00  \n",
       "2635147  2018-01-11  00:00:00  \n",
       "2969136  2018-01-12  00:00:00  \n",
       "2648097  2018-01-11  00:00:00  \n",
       "...             ...       ...  \n",
       "475510   2018-01-02  23:59:59  \n",
       "865091   2018-01-04  23:59:59  \n",
       "3102731  2018-01-12  23:59:59  \n",
       "4885138  2018-01-18  23:59:59  \n",
       "7314718  2018-01-26  23:59:59  \n",
       "3723325  2018-01-14  23:59:59  \n",
       "7597246  2018-01-27  23:59:59  \n",
       "5527651  2018-01-20  23:59:59  \n",
       "6641081  2018-01-24  23:59:59  \n",
       "743708   2018-01-03  23:59:59  \n",
       "5525960  2018-01-20  23:59:59  \n",
       "6622950  2018-01-24  23:59:59  \n",
       "6975451  2018-01-25  23:59:59  \n",
       "2185037  2018-01-09  23:59:59  \n",
       "2190264  2018-01-09  23:59:59  \n",
       "5510839  2018-01-20  23:59:59  \n",
       "1127189  2018-01-05  23:59:59  \n",
       "3963249  2018-01-15  23:59:59  \n",
       "2198236  2018-01-09  23:59:59  \n",
       "472404   2018-01-02  23:59:59  \n",
       "5223740  2018-01-19  23:59:59  \n",
       "6316979  2018-01-23  23:59:59  \n",
       "4565660  2018-01-17  23:59:59  \n",
       "867497   2018-01-04  23:59:59  \n",
       "2788713  2018-01-11  23:59:59  \n",
       "5214201  2018-01-19  23:59:59  \n",
       "6623570  2018-01-24  23:59:59  \n",
       "6973117  2018-01-25  23:59:59  \n",
       "3427267  2018-01-13  23:59:59  \n",
       "5504232  2018-01-20  23:59:59  \n",
       "\n",
       "[8641672 rows x 11 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[\"date\"] = [date[0:10] for date in data1[\"tpep_pickup_datetime\"]]\n",
    "data1[\"time\"] = [date[11:] for date in data1[\"tpep_pickup_datetime\"]]\n",
    "\n",
    "data1[\"Rank\"] = data1[\"time\"].rank() \n",
    "\n",
    "data1.sort_values(\"time\", inplace = True)\n",
    "\n",
    "data1 = data1.dropna()\n",
    "data1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = data1\n",
    "\n",
    "df1['tpep_pickup_datetime'][0][0:10]\n",
    "\n",
    "Jan_01_18 = df1[(df1['tpep_pickup_datetime'].str[0:10] == \"2018-01-01\")]\n",
    "\n",
    "Jan_01_18.plot(x = 'tpep_pickup_datetime', y = 'tip_amount', color = 'blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jan_01_18 = data1[(data1[\"tip_amount\"] > 0) & (data1[\"date\"] == \"2018-01-01\")]\n",
    "\n",
    "\n",
    "\n",
    "jan_01_18.plot(x = \"time\", y = \"total_amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the graph that we made above, we assumed that there might be some differences of the tip amount that the drivers get between driving at daytime and nighttime, since from the graph, the y axis in the middle of the day seems to be lower. Therefore, we set up a new variable ‘DayNight’, with 20:00~05:00 to be the day shift, and 5:00~20:00 to be the night shift. We did this by making a loop and filtering the time variable that we separated from “tpep_pickup_datetime’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DayNight</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>Rank</th>\n",
       "      <th>tip_percent</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2469978</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-10 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>22.28</td>\n",
       "      <td>2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>58.56</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158535</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>18.49</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7.70</td>\n",
       "      <td>59.00</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7623170</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-28 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>15.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485944</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-07 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789112</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-25 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.22</td>\n",
       "      <td>17.02</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.158571</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182801</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-06 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1.70</td>\n",
       "      <td>13.00</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.161905</td>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913056</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.49</td>\n",
       "      <td>10.79</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.355714</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331252</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5369427</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744761</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-04 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2.59</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.26</td>\n",
       "      <td>13.56</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3442553</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-14 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719140</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-15 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1.55</td>\n",
       "      <td>9.35</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443732</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-31 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.16</td>\n",
       "      <td>12.96</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.227368</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356360</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-02 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>14.57</td>\n",
       "      <td>2</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>54.56</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114303</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3.47</td>\n",
       "      <td>1</td>\n",
       "      <td>16.5</td>\n",
       "      <td>2.67</td>\n",
       "      <td>20.47</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.161818</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930677</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-12 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2.97</td>\n",
       "      <td>1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>2.30</td>\n",
       "      <td>17.60</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.158621</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195509</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-10 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622750</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-21 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>7.42</td>\n",
       "      <td>1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5222268</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1.09</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644820</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-03 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5048098</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-19 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>10.71</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9.71</td>\n",
       "      <td>58.27</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.231190</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7591736</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-27 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>13.83</td>\n",
       "      <td>2</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-27</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896706</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-19 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4.54</td>\n",
       "      <td>2</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860665</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-12 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>11.49</td>\n",
       "      <td>2</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>45.06</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364403</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-06 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>5.64</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191873</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4966341</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-19 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635147</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-11 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969136</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-12 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648097</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-11 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>17.64</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475510</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-02 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>2.96</td>\n",
       "      <td>17.76</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.219259</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865091</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-04 23:59:59</td>\n",
       "      <td>3</td>\n",
       "      <td>8.57</td>\n",
       "      <td>1</td>\n",
       "      <td>29.5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>40.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3102731</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-12 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>2.95</td>\n",
       "      <td>17.75</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.218519</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4885138</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-18 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7314718</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-26 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.81</td>\n",
       "      <td>1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>3.96</td>\n",
       "      <td>23.76</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.214054</td>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3723325</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-14 23:59:59</td>\n",
       "      <td>2</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>13.11</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597246</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-27 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>4.20</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-27</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527651</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>11.16</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6641081</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-24 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.46</td>\n",
       "      <td>8.76</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.243333</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743708</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-03 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.08</td>\n",
       "      <td>15.38</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525960</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>11.16</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6622950</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-24 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.15</td>\n",
       "      <td>12.95</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.226316</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6975451</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-25 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185037</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1</td>\n",
       "      <td>19.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>24.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190264</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>11.60</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5510839</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>4.41</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127189</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-05 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1</td>\n",
       "      <td>10.5</td>\n",
       "      <td>2.35</td>\n",
       "      <td>14.15</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.223810</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963249</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-15 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>16.55</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198236</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 23:59:59</td>\n",
       "      <td>6</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472404</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-02 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223740</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-19 23:59:59</td>\n",
       "      <td>2</td>\n",
       "      <td>4.61</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.61</td>\n",
       "      <td>27.67</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.288125</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316979</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-23 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>62.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4565660</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-17 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>8.10</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867497</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-04 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788713</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-11 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214201</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-19 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1.55</td>\n",
       "      <td>9.35</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6623570</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-24 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>3.95</td>\n",
       "      <td>23.75</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.213514</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6973117</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-25 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>14.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427267</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-13 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504232</th>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>2</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>9.95</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.235714</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8641672 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DayNight tpep_pickup_datetime  passenger_count  trip_distance  \\\n",
       "2469978    night  2018-01-10 00:00:00                1          22.28   \n",
       "2158535    night  2018-01-09 00:00:00                1          18.49   \n",
       "7623170    night  2018-01-28 00:00:00                1           1.91   \n",
       "1485944    night  2018-01-07 00:00:00                1           1.06   \n",
       "6789112    night  2018-01-25 00:00:00                5           2.80   \n",
       "1182801    night  2018-01-06 00:00:00                2           2.80   \n",
       "1913056    night  2018-01-09 00:00:00                1           1.30   \n",
       "5331252    night  2018-01-20 00:00:00                2           1.69   \n",
       "5369427    night  2018-01-20 00:00:00                1           6.50   \n",
       "744761     night  2018-01-04 00:00:00                2           2.59   \n",
       "3442553    night  2018-01-14 00:00:00                1           0.91   \n",
       "3719140    night  2018-01-15 00:00:00                1           0.80   \n",
       "8443732    night  2018-01-31 00:00:00                1           1.86   \n",
       "356360     night  2018-01-02 00:00:00                1          14.57   \n",
       "1114303    night  2018-01-05 00:00:00                1           3.47   \n",
       "2930677    night  2018-01-12 00:00:00                1           2.97   \n",
       "2195509    night  2018-01-10 00:00:00                3           3.59   \n",
       "5622750    night  2018-01-21 00:00:00                1           7.42   \n",
       "5222268    night  2018-01-20 00:00:00                5           1.09   \n",
       "644820     night  2018-01-03 00:00:00                2           1.53   \n",
       "5048098    night  2018-01-19 00:00:00                6          10.71   \n",
       "7591736    night  2018-01-27 00:00:00                1          13.83   \n",
       "4896706    night  2018-01-19 00:00:00                1           4.54   \n",
       "2860665    night  2018-01-12 00:00:00                1          11.49   \n",
       "1364403    night  2018-01-06 00:00:00                1           5.64   \n",
       "2191873    night  2018-01-09 00:00:00                5           1.25   \n",
       "4966341    night  2018-01-19 00:00:00                1           1.42   \n",
       "2635147    night  2018-01-11 00:00:00                1           3.45   \n",
       "2969136    night  2018-01-12 00:00:00                1           5.00   \n",
       "2648097    night  2018-01-11 00:00:00                6           2.00   \n",
       "...          ...                  ...              ...            ...   \n",
       "475510     night  2018-01-02 23:59:59                1           3.51   \n",
       "865091     night  2018-01-04 23:59:59                3           8.57   \n",
       "3102731    night  2018-01-12 23:59:59                1           2.80   \n",
       "4885138    night  2018-01-18 23:59:59                1           0.80   \n",
       "7314718    night  2018-01-26 23:59:59                1           3.81   \n",
       "3723325    night  2018-01-14 23:59:59                2           1.91   \n",
       "7597246    night  2018-01-27 23:59:59                1           4.20   \n",
       "5527651    night  2018-01-20 23:59:59                1           1.61   \n",
       "6641081    night  2018-01-24 23:59:59                1           1.38   \n",
       "743708     night  2018-01-03 23:59:59                1           3.25   \n",
       "5525960    night  2018-01-20 23:59:59                1           1.67   \n",
       "6622950    night  2018-01-24 23:59:59                1           2.00   \n",
       "6975451    night  2018-01-25 23:59:59                1           0.80   \n",
       "2185037    night  2018-01-09 23:59:59                1           4.70   \n",
       "2190264    night  2018-01-09 23:59:59                1           1.40   \n",
       "5510839    night  2018-01-20 23:59:59                1           4.41   \n",
       "1127189    night  2018-01-05 23:59:59                1           1.50   \n",
       "3963249    night  2018-01-15 23:59:59                1           3.20   \n",
       "2198236    night  2018-01-09 23:59:59                6           0.82   \n",
       "472404     night  2018-01-02 23:59:59                1           1.15   \n",
       "5223740    night  2018-01-19 23:59:59                2           4.61   \n",
       "6316979    night  2018-01-23 23:59:59                1          17.00   \n",
       "4565660    night  2018-01-17 23:59:59                1           8.10   \n",
       "867497     night  2018-01-04 23:59:59                1           2.08   \n",
       "2788713    night  2018-01-11 23:59:59                1           2.15   \n",
       "5214201    night  2018-01-19 23:59:59                1           1.00   \n",
       "6623570    night  2018-01-24 23:59:59                1           5.50   \n",
       "6973117    night  2018-01-25 23:59:59                1           1.53   \n",
       "3427267    night  2018-01-13 23:59:59                1           0.80   \n",
       "5504232    night  2018-01-20 23:59:59                2           1.30   \n",
       "\n",
       "         payment_type  fare_amount  tip_amount  total_amount       Rank  \\\n",
       "2469978             2         52.0        0.00         58.56       63.5   \n",
       "2158535             1         50.0        7.70         59.00       63.5   \n",
       "7623170             1         11.0        3.00         15.30       63.5   \n",
       "1485944             2          6.5        0.00          7.30       63.5   \n",
       "6789112             1         14.0        2.22         17.02       63.5   \n",
       "1182801             1         10.5        1.70         13.00       63.5   \n",
       "1913056             1          7.0        2.49         10.79       63.5   \n",
       "5331252             2          9.0        0.00          9.80       63.5   \n",
       "5369427             2         19.5        0.00         20.30       63.5   \n",
       "744761              1         10.0        2.26         13.56       63.5   \n",
       "3442553             2          7.0        0.00          8.30       63.5   \n",
       "3719140             1          6.5        1.55          9.35       63.5   \n",
       "8443732             1          9.5        2.16         12.96       63.5   \n",
       "356360              2         48.0        0.00         54.56       63.5   \n",
       "1114303             1         16.5        2.67         20.47       63.5   \n",
       "2930677             1         14.5        2.30         17.60       63.5   \n",
       "2195509             2         15.0        0.00         16.30       63.5   \n",
       "5622750             1         24.5        0.00         25.30       63.5   \n",
       "5222268             2          6.0        0.00          7.30       63.5   \n",
       "644820              2         10.5        0.00         12.30       63.5   \n",
       "5048098             1         42.0        9.71         58.27       63.5   \n",
       "7591736             2         38.5        0.00         39.80       63.5   \n",
       "4896706             2         15.5        0.00         16.80       63.5   \n",
       "2860665             2         38.5        0.00         45.06       63.5   \n",
       "1364403             2         20.0        0.00         21.30       63.5   \n",
       "2191873             2          8.5        0.00          9.80       63.5   \n",
       "4966341             2         11.5        0.00         12.30       63.5   \n",
       "2635147             2         12.0        0.00         12.80       63.5   \n",
       "2969136             2         23.0        0.00         24.80       63.5   \n",
       "2648097             1         15.0        0.84         17.64       63.5   \n",
       "...               ...          ...         ...           ...        ...   \n",
       "475510              1         13.5        2.96         17.76  8641628.0   \n",
       "865091              1         29.5       10.00         40.80  8641628.0   \n",
       "3102731             1         13.5        2.95         17.75  8641628.0   \n",
       "4885138             2          5.0        0.00          6.30  8641628.0   \n",
       "7314718             1         18.5        3.96         23.76  8641628.0   \n",
       "3723325             1          8.0        1.86         13.11  8641628.0   \n",
       "7597246             2         17.0        0.00         18.30  8641628.0   \n",
       "5527651             1          8.0        1.86         11.16  8641628.0   \n",
       "6641081             1          6.0        1.46          8.76  8641628.0   \n",
       "743708              1         11.0        3.08         15.38  8641628.0   \n",
       "5525960             1          8.0        1.86         11.16  8641628.0   \n",
       "6622950             1          9.5        2.15         12.95  8641628.0   \n",
       "6975451             2          5.0        0.00          6.30  8641628.0   \n",
       "2185037             1         19.5        4.00         24.80  8641628.0   \n",
       "2190264             1          8.0        2.30         11.60  8641628.0   \n",
       "5510839             1         17.0        0.00         18.30  8641628.0   \n",
       "1127189             1         10.5        2.35         14.15  8641628.0   \n",
       "3963249             1         12.5        2.75         16.55  8641628.0   \n",
       "2198236             2          4.5        0.00          5.80  8641628.0   \n",
       "472404              1          7.5        1.00          9.80  8641628.0   \n",
       "5223740             1         16.0        4.61         27.67  8641628.0   \n",
       "6316979             1         52.0       10.00         62.80  8641628.0   \n",
       "4565660             2         25.0        0.00         26.30  8641628.0   \n",
       "867497              2          9.0        0.00         10.30  8641628.0   \n",
       "2788713             2         10.0        0.00         11.30  8641628.0   \n",
       "5214201             1          6.5        1.55          9.35  8641628.0   \n",
       "6623570             1         18.5        3.95         23.75  8641628.0   \n",
       "6973117             1          8.0        5.00         14.30  8641628.0   \n",
       "3427267             2          6.0        0.00          7.30  8641628.0   \n",
       "5504232             1          7.0        1.65          9.95  8641628.0   \n",
       "\n",
       "         tip_percent        date      time  \n",
       "2469978     0.000000  2018-01-10  00:00:00  \n",
       "2158535     0.154000  2018-01-09  00:00:00  \n",
       "7623170     0.272727  2018-01-28  00:00:00  \n",
       "1485944     0.000000  2018-01-07  00:00:00  \n",
       "6789112     0.158571  2018-01-25  00:00:00  \n",
       "1182801     0.161905  2018-01-06  00:00:00  \n",
       "1913056     0.355714  2018-01-09  00:00:00  \n",
       "5331252     0.000000  2018-01-20  00:00:00  \n",
       "5369427     0.000000  2018-01-20  00:00:00  \n",
       "744761      0.226000  2018-01-04  00:00:00  \n",
       "3442553     0.000000  2018-01-14  00:00:00  \n",
       "3719140     0.238462  2018-01-15  00:00:00  \n",
       "8443732     0.227368  2018-01-31  00:00:00  \n",
       "356360      0.000000  2018-01-02  00:00:00  \n",
       "1114303     0.161818  2018-01-05  00:00:00  \n",
       "2930677     0.158621  2018-01-12  00:00:00  \n",
       "2195509     0.000000  2018-01-10  00:00:00  \n",
       "5622750     0.000000  2018-01-21  00:00:00  \n",
       "5222268     0.000000  2018-01-20  00:00:00  \n",
       "644820      0.000000  2018-01-03  00:00:00  \n",
       "5048098     0.231190  2018-01-19  00:00:00  \n",
       "7591736     0.000000  2018-01-27  00:00:00  \n",
       "4896706     0.000000  2018-01-19  00:00:00  \n",
       "2860665     0.000000  2018-01-12  00:00:00  \n",
       "1364403     0.000000  2018-01-06  00:00:00  \n",
       "2191873     0.000000  2018-01-09  00:00:00  \n",
       "4966341     0.000000  2018-01-19  00:00:00  \n",
       "2635147     0.000000  2018-01-11  00:00:00  \n",
       "2969136     0.000000  2018-01-12  00:00:00  \n",
       "2648097     0.056000  2018-01-11  00:00:00  \n",
       "...              ...         ...       ...  \n",
       "475510      0.219259  2018-01-02  23:59:59  \n",
       "865091      0.338983  2018-01-04  23:59:59  \n",
       "3102731     0.218519  2018-01-12  23:59:59  \n",
       "4885138     0.000000  2018-01-18  23:59:59  \n",
       "7314718     0.214054  2018-01-26  23:59:59  \n",
       "3723325     0.232500  2018-01-14  23:59:59  \n",
       "7597246     0.000000  2018-01-27  23:59:59  \n",
       "5527651     0.232500  2018-01-20  23:59:59  \n",
       "6641081     0.243333  2018-01-24  23:59:59  \n",
       "743708      0.280000  2018-01-03  23:59:59  \n",
       "5525960     0.232500  2018-01-20  23:59:59  \n",
       "6622950     0.226316  2018-01-24  23:59:59  \n",
       "6975451     0.000000  2018-01-25  23:59:59  \n",
       "2185037     0.205128  2018-01-09  23:59:59  \n",
       "2190264     0.287500  2018-01-09  23:59:59  \n",
       "5510839     0.000000  2018-01-20  23:59:59  \n",
       "1127189     0.223810  2018-01-05  23:59:59  \n",
       "3963249     0.220000  2018-01-15  23:59:59  \n",
       "2198236     0.000000  2018-01-09  23:59:59  \n",
       "472404      0.133333  2018-01-02  23:59:59  \n",
       "5223740     0.288125  2018-01-19  23:59:59  \n",
       "6316979     0.192308  2018-01-23  23:59:59  \n",
       "4565660     0.000000  2018-01-17  23:59:59  \n",
       "867497      0.000000  2018-01-04  23:59:59  \n",
       "2788713     0.000000  2018-01-11  23:59:59  \n",
       "5214201     0.238462  2018-01-19  23:59:59  \n",
       "6623570     0.213514  2018-01-24  23:59:59  \n",
       "6973117     0.625000  2018-01-25  23:59:59  \n",
       "3427267     0.000000  2018-01-13  23:59:59  \n",
       "5504232     0.235714  2018-01-20  23:59:59  \n",
       "\n",
       "[8641672 rows x 12 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section=[]\n",
    "\n",
    "for date_str in data1[\"tpep_pickup_datetime\"]:\n",
    "    time = int(date_str[11:13])\n",
    "    #print(time)\n",
    "    if (time>=0 and time<5) or time>=20:\n",
    "        sec = 'night'\n",
    "    else:\n",
    "        sec = 'day'\n",
    "#     elif time>=6 and time<=11:\n",
    "#         sec = 2\n",
    "#     elif time>=12 and time<23:\n",
    "#         sec = 3\n",
    "    #print(sec)\n",
    "    section.append(sec)\n",
    "#print(\"se\")\n",
    "data1.insert(0,'DayNight',section)\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, according to the other graph above, we found that there have been some differences on the tip amount on different date, and we then suggested that this might be related to driving on weekday or weekends. Hence, we made another new variable ‘Week’ so that we could make a model to check if our guess is accurate. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week</th>\n",
       "      <th>DayNight</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>Rank</th>\n",
       "      <th>tip_percent</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2469978</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-10 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>22.28</td>\n",
       "      <td>2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>58.56</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158535</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>18.49</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7.70</td>\n",
       "      <td>59.00</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7623170</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-28 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>15.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485944</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-07 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789112</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-25 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.22</td>\n",
       "      <td>17.02</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.158571</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182801</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-06 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1.70</td>\n",
       "      <td>13.00</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.161905</td>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913056</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.49</td>\n",
       "      <td>10.79</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.355714</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331252</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5369427</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744761</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-04 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2.59</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.26</td>\n",
       "      <td>13.56</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3442553</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-14 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719140</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-15 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1.55</td>\n",
       "      <td>9.35</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443732</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-31 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.16</td>\n",
       "      <td>12.96</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.227368</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356360</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-02 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>14.57</td>\n",
       "      <td>2</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>54.56</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114303</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3.47</td>\n",
       "      <td>1</td>\n",
       "      <td>16.5</td>\n",
       "      <td>2.67</td>\n",
       "      <td>20.47</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.161818</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930677</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-12 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2.97</td>\n",
       "      <td>1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>2.30</td>\n",
       "      <td>17.60</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.158621</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195509</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-10 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622750</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-21 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>7.42</td>\n",
       "      <td>1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5222268</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1.09</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644820</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-03 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5048098</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-19 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>10.71</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9.71</td>\n",
       "      <td>58.27</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.231190</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7591736</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-27 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>13.83</td>\n",
       "      <td>2</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-27</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896706</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-19 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4.54</td>\n",
       "      <td>2</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860665</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-12 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>11.49</td>\n",
       "      <td>2</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>45.06</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364403</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-06 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>5.64</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191873</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4966341</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-19 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635147</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-11 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969136</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-12 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648097</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-11 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>17.64</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475510</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-02 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>2.96</td>\n",
       "      <td>17.76</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.219259</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865091</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-04 23:59:59</td>\n",
       "      <td>3</td>\n",
       "      <td>8.57</td>\n",
       "      <td>1</td>\n",
       "      <td>29.5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>40.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3102731</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-12 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>2.95</td>\n",
       "      <td>17.75</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.218519</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4885138</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-18 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7314718</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-26 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.81</td>\n",
       "      <td>1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>3.96</td>\n",
       "      <td>23.76</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.214054</td>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3723325</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-14 23:59:59</td>\n",
       "      <td>2</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>13.11</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597246</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-27 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>4.20</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-27</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527651</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>11.16</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6641081</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-24 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.46</td>\n",
       "      <td>8.76</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.243333</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743708</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-03 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.08</td>\n",
       "      <td>15.38</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525960</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>11.16</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6622950</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-24 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.15</td>\n",
       "      <td>12.95</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.226316</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6975451</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-25 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185037</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1</td>\n",
       "      <td>19.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>24.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190264</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>11.60</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5510839</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>4.41</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127189</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-05 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1</td>\n",
       "      <td>10.5</td>\n",
       "      <td>2.35</td>\n",
       "      <td>14.15</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.223810</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963249</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-15 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>16.55</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198236</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 23:59:59</td>\n",
       "      <td>6</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472404</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-02 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223740</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-19 23:59:59</td>\n",
       "      <td>2</td>\n",
       "      <td>4.61</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.61</td>\n",
       "      <td>27.67</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.288125</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316979</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-23 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>62.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4565660</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-17 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>8.10</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867497</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-04 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788713</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-11 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214201</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-19 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1.55</td>\n",
       "      <td>9.35</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6623570</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-24 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>3.95</td>\n",
       "      <td>23.75</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.213514</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6973117</th>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-25 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>14.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427267</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-13 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504232</th>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>2</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>9.95</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.235714</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8641672 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Week DayNight tpep_pickup_datetime  passenger_count  \\\n",
       "2469978  weekday    night  2018-01-10 00:00:00                1   \n",
       "2158535  weekday    night  2018-01-09 00:00:00                1   \n",
       "7623170  weekend    night  2018-01-28 00:00:00                1   \n",
       "1485944  weekend    night  2018-01-07 00:00:00                1   \n",
       "6789112  weekday    night  2018-01-25 00:00:00                5   \n",
       "1182801  weekend    night  2018-01-06 00:00:00                2   \n",
       "1913056  weekday    night  2018-01-09 00:00:00                1   \n",
       "5331252  weekend    night  2018-01-20 00:00:00                2   \n",
       "5369427  weekend    night  2018-01-20 00:00:00                1   \n",
       "744761   weekday    night  2018-01-04 00:00:00                2   \n",
       "3442553  weekend    night  2018-01-14 00:00:00                1   \n",
       "3719140  weekday    night  2018-01-15 00:00:00                1   \n",
       "8443732  weekday    night  2018-01-31 00:00:00                1   \n",
       "356360   weekday    night  2018-01-02 00:00:00                1   \n",
       "1114303  weekday    night  2018-01-05 00:00:00                1   \n",
       "2930677  weekday    night  2018-01-12 00:00:00                1   \n",
       "2195509  weekday    night  2018-01-10 00:00:00                3   \n",
       "5622750  weekend    night  2018-01-21 00:00:00                1   \n",
       "5222268  weekend    night  2018-01-20 00:00:00                5   \n",
       "644820   weekday    night  2018-01-03 00:00:00                2   \n",
       "5048098  weekday    night  2018-01-19 00:00:00                6   \n",
       "7591736  weekend    night  2018-01-27 00:00:00                1   \n",
       "4896706  weekday    night  2018-01-19 00:00:00                1   \n",
       "2860665  weekday    night  2018-01-12 00:00:00                1   \n",
       "1364403  weekend    night  2018-01-06 00:00:00                1   \n",
       "2191873  weekday    night  2018-01-09 00:00:00                5   \n",
       "4966341  weekday    night  2018-01-19 00:00:00                1   \n",
       "2635147  weekday    night  2018-01-11 00:00:00                1   \n",
       "2969136  weekday    night  2018-01-12 00:00:00                1   \n",
       "2648097  weekday    night  2018-01-11 00:00:00                6   \n",
       "...          ...      ...                  ...              ...   \n",
       "475510   weekday    night  2018-01-02 23:59:59                1   \n",
       "865091   weekday    night  2018-01-04 23:59:59                3   \n",
       "3102731  weekday    night  2018-01-12 23:59:59                1   \n",
       "4885138  weekday    night  2018-01-18 23:59:59                1   \n",
       "7314718  weekday    night  2018-01-26 23:59:59                1   \n",
       "3723325  weekend    night  2018-01-14 23:59:59                2   \n",
       "7597246  weekend    night  2018-01-27 23:59:59                1   \n",
       "5527651  weekend    night  2018-01-20 23:59:59                1   \n",
       "6641081  weekday    night  2018-01-24 23:59:59                1   \n",
       "743708   weekday    night  2018-01-03 23:59:59                1   \n",
       "5525960  weekend    night  2018-01-20 23:59:59                1   \n",
       "6622950  weekday    night  2018-01-24 23:59:59                1   \n",
       "6975451  weekday    night  2018-01-25 23:59:59                1   \n",
       "2185037  weekday    night  2018-01-09 23:59:59                1   \n",
       "2190264  weekday    night  2018-01-09 23:59:59                1   \n",
       "5510839  weekend    night  2018-01-20 23:59:59                1   \n",
       "1127189  weekday    night  2018-01-05 23:59:59                1   \n",
       "3963249  weekday    night  2018-01-15 23:59:59                1   \n",
       "2198236  weekday    night  2018-01-09 23:59:59                6   \n",
       "472404   weekday    night  2018-01-02 23:59:59                1   \n",
       "5223740  weekday    night  2018-01-19 23:59:59                2   \n",
       "6316979  weekday    night  2018-01-23 23:59:59                1   \n",
       "4565660  weekday    night  2018-01-17 23:59:59                1   \n",
       "867497   weekday    night  2018-01-04 23:59:59                1   \n",
       "2788713  weekday    night  2018-01-11 23:59:59                1   \n",
       "5214201  weekday    night  2018-01-19 23:59:59                1   \n",
       "6623570  weekday    night  2018-01-24 23:59:59                1   \n",
       "6973117  weekday    night  2018-01-25 23:59:59                1   \n",
       "3427267  weekend    night  2018-01-13 23:59:59                1   \n",
       "5504232  weekend    night  2018-01-20 23:59:59                2   \n",
       "\n",
       "         trip_distance  payment_type  fare_amount  tip_amount  total_amount  \\\n",
       "2469978          22.28             2         52.0        0.00         58.56   \n",
       "2158535          18.49             1         50.0        7.70         59.00   \n",
       "7623170           1.91             1         11.0        3.00         15.30   \n",
       "1485944           1.06             2          6.5        0.00          7.30   \n",
       "6789112           2.80             1         14.0        2.22         17.02   \n",
       "1182801           2.80             1         10.5        1.70         13.00   \n",
       "1913056           1.30             1          7.0        2.49         10.79   \n",
       "5331252           1.69             2          9.0        0.00          9.80   \n",
       "5369427           6.50             2         19.5        0.00         20.30   \n",
       "744761            2.59             1         10.0        2.26         13.56   \n",
       "3442553           0.91             2          7.0        0.00          8.30   \n",
       "3719140           0.80             1          6.5        1.55          9.35   \n",
       "8443732           1.86             1          9.5        2.16         12.96   \n",
       "356360           14.57             2         48.0        0.00         54.56   \n",
       "1114303           3.47             1         16.5        2.67         20.47   \n",
       "2930677           2.97             1         14.5        2.30         17.60   \n",
       "2195509           3.59             2         15.0        0.00         16.30   \n",
       "5622750           7.42             1         24.5        0.00         25.30   \n",
       "5222268           1.09             2          6.0        0.00          7.30   \n",
       "644820            1.53             2         10.5        0.00         12.30   \n",
       "5048098          10.71             1         42.0        9.71         58.27   \n",
       "7591736          13.83             2         38.5        0.00         39.80   \n",
       "4896706           4.54             2         15.5        0.00         16.80   \n",
       "2860665          11.49             2         38.5        0.00         45.06   \n",
       "1364403           5.64             2         20.0        0.00         21.30   \n",
       "2191873           1.25             2          8.5        0.00          9.80   \n",
       "4966341           1.42             2         11.5        0.00         12.30   \n",
       "2635147           3.45             2         12.0        0.00         12.80   \n",
       "2969136           5.00             2         23.0        0.00         24.80   \n",
       "2648097           2.00             1         15.0        0.84         17.64   \n",
       "...                ...           ...          ...         ...           ...   \n",
       "475510            3.51             1         13.5        2.96         17.76   \n",
       "865091            8.57             1         29.5       10.00         40.80   \n",
       "3102731           2.80             1         13.5        2.95         17.75   \n",
       "4885138           0.80             2          5.0        0.00          6.30   \n",
       "7314718           3.81             1         18.5        3.96         23.76   \n",
       "3723325           1.91             1          8.0        1.86         13.11   \n",
       "7597246           4.20             2         17.0        0.00         18.30   \n",
       "5527651           1.61             1          8.0        1.86         11.16   \n",
       "6641081           1.38             1          6.0        1.46          8.76   \n",
       "743708            3.25             1         11.0        3.08         15.38   \n",
       "5525960           1.67             1          8.0        1.86         11.16   \n",
       "6622950           2.00             1          9.5        2.15         12.95   \n",
       "6975451           0.80             2          5.0        0.00          6.30   \n",
       "2185037           4.70             1         19.5        4.00         24.80   \n",
       "2190264           1.40             1          8.0        2.30         11.60   \n",
       "5510839           4.41             1         17.0        0.00         18.30   \n",
       "1127189           1.50             1         10.5        2.35         14.15   \n",
       "3963249           3.20             1         12.5        2.75         16.55   \n",
       "2198236           0.82             2          4.5        0.00          5.80   \n",
       "472404            1.15             1          7.5        1.00          9.80   \n",
       "5223740           4.61             1         16.0        4.61         27.67   \n",
       "6316979          17.00             1         52.0       10.00         62.80   \n",
       "4565660           8.10             2         25.0        0.00         26.30   \n",
       "867497            2.08             2          9.0        0.00         10.30   \n",
       "2788713           2.15             2         10.0        0.00         11.30   \n",
       "5214201           1.00             1          6.5        1.55          9.35   \n",
       "6623570           5.50             1         18.5        3.95         23.75   \n",
       "6973117           1.53             1          8.0        5.00         14.30   \n",
       "3427267           0.80             2          6.0        0.00          7.30   \n",
       "5504232           1.30             1          7.0        1.65          9.95   \n",
       "\n",
       "              Rank  tip_percent        date      time  \n",
       "2469978       63.5     0.000000  2018-01-10  00:00:00  \n",
       "2158535       63.5     0.154000  2018-01-09  00:00:00  \n",
       "7623170       63.5     0.272727  2018-01-28  00:00:00  \n",
       "1485944       63.5     0.000000  2018-01-07  00:00:00  \n",
       "6789112       63.5     0.158571  2018-01-25  00:00:00  \n",
       "1182801       63.5     0.161905  2018-01-06  00:00:00  \n",
       "1913056       63.5     0.355714  2018-01-09  00:00:00  \n",
       "5331252       63.5     0.000000  2018-01-20  00:00:00  \n",
       "5369427       63.5     0.000000  2018-01-20  00:00:00  \n",
       "744761        63.5     0.226000  2018-01-04  00:00:00  \n",
       "3442553       63.5     0.000000  2018-01-14  00:00:00  \n",
       "3719140       63.5     0.238462  2018-01-15  00:00:00  \n",
       "8443732       63.5     0.227368  2018-01-31  00:00:00  \n",
       "356360        63.5     0.000000  2018-01-02  00:00:00  \n",
       "1114303       63.5     0.161818  2018-01-05  00:00:00  \n",
       "2930677       63.5     0.158621  2018-01-12  00:00:00  \n",
       "2195509       63.5     0.000000  2018-01-10  00:00:00  \n",
       "5622750       63.5     0.000000  2018-01-21  00:00:00  \n",
       "5222268       63.5     0.000000  2018-01-20  00:00:00  \n",
       "644820        63.5     0.000000  2018-01-03  00:00:00  \n",
       "5048098       63.5     0.231190  2018-01-19  00:00:00  \n",
       "7591736       63.5     0.000000  2018-01-27  00:00:00  \n",
       "4896706       63.5     0.000000  2018-01-19  00:00:00  \n",
       "2860665       63.5     0.000000  2018-01-12  00:00:00  \n",
       "1364403       63.5     0.000000  2018-01-06  00:00:00  \n",
       "2191873       63.5     0.000000  2018-01-09  00:00:00  \n",
       "4966341       63.5     0.000000  2018-01-19  00:00:00  \n",
       "2635147       63.5     0.000000  2018-01-11  00:00:00  \n",
       "2969136       63.5     0.000000  2018-01-12  00:00:00  \n",
       "2648097       63.5     0.056000  2018-01-11  00:00:00  \n",
       "...            ...          ...         ...       ...  \n",
       "475510   8641628.0     0.219259  2018-01-02  23:59:59  \n",
       "865091   8641628.0     0.338983  2018-01-04  23:59:59  \n",
       "3102731  8641628.0     0.218519  2018-01-12  23:59:59  \n",
       "4885138  8641628.0     0.000000  2018-01-18  23:59:59  \n",
       "7314718  8641628.0     0.214054  2018-01-26  23:59:59  \n",
       "3723325  8641628.0     0.232500  2018-01-14  23:59:59  \n",
       "7597246  8641628.0     0.000000  2018-01-27  23:59:59  \n",
       "5527651  8641628.0     0.232500  2018-01-20  23:59:59  \n",
       "6641081  8641628.0     0.243333  2018-01-24  23:59:59  \n",
       "743708   8641628.0     0.280000  2018-01-03  23:59:59  \n",
       "5525960  8641628.0     0.232500  2018-01-20  23:59:59  \n",
       "6622950  8641628.0     0.226316  2018-01-24  23:59:59  \n",
       "6975451  8641628.0     0.000000  2018-01-25  23:59:59  \n",
       "2185037  8641628.0     0.205128  2018-01-09  23:59:59  \n",
       "2190264  8641628.0     0.287500  2018-01-09  23:59:59  \n",
       "5510839  8641628.0     0.000000  2018-01-20  23:59:59  \n",
       "1127189  8641628.0     0.223810  2018-01-05  23:59:59  \n",
       "3963249  8641628.0     0.220000  2018-01-15  23:59:59  \n",
       "2198236  8641628.0     0.000000  2018-01-09  23:59:59  \n",
       "472404   8641628.0     0.133333  2018-01-02  23:59:59  \n",
       "5223740  8641628.0     0.288125  2018-01-19  23:59:59  \n",
       "6316979  8641628.0     0.192308  2018-01-23  23:59:59  \n",
       "4565660  8641628.0     0.000000  2018-01-17  23:59:59  \n",
       "867497   8641628.0     0.000000  2018-01-04  23:59:59  \n",
       "2788713  8641628.0     0.000000  2018-01-11  23:59:59  \n",
       "5214201  8641628.0     0.238462  2018-01-19  23:59:59  \n",
       "6623570  8641628.0     0.213514  2018-01-24  23:59:59  \n",
       "6973117  8641628.0     0.625000  2018-01-25  23:59:59  \n",
       "3427267  8641628.0     0.000000  2018-01-13  23:59:59  \n",
       "5504232  8641628.0     0.235714  2018-01-20  23:59:59  \n",
       "\n",
       "[8641672 rows x 13 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekend = []\n",
    "\n",
    "for date in data1[\"date\"]:\n",
    "    date_time_obj = datetime.strptime(date, '%Y-%m-%d')\n",
    "    \n",
    "    weekno = date_time_obj.weekday()\n",
    "\n",
    "    if weekno<5:\n",
    "        sec = 'weekday'\n",
    "    else:\n",
    "        sec = 'weekend'\n",
    "    weekend.append(sec)\n",
    "#print(\"se\")\n",
    "data1.insert(0,'Week',weekend)\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we wanted to set up a binary variable, which could be our Y value later in the model. We first calculated the mean value of the tip amount in the whole dataset, and defined the ones that are the same as or higher than the mean value to be 1, and others to be 0. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot insert Y, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-39fa72e61305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mislager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#print(\"se\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdata1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mislager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   3471\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3472\u001b[0m         self._data.insert(loc, column, value,\n\u001b[0;32m-> 3473\u001b[0;31m                           allow_duplicates=allow_duplicates)\n\u001b[0m\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cannot insert {}, already exists'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot insert Y, already exists"
     ]
    }
   ],
   "source": [
    "meanTip = data1['tip_amount'].mean()\n",
    "islager = []\n",
    "\n",
    "for tip in data1[\"tip_amount\"]:\n",
    "    if tip >= meanTip :\n",
    "        sec = 1\n",
    "    else:\n",
    "        sec = 0\n",
    "        \n",
    "    islager.append(sec)\n",
    "#print(\"se\")\n",
    "data1.insert(0,'Y',islager)\n",
    "data1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>Week</th>\n",
       "      <th>DayNight</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>Rank</th>\n",
       "      <th>tip_percent</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2469978</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-10 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>22.28</td>\n",
       "      <td>2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>58.56</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158535</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>18.49</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7.70</td>\n",
       "      <td>59.00</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7623170</th>\n",
       "      <td>1</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-28 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>15.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485944</th>\n",
       "      <td>0</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-07 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789112</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-25 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.22</td>\n",
       "      <td>17.02</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.158571</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182801</th>\n",
       "      <td>0</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-06 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1.70</td>\n",
       "      <td>13.00</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.161905</td>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913056</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.49</td>\n",
       "      <td>10.79</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.355714</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331252</th>\n",
       "      <td>0</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5369427</th>\n",
       "      <td>0</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744761</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-04 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2.59</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.26</td>\n",
       "      <td>13.56</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3442553</th>\n",
       "      <td>0</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-14 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719140</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-15 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1.55</td>\n",
       "      <td>9.35</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443732</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-31 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.16</td>\n",
       "      <td>12.96</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.227368</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356360</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-02 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>14.57</td>\n",
       "      <td>2</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>54.56</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114303</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3.47</td>\n",
       "      <td>1</td>\n",
       "      <td>16.5</td>\n",
       "      <td>2.67</td>\n",
       "      <td>20.47</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.161818</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930677</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-12 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2.97</td>\n",
       "      <td>1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>2.30</td>\n",
       "      <td>17.60</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.158621</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195509</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-10 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622750</th>\n",
       "      <td>0</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-21 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>7.42</td>\n",
       "      <td>1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5222268</th>\n",
       "      <td>0</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1.09</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644820</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-03 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5048098</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-19 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>10.71</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9.71</td>\n",
       "      <td>58.27</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.231190</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7591736</th>\n",
       "      <td>0</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-27 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>13.83</td>\n",
       "      <td>2</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-27</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896706</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-19 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4.54</td>\n",
       "      <td>2</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860665</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-12 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>11.49</td>\n",
       "      <td>2</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>45.06</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364403</th>\n",
       "      <td>0</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-06 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>5.64</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191873</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4966341</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-19 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.30</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635147</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-11 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969136</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-12 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.80</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648097</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-11 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>17.64</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475510</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-02 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>2.96</td>\n",
       "      <td>17.76</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.219259</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865091</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-04 23:59:59</td>\n",
       "      <td>3</td>\n",
       "      <td>8.57</td>\n",
       "      <td>1</td>\n",
       "      <td>29.5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>40.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3102731</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-12 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>2.95</td>\n",
       "      <td>17.75</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.218519</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4885138</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-18 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7314718</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-26 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.81</td>\n",
       "      <td>1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>3.96</td>\n",
       "      <td>23.76</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.214054</td>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3723325</th>\n",
       "      <td>1</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-14 23:59:59</td>\n",
       "      <td>2</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>13.11</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597246</th>\n",
       "      <td>0</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-27 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>4.20</td>\n",
       "      <td>2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-27</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527651</th>\n",
       "      <td>1</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>11.16</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6641081</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-24 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.46</td>\n",
       "      <td>8.76</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.243333</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743708</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-03 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.08</td>\n",
       "      <td>15.38</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525960</th>\n",
       "      <td>1</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>11.16</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6622950</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-24 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.15</td>\n",
       "      <td>12.95</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.226316</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6975451</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-25 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185037</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1</td>\n",
       "      <td>19.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>24.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190264</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>11.60</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5510839</th>\n",
       "      <td>0</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>4.41</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127189</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-05 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1</td>\n",
       "      <td>10.5</td>\n",
       "      <td>2.35</td>\n",
       "      <td>14.15</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.223810</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963249</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-15 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>16.55</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198236</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-09 23:59:59</td>\n",
       "      <td>6</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472404</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-02 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223740</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-19 23:59:59</td>\n",
       "      <td>2</td>\n",
       "      <td>4.61</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.61</td>\n",
       "      <td>27.67</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.288125</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316979</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-23 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>62.80</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4565660</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-17 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>8.10</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867497</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-04 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788713</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-11 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214201</th>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-19 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1.55</td>\n",
       "      <td>9.35</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6623570</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-24 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>3.95</td>\n",
       "      <td>23.75</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.213514</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6973117</th>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-25 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>14.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427267</th>\n",
       "      <td>0</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-13 23:59:59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.30</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504232</th>\n",
       "      <td>0</td>\n",
       "      <td>weekend</td>\n",
       "      <td>night</td>\n",
       "      <td>2018-01-20 23:59:59</td>\n",
       "      <td>2</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>9.95</td>\n",
       "      <td>8641628.0</td>\n",
       "      <td>0.235714</td>\n",
       "      <td>2018-01-20</td>\n",
       "      <td>23:59:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8641672 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Y     Week DayNight tpep_pickup_datetime  passenger_count  \\\n",
       "2469978  0  weekday    night  2018-01-10 00:00:00                1   \n",
       "2158535  1  weekday    night  2018-01-09 00:00:00                1   \n",
       "7623170  1  weekend    night  2018-01-28 00:00:00                1   \n",
       "1485944  0  weekend    night  2018-01-07 00:00:00                1   \n",
       "6789112  1  weekday    night  2018-01-25 00:00:00                5   \n",
       "1182801  0  weekend    night  2018-01-06 00:00:00                2   \n",
       "1913056  1  weekday    night  2018-01-09 00:00:00                1   \n",
       "5331252  0  weekend    night  2018-01-20 00:00:00                2   \n",
       "5369427  0  weekend    night  2018-01-20 00:00:00                1   \n",
       "744761   1  weekday    night  2018-01-04 00:00:00                2   \n",
       "3442553  0  weekend    night  2018-01-14 00:00:00                1   \n",
       "3719140  0  weekday    night  2018-01-15 00:00:00                1   \n",
       "8443732  1  weekday    night  2018-01-31 00:00:00                1   \n",
       "356360   0  weekday    night  2018-01-02 00:00:00                1   \n",
       "1114303  1  weekday    night  2018-01-05 00:00:00                1   \n",
       "2930677  1  weekday    night  2018-01-12 00:00:00                1   \n",
       "2195509  0  weekday    night  2018-01-10 00:00:00                3   \n",
       "5622750  0  weekend    night  2018-01-21 00:00:00                1   \n",
       "5222268  0  weekend    night  2018-01-20 00:00:00                5   \n",
       "644820   0  weekday    night  2018-01-03 00:00:00                2   \n",
       "5048098  1  weekday    night  2018-01-19 00:00:00                6   \n",
       "7591736  0  weekend    night  2018-01-27 00:00:00                1   \n",
       "4896706  0  weekday    night  2018-01-19 00:00:00                1   \n",
       "2860665  0  weekday    night  2018-01-12 00:00:00                1   \n",
       "1364403  0  weekend    night  2018-01-06 00:00:00                1   \n",
       "2191873  0  weekday    night  2018-01-09 00:00:00                5   \n",
       "4966341  0  weekday    night  2018-01-19 00:00:00                1   \n",
       "2635147  0  weekday    night  2018-01-11 00:00:00                1   \n",
       "2969136  0  weekday    night  2018-01-12 00:00:00                1   \n",
       "2648097  0  weekday    night  2018-01-11 00:00:00                6   \n",
       "...     ..      ...      ...                  ...              ...   \n",
       "475510   1  weekday    night  2018-01-02 23:59:59                1   \n",
       "865091   1  weekday    night  2018-01-04 23:59:59                3   \n",
       "3102731  1  weekday    night  2018-01-12 23:59:59                1   \n",
       "4885138  0  weekday    night  2018-01-18 23:59:59                1   \n",
       "7314718  1  weekday    night  2018-01-26 23:59:59                1   \n",
       "3723325  1  weekend    night  2018-01-14 23:59:59                2   \n",
       "7597246  0  weekend    night  2018-01-27 23:59:59                1   \n",
       "5527651  1  weekend    night  2018-01-20 23:59:59                1   \n",
       "6641081  0  weekday    night  2018-01-24 23:59:59                1   \n",
       "743708   1  weekday    night  2018-01-03 23:59:59                1   \n",
       "5525960  1  weekend    night  2018-01-20 23:59:59                1   \n",
       "6622950  1  weekday    night  2018-01-24 23:59:59                1   \n",
       "6975451  0  weekday    night  2018-01-25 23:59:59                1   \n",
       "2185037  1  weekday    night  2018-01-09 23:59:59                1   \n",
       "2190264  1  weekday    night  2018-01-09 23:59:59                1   \n",
       "5510839  0  weekend    night  2018-01-20 23:59:59                1   \n",
       "1127189  1  weekday    night  2018-01-05 23:59:59                1   \n",
       "3963249  1  weekday    night  2018-01-15 23:59:59                1   \n",
       "2198236  0  weekday    night  2018-01-09 23:59:59                6   \n",
       "472404   0  weekday    night  2018-01-02 23:59:59                1   \n",
       "5223740  1  weekday    night  2018-01-19 23:59:59                2   \n",
       "6316979  1  weekday    night  2018-01-23 23:59:59                1   \n",
       "4565660  0  weekday    night  2018-01-17 23:59:59                1   \n",
       "867497   0  weekday    night  2018-01-04 23:59:59                1   \n",
       "2788713  0  weekday    night  2018-01-11 23:59:59                1   \n",
       "5214201  0  weekday    night  2018-01-19 23:59:59                1   \n",
       "6623570  1  weekday    night  2018-01-24 23:59:59                1   \n",
       "6973117  1  weekday    night  2018-01-25 23:59:59                1   \n",
       "3427267  0  weekend    night  2018-01-13 23:59:59                1   \n",
       "5504232  0  weekend    night  2018-01-20 23:59:59                2   \n",
       "\n",
       "         trip_distance  payment_type  fare_amount  tip_amount  total_amount  \\\n",
       "2469978          22.28             2         52.0        0.00         58.56   \n",
       "2158535          18.49             1         50.0        7.70         59.00   \n",
       "7623170           1.91             1         11.0        3.00         15.30   \n",
       "1485944           1.06             2          6.5        0.00          7.30   \n",
       "6789112           2.80             1         14.0        2.22         17.02   \n",
       "1182801           2.80             1         10.5        1.70         13.00   \n",
       "1913056           1.30             1          7.0        2.49         10.79   \n",
       "5331252           1.69             2          9.0        0.00          9.80   \n",
       "5369427           6.50             2         19.5        0.00         20.30   \n",
       "744761            2.59             1         10.0        2.26         13.56   \n",
       "3442553           0.91             2          7.0        0.00          8.30   \n",
       "3719140           0.80             1          6.5        1.55          9.35   \n",
       "8443732           1.86             1          9.5        2.16         12.96   \n",
       "356360           14.57             2         48.0        0.00         54.56   \n",
       "1114303           3.47             1         16.5        2.67         20.47   \n",
       "2930677           2.97             1         14.5        2.30         17.60   \n",
       "2195509           3.59             2         15.0        0.00         16.30   \n",
       "5622750           7.42             1         24.5        0.00         25.30   \n",
       "5222268           1.09             2          6.0        0.00          7.30   \n",
       "644820            1.53             2         10.5        0.00         12.30   \n",
       "5048098          10.71             1         42.0        9.71         58.27   \n",
       "7591736          13.83             2         38.5        0.00         39.80   \n",
       "4896706           4.54             2         15.5        0.00         16.80   \n",
       "2860665          11.49             2         38.5        0.00         45.06   \n",
       "1364403           5.64             2         20.0        0.00         21.30   \n",
       "2191873           1.25             2          8.5        0.00          9.80   \n",
       "4966341           1.42             2         11.5        0.00         12.30   \n",
       "2635147           3.45             2         12.0        0.00         12.80   \n",
       "2969136           5.00             2         23.0        0.00         24.80   \n",
       "2648097           2.00             1         15.0        0.84         17.64   \n",
       "...                ...           ...          ...         ...           ...   \n",
       "475510            3.51             1         13.5        2.96         17.76   \n",
       "865091            8.57             1         29.5       10.00         40.80   \n",
       "3102731           2.80             1         13.5        2.95         17.75   \n",
       "4885138           0.80             2          5.0        0.00          6.30   \n",
       "7314718           3.81             1         18.5        3.96         23.76   \n",
       "3723325           1.91             1          8.0        1.86         13.11   \n",
       "7597246           4.20             2         17.0        0.00         18.30   \n",
       "5527651           1.61             1          8.0        1.86         11.16   \n",
       "6641081           1.38             1          6.0        1.46          8.76   \n",
       "743708            3.25             1         11.0        3.08         15.38   \n",
       "5525960           1.67             1          8.0        1.86         11.16   \n",
       "6622950           2.00             1          9.5        2.15         12.95   \n",
       "6975451           0.80             2          5.0        0.00          6.30   \n",
       "2185037           4.70             1         19.5        4.00         24.80   \n",
       "2190264           1.40             1          8.0        2.30         11.60   \n",
       "5510839           4.41             1         17.0        0.00         18.30   \n",
       "1127189           1.50             1         10.5        2.35         14.15   \n",
       "3963249           3.20             1         12.5        2.75         16.55   \n",
       "2198236           0.82             2          4.5        0.00          5.80   \n",
       "472404            1.15             1          7.5        1.00          9.80   \n",
       "5223740           4.61             1         16.0        4.61         27.67   \n",
       "6316979          17.00             1         52.0       10.00         62.80   \n",
       "4565660           8.10             2         25.0        0.00         26.30   \n",
       "867497            2.08             2          9.0        0.00         10.30   \n",
       "2788713           2.15             2         10.0        0.00         11.30   \n",
       "5214201           1.00             1          6.5        1.55          9.35   \n",
       "6623570           5.50             1         18.5        3.95         23.75   \n",
       "6973117           1.53             1          8.0        5.00         14.30   \n",
       "3427267           0.80             2          6.0        0.00          7.30   \n",
       "5504232           1.30             1          7.0        1.65          9.95   \n",
       "\n",
       "              Rank  tip_percent        date      time  \n",
       "2469978       63.5     0.000000  2018-01-10  00:00:00  \n",
       "2158535       63.5     0.154000  2018-01-09  00:00:00  \n",
       "7623170       63.5     0.272727  2018-01-28  00:00:00  \n",
       "1485944       63.5     0.000000  2018-01-07  00:00:00  \n",
       "6789112       63.5     0.158571  2018-01-25  00:00:00  \n",
       "1182801       63.5     0.161905  2018-01-06  00:00:00  \n",
       "1913056       63.5     0.355714  2018-01-09  00:00:00  \n",
       "5331252       63.5     0.000000  2018-01-20  00:00:00  \n",
       "5369427       63.5     0.000000  2018-01-20  00:00:00  \n",
       "744761        63.5     0.226000  2018-01-04  00:00:00  \n",
       "3442553       63.5     0.000000  2018-01-14  00:00:00  \n",
       "3719140       63.5     0.238462  2018-01-15  00:00:00  \n",
       "8443732       63.5     0.227368  2018-01-31  00:00:00  \n",
       "356360        63.5     0.000000  2018-01-02  00:00:00  \n",
       "1114303       63.5     0.161818  2018-01-05  00:00:00  \n",
       "2930677       63.5     0.158621  2018-01-12  00:00:00  \n",
       "2195509       63.5     0.000000  2018-01-10  00:00:00  \n",
       "5622750       63.5     0.000000  2018-01-21  00:00:00  \n",
       "5222268       63.5     0.000000  2018-01-20  00:00:00  \n",
       "644820        63.5     0.000000  2018-01-03  00:00:00  \n",
       "5048098       63.5     0.231190  2018-01-19  00:00:00  \n",
       "7591736       63.5     0.000000  2018-01-27  00:00:00  \n",
       "4896706       63.5     0.000000  2018-01-19  00:00:00  \n",
       "2860665       63.5     0.000000  2018-01-12  00:00:00  \n",
       "1364403       63.5     0.000000  2018-01-06  00:00:00  \n",
       "2191873       63.5     0.000000  2018-01-09  00:00:00  \n",
       "4966341       63.5     0.000000  2018-01-19  00:00:00  \n",
       "2635147       63.5     0.000000  2018-01-11  00:00:00  \n",
       "2969136       63.5     0.000000  2018-01-12  00:00:00  \n",
       "2648097       63.5     0.056000  2018-01-11  00:00:00  \n",
       "...            ...          ...         ...       ...  \n",
       "475510   8641628.0     0.219259  2018-01-02  23:59:59  \n",
       "865091   8641628.0     0.338983  2018-01-04  23:59:59  \n",
       "3102731  8641628.0     0.218519  2018-01-12  23:59:59  \n",
       "4885138  8641628.0     0.000000  2018-01-18  23:59:59  \n",
       "7314718  8641628.0     0.214054  2018-01-26  23:59:59  \n",
       "3723325  8641628.0     0.232500  2018-01-14  23:59:59  \n",
       "7597246  8641628.0     0.000000  2018-01-27  23:59:59  \n",
       "5527651  8641628.0     0.232500  2018-01-20  23:59:59  \n",
       "6641081  8641628.0     0.243333  2018-01-24  23:59:59  \n",
       "743708   8641628.0     0.280000  2018-01-03  23:59:59  \n",
       "5525960  8641628.0     0.232500  2018-01-20  23:59:59  \n",
       "6622950  8641628.0     0.226316  2018-01-24  23:59:59  \n",
       "6975451  8641628.0     0.000000  2018-01-25  23:59:59  \n",
       "2185037  8641628.0     0.205128  2018-01-09  23:59:59  \n",
       "2190264  8641628.0     0.287500  2018-01-09  23:59:59  \n",
       "5510839  8641628.0     0.000000  2018-01-20  23:59:59  \n",
       "1127189  8641628.0     0.223810  2018-01-05  23:59:59  \n",
       "3963249  8641628.0     0.220000  2018-01-15  23:59:59  \n",
       "2198236  8641628.0     0.000000  2018-01-09  23:59:59  \n",
       "472404   8641628.0     0.133333  2018-01-02  23:59:59  \n",
       "5223740  8641628.0     0.288125  2018-01-19  23:59:59  \n",
       "6316979  8641628.0     0.192308  2018-01-23  23:59:59  \n",
       "4565660  8641628.0     0.000000  2018-01-17  23:59:59  \n",
       "867497   8641628.0     0.000000  2018-01-04  23:59:59  \n",
       "2788713  8641628.0     0.000000  2018-01-11  23:59:59  \n",
       "5214201  8641628.0     0.238462  2018-01-19  23:59:59  \n",
       "6623570  8641628.0     0.213514  2018-01-24  23:59:59  \n",
       "6973117  8641628.0     0.625000  2018-01-25  23:59:59  \n",
       "3427267  8641628.0     0.000000  2018-01-13  23:59:59  \n",
       "5504232  8641628.0     0.235714  2018-01-20  23:59:59  \n",
       "\n",
       "[8641672 rows x 14 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create more useful variables, we changed both our “Week” and “DayNight” into binary.\n",
    "We also created a new dataframe “inputDF” to reduce error and easy to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = data1[['Y','DayNight','Week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isNight</th>\n",
       "      <th>Y</th>\n",
       "      <th>DayNight</th>\n",
       "      <th>Week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2469978</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158535</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7623170</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485944</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789112</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182801</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913056</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331252</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5369427</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744761</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3442553</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719140</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443732</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356360</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114303</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930677</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195509</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622750</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5222268</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644820</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5048098</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7591736</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896706</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860665</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364403</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191873</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4966341</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635147</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969136</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648097</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475510</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865091</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3102731</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4885138</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7314718</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3723325</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597246</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527651</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6641081</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743708</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525960</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6622950</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6975451</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185037</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190264</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5510839</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127189</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963249</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198236</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472404</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223740</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316979</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4565660</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867497</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788713</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214201</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6623570</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6973117</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427267</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504232</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8641672 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         isNight  Y DayNight     Week\n",
       "2469978        1  0    night  weekday\n",
       "2158535        1  1    night  weekday\n",
       "7623170        1  1    night  weekend\n",
       "1485944        1  0    night  weekend\n",
       "6789112        1  1    night  weekday\n",
       "1182801        1  0    night  weekend\n",
       "1913056        1  1    night  weekday\n",
       "5331252        1  0    night  weekend\n",
       "5369427        1  0    night  weekend\n",
       "744761         1  1    night  weekday\n",
       "3442553        1  0    night  weekend\n",
       "3719140        1  0    night  weekday\n",
       "8443732        1  1    night  weekday\n",
       "356360         1  0    night  weekday\n",
       "1114303        1  1    night  weekday\n",
       "2930677        1  1    night  weekday\n",
       "2195509        1  0    night  weekday\n",
       "5622750        1  0    night  weekend\n",
       "5222268        1  0    night  weekend\n",
       "644820         1  0    night  weekday\n",
       "5048098        1  1    night  weekday\n",
       "7591736        1  0    night  weekend\n",
       "4896706        1  0    night  weekday\n",
       "2860665        1  0    night  weekday\n",
       "1364403        1  0    night  weekend\n",
       "2191873        1  0    night  weekday\n",
       "4966341        1  0    night  weekday\n",
       "2635147        1  0    night  weekday\n",
       "2969136        1  0    night  weekday\n",
       "2648097        1  0    night  weekday\n",
       "...          ... ..      ...      ...\n",
       "475510         1  1    night  weekday\n",
       "865091         1  1    night  weekday\n",
       "3102731        1  1    night  weekday\n",
       "4885138        1  0    night  weekday\n",
       "7314718        1  1    night  weekday\n",
       "3723325        1  1    night  weekend\n",
       "7597246        1  0    night  weekend\n",
       "5527651        1  1    night  weekend\n",
       "6641081        1  0    night  weekday\n",
       "743708         1  1    night  weekday\n",
       "5525960        1  1    night  weekend\n",
       "6622950        1  1    night  weekday\n",
       "6975451        1  0    night  weekday\n",
       "2185037        1  1    night  weekday\n",
       "2190264        1  1    night  weekday\n",
       "5510839        1  0    night  weekend\n",
       "1127189        1  1    night  weekday\n",
       "3963249        1  1    night  weekday\n",
       "2198236        1  0    night  weekday\n",
       "472404         1  0    night  weekday\n",
       "5223740        1  1    night  weekday\n",
       "6316979        1  1    night  weekday\n",
       "4565660        1  0    night  weekday\n",
       "867497         1  0    night  weekday\n",
       "2788713        1  0    night  weekday\n",
       "5214201        1  0    night  weekday\n",
       "6623570        1  1    night  weekday\n",
       "6973117        1  1    night  weekday\n",
       "3427267        1  0    night  weekend\n",
       "5504232        1  0    night  weekend\n",
       "\n",
       "[8641672 rows x 4 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputDF \n",
    "isNight = []\n",
    "for val in inputDF[\"DayNight\"]:\n",
    "    if val == 'night':\n",
    "        sec = 1\n",
    "    else:\n",
    "        sec = 0\n",
    "    isNight.append(sec)\n",
    "#print(\"se\")\n",
    "inputDF.insert(0,'isNight',isNight)\n",
    "inputDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isWeekend</th>\n",
       "      <th>isNight</th>\n",
       "      <th>Y</th>\n",
       "      <th>DayNight</th>\n",
       "      <th>Week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2469978</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158535</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7623170</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485944</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789112</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182801</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913056</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331252</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5369427</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744761</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3442553</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719140</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443732</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356360</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114303</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930677</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195509</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622750</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5222268</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644820</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5048098</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7591736</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896706</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860665</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364403</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191873</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4966341</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635147</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969136</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648097</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475510</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865091</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3102731</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4885138</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7314718</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3723325</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597246</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527651</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6641081</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743708</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525960</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6622950</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6975451</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185037</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190264</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5510839</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127189</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963249</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198236</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472404</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223740</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316979</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4565660</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867497</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788713</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214201</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6623570</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6973117</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427267</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504232</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8641672 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         isWeekend  isNight  Y DayNight     Week\n",
       "2469978          0        1  0    night  weekday\n",
       "2158535          0        1  1    night  weekday\n",
       "7623170          1        1  1    night  weekend\n",
       "1485944          1        1  0    night  weekend\n",
       "6789112          0        1  1    night  weekday\n",
       "1182801          1        1  0    night  weekend\n",
       "1913056          0        1  1    night  weekday\n",
       "5331252          1        1  0    night  weekend\n",
       "5369427          1        1  0    night  weekend\n",
       "744761           0        1  1    night  weekday\n",
       "3442553          1        1  0    night  weekend\n",
       "3719140          0        1  0    night  weekday\n",
       "8443732          0        1  1    night  weekday\n",
       "356360           0        1  0    night  weekday\n",
       "1114303          0        1  1    night  weekday\n",
       "2930677          0        1  1    night  weekday\n",
       "2195509          0        1  0    night  weekday\n",
       "5622750          1        1  0    night  weekend\n",
       "5222268          1        1  0    night  weekend\n",
       "644820           0        1  0    night  weekday\n",
       "5048098          0        1  1    night  weekday\n",
       "7591736          1        1  0    night  weekend\n",
       "4896706          0        1  0    night  weekday\n",
       "2860665          0        1  0    night  weekday\n",
       "1364403          1        1  0    night  weekend\n",
       "2191873          0        1  0    night  weekday\n",
       "4966341          0        1  0    night  weekday\n",
       "2635147          0        1  0    night  weekday\n",
       "2969136          0        1  0    night  weekday\n",
       "2648097          0        1  0    night  weekday\n",
       "...            ...      ... ..      ...      ...\n",
       "475510           0        1  1    night  weekday\n",
       "865091           0        1  1    night  weekday\n",
       "3102731          0        1  1    night  weekday\n",
       "4885138          0        1  0    night  weekday\n",
       "7314718          0        1  1    night  weekday\n",
       "3723325          1        1  1    night  weekend\n",
       "7597246          1        1  0    night  weekend\n",
       "5527651          1        1  1    night  weekend\n",
       "6641081          0        1  0    night  weekday\n",
       "743708           0        1  1    night  weekday\n",
       "5525960          1        1  1    night  weekend\n",
       "6622950          0        1  1    night  weekday\n",
       "6975451          0        1  0    night  weekday\n",
       "2185037          0        1  1    night  weekday\n",
       "2190264          0        1  1    night  weekday\n",
       "5510839          1        1  0    night  weekend\n",
       "1127189          0        1  1    night  weekday\n",
       "3963249          0        1  1    night  weekday\n",
       "2198236          0        1  0    night  weekday\n",
       "472404           0        1  0    night  weekday\n",
       "5223740          0        1  1    night  weekday\n",
       "6316979          0        1  1    night  weekday\n",
       "4565660          0        1  0    night  weekday\n",
       "867497           0        1  0    night  weekday\n",
       "2788713          0        1  0    night  weekday\n",
       "5214201          0        1  0    night  weekday\n",
       "6623570          0        1  1    night  weekday\n",
       "6973117          0        1  1    night  weekday\n",
       "3427267          1        1  0    night  weekend\n",
       "5504232          1        1  0    night  weekend\n",
       "\n",
       "[8641672 rows x 5 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputDF \n",
    "isWeekend = []\n",
    "for val in inputDF[\"Week\"]:\n",
    "\n",
    "\n",
    "    if val == 'weekend':\n",
    "        sec = 1\n",
    "    else:\n",
    "        sec = 0\n",
    "    isWeekend.append(sec)\n",
    "#print(\"se\")\n",
    "inputDF.insert(0,'isWeekend',isWeekend)\n",
    "inputDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">tip_percent</th>\n",
       "      <th colspan=\"2\" halign=\"left\">tip_amount</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">fare_amount</th>\n",
       "      <th colspan=\"8\" halign=\"left\">total_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Section2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>resttime</th>\n",
       "      <td>3624090.0</td>\n",
       "      <td>0.157349</td>\n",
       "      <td>2.653446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208929</td>\n",
       "      <td>0.236901</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>3624090.0</td>\n",
       "      <td>1.867912</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8016.0</td>\n",
       "      <td>3624090.0</td>\n",
       "      <td>15.743352</td>\n",
       "      <td>13.810383</td>\n",
       "      <td>0.31</td>\n",
       "      <td>8.30</td>\n",
       "      <td>11.75</td>\n",
       "      <td>17.3</td>\n",
       "      <td>8016.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worktime</th>\n",
       "      <td>5017582.0</td>\n",
       "      <td>0.157571</td>\n",
       "      <td>8.257742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204156</td>\n",
       "      <td>0.229091</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>5017582.0</td>\n",
       "      <td>1.773433</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3013.5</td>\n",
       "      <td>5017582.0</td>\n",
       "      <td>15.212323</td>\n",
       "      <td>13.508602</td>\n",
       "      <td>0.31</td>\n",
       "      <td>8.16</td>\n",
       "      <td>11.16</td>\n",
       "      <td>16.3</td>\n",
       "      <td>3014.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tip_percent                                                    \\\n",
       "               count      mean       std  min  25%       50%       75%   \n",
       "Section2                                                                 \n",
       "resttime   3624090.0  0.157349  2.653446  0.0  0.0  0.208929  0.236901   \n",
       "worktime   5017582.0  0.157571  8.257742  0.0  0.0  0.204156  0.229091   \n",
       "\n",
       "                  tip_amount            ... fare_amount         total_amount  \\\n",
       "              max      count      mean  ...         75%     max        count   \n",
       "Section2                                ...                                    \n",
       "resttime   5000.0  3624090.0  1.867912  ...        14.0  8016.0    3624090.0   \n",
       "worktime  10000.0  5017582.0  1.773433  ...        13.0  3013.5    5017582.0   \n",
       "\n",
       "                                                                 \n",
       "               mean        std   min   25%    50%   75%     max  \n",
       "Section2                                                         \n",
       "resttime  15.743352  13.810383  0.31  8.30  11.75  17.3  8016.8  \n",
       "worktime  15.212323  13.508602  0.31  8.16  11.16  16.3  3014.3  \n",
       "\n",
       "[2 rows x 48 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tip_avg = data1.groupby('Section')['tip_percent','tip_amount','passenger_count','trip_distance','fare_amount','total_amount'].describe()\n",
    "tip_avg\n",
    "tip_avg2 = data1.groupby('Section2')['tip_percent','tip_amount','passenger_count','trip_distance','fare_amount','total_amount'].describe()\n",
    "tip_avg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 5)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "# ax = sns.boxplot(x=tips[\"total_bill\"])\n",
    "\n",
    "ax = sns.boxplot(x=\"Section\", y=\"tip_amount\", data=data1)\n",
    "ax.set(ylim=(0, 5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'fp-zyl'                       \n",
    "prefix = 'sagemaker/linear-loss-SGD'   \n",
    "s3_train_key = '{}/train/recordio-pb-data'.format(prefix)\n",
    "s3_train_path = os.path.join('s3://', bucket, s3_train_key)\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isWeekend</th>\n",
       "      <th>isNight</th>\n",
       "      <th>Y</th>\n",
       "      <th>DayNight</th>\n",
       "      <th>Week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2469978</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158535</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7623170</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485944</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789112</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182801</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913056</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331252</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5369427</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744761</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3442553</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719140</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443732</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356360</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114303</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930677</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195509</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622750</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5222268</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644820</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5048098</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7591736</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896706</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860665</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364403</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191873</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4966341</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635147</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969136</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648097</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475510</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865091</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3102731</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4885138</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7314718</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3723325</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597246</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527651</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6641081</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743708</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525960</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6622950</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6975451</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185037</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190264</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5510839</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127189</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963249</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198236</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472404</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223740</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316979</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4565660</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867497</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788713</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214201</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6623570</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6973117</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427267</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504232</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8641672 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         isWeekend  isNight  Y DayNight     Week\n",
       "2469978          0        1  0    night  weekday\n",
       "2158535          0        1  1    night  weekday\n",
       "7623170          1        1  1    night  weekend\n",
       "1485944          1        1  0    night  weekend\n",
       "6789112          0        1  1    night  weekday\n",
       "1182801          1        1  0    night  weekend\n",
       "1913056          0        1  1    night  weekday\n",
       "5331252          1        1  0    night  weekend\n",
       "5369427          1        1  0    night  weekend\n",
       "744761           0        1  1    night  weekday\n",
       "3442553          1        1  0    night  weekend\n",
       "3719140          0        1  0    night  weekday\n",
       "8443732          0        1  1    night  weekday\n",
       "356360           0        1  0    night  weekday\n",
       "1114303          0        1  1    night  weekday\n",
       "2930677          0        1  1    night  weekday\n",
       "2195509          0        1  0    night  weekday\n",
       "5622750          1        1  0    night  weekend\n",
       "5222268          1        1  0    night  weekend\n",
       "644820           0        1  0    night  weekday\n",
       "5048098          0        1  1    night  weekday\n",
       "7591736          1        1  0    night  weekend\n",
       "4896706          0        1  0    night  weekday\n",
       "2860665          0        1  0    night  weekday\n",
       "1364403          1        1  0    night  weekend\n",
       "2191873          0        1  0    night  weekday\n",
       "4966341          0        1  0    night  weekday\n",
       "2635147          0        1  0    night  weekday\n",
       "2969136          0        1  0    night  weekday\n",
       "2648097          0        1  0    night  weekday\n",
       "...            ...      ... ..      ...      ...\n",
       "475510           0        1  1    night  weekday\n",
       "865091           0        1  1    night  weekday\n",
       "3102731          0        1  1    night  weekday\n",
       "4885138          0        1  0    night  weekday\n",
       "7314718          0        1  1    night  weekday\n",
       "3723325          1        1  1    night  weekend\n",
       "7597246          1        1  0    night  weekend\n",
       "5527651          1        1  1    night  weekend\n",
       "6641081          0        1  0    night  weekday\n",
       "743708           0        1  1    night  weekday\n",
       "5525960          1        1  1    night  weekend\n",
       "6622950          0        1  1    night  weekday\n",
       "6975451          0        1  0    night  weekday\n",
       "2185037          0        1  1    night  weekday\n",
       "2190264          0        1  1    night  weekday\n",
       "5510839          1        1  0    night  weekend\n",
       "1127189          0        1  1    night  weekday\n",
       "3963249          0        1  1    night  weekday\n",
       "2198236          0        1  0    night  weekday\n",
       "472404           0        1  0    night  weekday\n",
       "5223740          0        1  1    night  weekday\n",
       "6316979          0        1  1    night  weekday\n",
       "4565660          0        1  0    night  weekday\n",
       "867497           0        1  0    night  weekday\n",
       "2788713          0        1  0    night  weekday\n",
       "5214201          0        1  0    night  weekday\n",
       "6623570          0        1  1    night  weekday\n",
       "6973117          0        1  1    night  weekday\n",
       "3427267          1        1  0    night  weekend\n",
       "5504232          1        1  0    night  weekend\n",
       "\n",
       "[8641672 rows x 5 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm access to s3 bucket\n",
    "\n",
    "# for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "#     print(obj.key)\n",
    "inputDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to the model training part, we used the default setting, where we used 70% of the data as the training set, the other rest of the 30% as the test set, in the linear learner notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "inputDF['isWeekend'] = inputDF['isWeekend'].astype('float64')\n",
    "inputDF['isNight'] = inputDF['isNight'].astype('float64')\n",
    "\n",
    "\n",
    "\n",
    "currentData = inputDF.as_matrix()\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(currentData)\n",
    "train_size = int(currentData.shape[0] * 0.7)\n",
    "train_features  = currentData[:train_size, 0:2]\n",
    "train_labels = currentData[:train_size, 2]\n",
    "test_features = currentData[train_size:, 0:2]\n",
    "test_labels = currentData[train_size:, 2]\n",
    "\n",
    "# train_features\n",
    "# Convert the processed training data to protobuf and write to S3 for linear learner\n",
    "vectors = np.array([t for t in train_features]).astype('float32')\n",
    "labels = np.array([t for t in train_labels]).astype('float32')\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, vectors, labels)\n",
    "buf.seek(0)\n",
    "boto3.resource('s3').Bucket(bucket).Object(s3_train_key).upload_fileobj(buf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the model training setup function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "def predictor_from_hyperparams(s3_train_data, hyperparams, output_path):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data, and return a deployed predictor\n",
    "    \"\"\"\n",
    "    # specify algorithm containers and instantiate an Estimator with given hyperparams\n",
    "    container = get_image_uri(boto3.Session().region_name, 'linear-learner')\n",
    "\n",
    "    linear = sagemaker.estimator.Estimator(container,\n",
    "        role,\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.c5.9xlarge',\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sagemaker.Session())\n",
    "    linear.set_hyperparameters(**hyperparams)\n",
    "    # train model\n",
    "    linear.fit({'train': s3_train_data})\n",
    "    # deploy a predictor\n",
    "    linear_predictor = linear.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "    linear_predictor.content_type = 'text/csv'\n",
    "    linear_predictor.serializer = csv_serializer\n",
    "    linear_predictor.deserializer = json_deserializer\n",
    "    return linear_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function for evaluating the parameters of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(linear_predictor, test_features, test_labels, model_name, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a test set given the prediction endpoint.  Return binary classification metrics.\n",
    "    \"\"\"\n",
    "    # split the test data set into 100 batches and evaluate using prediction endpoint\n",
    "    prediction_batches = [linear_predictor.predict(batch)['predictions'] for batch in np.array_split(test_features, 100)]\n",
    "    # parse raw predictions json to exctract predicted label\n",
    "    test_preds = np.concatenate([np.array([x['predicted_label'] for x in batch]) for batch in prediction_batches])\n",
    "    \n",
    "    # calculate true positives, false positives, true negatives, false negatives\n",
    "    tp = np.logical_and(test_labels, test_preds).sum()\n",
    "    fp = np.logical_and(1-test_labels, test_preds).sum()\n",
    "    tn = np.logical_and(1-test_labels, 1-test_preds).sum()\n",
    "    fn = np.logical_and(test_labels, 1-test_preds).sum()\n",
    "    \n",
    "    # calculate binary classification metrics\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    if verbose:\n",
    "        print(pd.crosstab(test_labels, test_preds, rownames=['actuals'], colnames=['predictions']))\n",
    "        print(\"\\n{:<11} {:.3f}\".format('Recall:', recall))\n",
    "        print(\"{:<11} {:.3f}\".format('Precision:', precision))\n",
    "        print(\"{:<11} {:.3f}\".format('Accuracy:', accuracy))\n",
    "        print(\"{:<11} {:.3f}\".format('F1:', f1))\n",
    "        \n",
    "    return {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn, 'Precision': precision, \n",
    "            'Recall': recall, 'Accuracy': accuracy, \n",
    "            'F1': f1, \n",
    "            'Model': model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_endpoint(predictor):\n",
    "        try:\n",
    "            boto3.client('sagemaker').delete_endpoint(EndpointName=predictor.endpoint)\n",
    "            print('Deleted {}'.format(predictor.endpoint))\n",
    "        except:\n",
    "            print('Already deleted: {}'.format(predictor.endpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose logistic regression due to the nature of our problem of interest. We would like to see how does the model predict whether a taxi driver receives a tip amount that is above the average tip amount that the population exhibits or not. This leaves us a perfect opportunity to apply logistic regression since our feature, also known as the dependent variable, is a binary variable, taking the only value of either 1 or 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-09 05:37:38 Starting - Starting the training job...\n",
      "2019-12-09 05:37:39 Starting - Launching requested ML instances......\n",
      "2019-12-09 05:38:47 Starting - Preparing the instances for training......\n",
      "2019-12-09 05:39:47 Downloading - Downloading input data...\n",
      "2019-12-09 05:40:28 Training - Downloading the training image..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:40:49 INFO 140431902009152] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'loss_insensitivity': u'0.01', u'epochs': u'15', u'feature_dim': u'auto', u'init_bias': u'0.0', u'lr_scheduler_factor': u'auto', u'num_calibration_samples': u'10000000', u'accuracy_top_k': u'3', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'num_point_for_scaler': u'10000', u'_log_level': u'info', u'quantile': u'0.5', u'bias_lr_mult': u'auto', u'lr_scheduler_step': u'auto', u'init_method': u'uniform', u'init_sigma': u'0.01', u'lr_scheduler_minimum_lr': u'auto', u'target_recall': u'0.8', u'num_models': u'auto', u'early_stopping_patience': u'3', u'momentum': u'auto', u'unbias_label': u'auto', u'wd': u'auto', u'optimizer': u'auto', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.001', u'learning_rate': u'auto', u'_kvstore': u'auto', u'normalize_data': u'true', u'binary_classifier_model_selection_criteria': u'accuracy', u'use_lr_scheduler': u'true', u'target_precision': u'0.8', u'unbias_data': u'auto', u'init_scale': u'0.07', u'bias_wd_mult': u'auto', u'f_beta': u'1.0', u'mini_batch_size': u'1000', u'huber_delta': u'1.0', u'num_classes': u'1', u'beta_1': u'auto', u'loss': u'auto', u'beta_2': u'auto', u'_enable_profiler': u'false', u'normalize_label': u'auto', u'_num_gpus': u'auto', u'balance_multiclass_weights': u'false', u'positive_example_weight_mult': u'1.0', u'l1': u'auto', u'margin': u'1.0'}\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:40:49 INFO 140431902009152] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'epochs': u'10', u'feature_dim': u'2', u'predictor_type': u'binary_classifier'}\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:40:49 INFO 140431902009152] Final configuration: {u'loss_insensitivity': u'0.01', u'epochs': u'10', u'feature_dim': u'2', u'init_bias': u'0.0', u'lr_scheduler_factor': u'auto', u'num_calibration_samples': u'10000000', u'accuracy_top_k': u'3', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'num_point_for_scaler': u'10000', u'_log_level': u'info', u'quantile': u'0.5', u'bias_lr_mult': u'auto', u'lr_scheduler_step': u'auto', u'init_method': u'uniform', u'init_sigma': u'0.01', u'lr_scheduler_minimum_lr': u'auto', u'target_recall': u'0.8', u'num_models': u'auto', u'early_stopping_patience': u'3', u'momentum': u'auto', u'unbias_label': u'auto', u'wd': u'auto', u'optimizer': u'auto', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.001', u'learning_rate': u'auto', u'_kvstore': u'auto', u'normalize_data': u'true', u'binary_classifier_model_selection_criteria': u'accuracy', u'use_lr_scheduler': u'true', u'target_precision': u'0.8', u'unbias_data': u'auto', u'init_scale': u'0.07', u'bias_wd_mult': u'auto', u'f_beta': u'1.0', u'mini_batch_size': u'1000', u'huber_delta': u'1.0', u'num_classes': u'1', u'predictor_type': u'binary_classifier', u'beta_1': u'auto', u'loss': u'auto', u'beta_2': u'auto', u'_enable_profiler': u'false', u'normalize_label': u'auto', u'_num_gpus': u'auto', u'balance_multiclass_weights': u'false', u'positive_example_weight_mult': u'1.0', u'l1': u'auto', u'margin': u'1.0'}\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:40:49 WARNING 140431902009152] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:40:49 INFO 140431902009152] Using default worker.\u001b[0m\n",
      "\u001b[34m[2019-12-09 05:40:50.071] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 25, \"num_examples\": 1, \"num_bytes\": 52000}\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:40:50 INFO 140431902009152] Create Store: local\u001b[0m\n",
      "\u001b[34m[2019-12-09 05:40:50.126] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 53, \"num_examples\": 11, \"num_bytes\": 572000}\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:40:50 INFO 140431902009152] Scaler algorithm parameters\n",
      " <algorithm.scaler.ScalerAlgorithmStable object at 0x7fb8884ea590>\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:40:50 INFO 140431902009152] Scaling model computed with parameters:\n",
      " {'stdev_weight': \u001b[0m\n",
      "\u001b[34m[0.4349889  0.45223308]\u001b[0m\n",
      "\u001b[34m<NDArray 2 @cpu(0)>, 'stdev_label': None, 'mean_label': None, 'mean_weight': \u001b[0m\n",
      "\u001b[34m[0.25345454 0.28672728]\u001b[0m\n",
      "\u001b[34m<NDArray 2 @cpu(0)>}\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:40:50 INFO 140431902009152] nvidia-smi took: 0.0252461433411 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:40:50 INFO 140431902009152] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}, \"Total Records Seen\": {\"count\": 1, \"max\": 12000, \"sum\": 12000.0, \"min\": 12000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 11000, \"sum\": 11000.0, \"min\": 11000}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1575870050.22962, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\"}, \"StartTime\": 1575870050.22958}\n",
      "\u001b[0m\n",
      "\n",
      "2019-12-09 05:40:47 Training - Training image download completed. Training in progress.\u001b[34m[2019-12-09 05:43:53.545] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 3, \"duration\": 183315, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6661069117284527, \"sum\": 0.6661069117284527, \"min\": 0.6661069117284527}}, \"EndTime\": 1575870233.54546, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.545355}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6662310061197002, \"sum\": 0.6662310061197002, \"min\": 0.6662310061197002}}, \"EndTime\": 1575870233.545566, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.545545}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6661074666045169, \"sum\": 0.6661074666045169, \"min\": 0.6661074666045169}}, \"EndTime\": 1575870233.545637, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.545619}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6662308986699764, \"sum\": 0.6662308986699764, \"min\": 0.6662308986699764}}, \"EndTime\": 1575870233.545706, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.545687}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6665041520554954, \"sum\": 0.6665041520554954, \"min\": 0.6665041520554954}}, \"EndTime\": 1575870233.545772, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.545754}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6681801762554858, \"sum\": 0.6681801762554858, \"min\": 0.6681801762554858}}, \"EndTime\": 1575870233.545836, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.545819}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6665034760575902, \"sum\": 0.6665034760575902, \"min\": 0.6665034760575902}}, \"EndTime\": 1575870233.5459, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.545882}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6681805834525044, \"sum\": 0.6681805834525044, \"min\": 0.6681805834525044}}, \"EndTime\": 1575870233.545966, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.545948}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6661126217595367, \"sum\": 0.6661126217595367, \"min\": 0.6661126217595367}}, \"EndTime\": 1575870233.546039, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.546021}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6662334598359007, \"sum\": 0.6662334598359007, \"min\": 0.6662334598359007}}, \"EndTime\": 1575870233.546136, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.546116}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6661105619718426, \"sum\": 0.6661105619718426, \"min\": 0.6661105619718426}}, \"EndTime\": 1575870233.546209, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.546191}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6662323257262144, \"sum\": 0.6662323257262144, \"min\": 0.6662323257262144}}, \"EndTime\": 1575870233.546278, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.54626}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6665015322054136, \"sum\": 0.6665015322054136, \"min\": 0.6665015322054136}}, \"EndTime\": 1575870233.546342, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.546324}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6681604547662998, \"sum\": 0.6681604547662998, \"min\": 0.6681604547662998}}, \"EndTime\": 1575870233.546406, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.546389}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6665018907074969, \"sum\": 0.6665018907074969, \"min\": 0.6665018907074969}}, \"EndTime\": 1575870233.546471, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.546453}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.668159986544215, \"sum\": 0.668159986544215, \"min\": 0.668159986544215}}, \"EndTime\": 1575870233.546535, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.546517}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672748086266842, \"sum\": 0.6672748086266842, \"min\": 0.6672748086266842}}, \"EndTime\": 1575870233.546599, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.546581}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6673758436957555, \"sum\": 0.6673758436957555, \"min\": 0.6673758436957555}}, \"EndTime\": 1575870233.546661, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.546644}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.667271114440807, \"sum\": 0.667271114440807, \"min\": 0.667271114440807}}, \"EndTime\": 1575870233.546724, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.546707}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6673742987868766, \"sum\": 0.6673742987868766, \"min\": 0.6673742987868766}}, \"EndTime\": 1575870233.546787, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.546771}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6675851908813214, \"sum\": 0.6675851908813214, \"min\": 0.6675851908813214}}, \"EndTime\": 1575870233.54685, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.546833}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.668920519422196, \"sum\": 0.668920519422196, \"min\": 0.668920519422196}}, \"EndTime\": 1575870233.546912, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.546896}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6675851029358062, \"sum\": 0.6675851029358062, \"min\": 0.6675851029358062}}, \"EndTime\": 1575870233.54698, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.546962}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6689206694219699, \"sum\": 0.6689206694219699, \"min\": 0.6689206694219699}}, \"EndTime\": 1575870233.547051, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.547032}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6678315540174351, \"sum\": 0.6678315540174351, \"min\": 0.6678315540174351}}, \"EndTime\": 1575870233.547122, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.547104}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6679304543392858, \"sum\": 0.6679304543392858, \"min\": 0.6679304543392858}}, \"EndTime\": 1575870233.54719, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.547173}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6678334420611394, \"sum\": 0.6678334420611394, \"min\": 0.6678334420611394}}, \"EndTime\": 1575870233.547256, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.54724}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6679309588656345, \"sum\": 0.6679309588656345, \"min\": 0.6679309588656345}}, \"EndTime\": 1575870233.547317, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.5473}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.668130530003103, \"sum\": 0.668130530003103, \"min\": 0.668130530003103}}, \"EndTime\": 1575870233.547387, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.54737}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6694113808075521, \"sum\": 0.6694113808075521, \"min\": 0.6694113808075521}}, \"EndTime\": 1575870233.547453, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.547436}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6681324525550291, \"sum\": 0.6681324525550291, \"min\": 0.6681324525550291}}, \"EndTime\": 1575870233.547517, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.547499}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6694117637176438, \"sum\": 0.6694117637176438, \"min\": 0.6694117637176438}}, \"EndTime\": 1575870233.547581, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870233.547563}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:43:53 INFO 140431902009152] #quality_metric: host=algo-1, epoch=0, train binary_classification_cross_entropy_objective <loss>=0.666106911728\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:43:53 INFO 140431902009152] #early_stopping_criteria_metric: host=algo-1, epoch=0, criteria=binary_classification_cross_entropy_objective, value=0.666106911728\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:43:53 INFO 140431902009152] Epoch 0: Loss improved. Updating best model\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:43:53 INFO 140431902009152] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6062, \"sum\": 6062.0, \"min\": 6062}, \"Total Records Seen\": {\"count\": 1, \"max\": 6061170, \"sum\": 6061170.0, \"min\": 6061170}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1575870233.551347, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575870050.229845}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:43:53 INFO 140431902009152] #throughput_metric: host=algo-1, train throughput=32997.5769839 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 05:46:57.823] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 184270, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660607022830801, \"sum\": 0.6660607022830801, \"min\": 0.6660607022830801}}, \"EndTime\": 1575870417.824087, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.82399}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6661476052666246, \"sum\": 0.6661476052666246, \"min\": 0.6661476052666246}}, \"EndTime\": 1575870417.824193, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.824171}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660633002375942, \"sum\": 0.6660633002375942, \"min\": 0.6660633002375942}}, \"EndTime\": 1575870417.824267, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.824248}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6661476049134704, \"sum\": 0.6661476049134704, \"min\": 0.6661476049134704}}, \"EndTime\": 1575870417.824331, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.824313}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660616074175968, \"sum\": 0.6660616074175968, \"min\": 0.6660616074175968}}, \"EndTime\": 1575870417.824397, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.824379}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672730462050158, \"sum\": 0.6672730462050158, \"min\": 0.6672730462050158}}, \"EndTime\": 1575870417.82446, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.824443}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660637073740721, \"sum\": 0.6660637073740721, \"min\": 0.6660637073740721}}, \"EndTime\": 1575870417.824526, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.824508}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.667273046144475, \"sum\": 0.667273046144475, \"min\": 0.667273046144475}}, \"EndTime\": 1575870417.824592, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.824574}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660637334469508, \"sum\": 0.6660637334469508, \"min\": 0.6660637334469508}}, \"EndTime\": 1575870417.824658, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.824639}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6661502653056678, \"sum\": 0.6661502653056678, \"min\": 0.6661502653056678}}, \"EndTime\": 1575870417.824719, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.824702}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660666289390227, \"sum\": 0.6660666289390227, \"min\": 0.6660666289390227}}, \"EndTime\": 1575870417.824783, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.824765}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6661502652047665, \"sum\": 0.6661502652047665, \"min\": 0.6661502652047665}}, \"EndTime\": 1575870417.824847, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.824829}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660655186319375, \"sum\": 0.6660655186319375, \"min\": 0.6660655186319375}}, \"EndTime\": 1575870417.824914, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.824897}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672640922396296, \"sum\": 0.6672640922396296, \"min\": 0.6672640922396296}}, \"EndTime\": 1575870417.824978, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.824961}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660673951527208, \"sum\": 0.6660673951527208, \"min\": 0.6660673951527208}}, \"EndTime\": 1575870417.825042, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825024}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672640925524235, \"sum\": 0.6672640925524235, \"min\": 0.6672640925524235}}, \"EndTime\": 1575870417.825107, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825089}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672323285615391, \"sum\": 0.6672323285615391, \"min\": 0.6672323285615391}}, \"EndTime\": 1575870417.82517, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825152}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6673049320341398, \"sum\": 0.6673049320341398, \"min\": 0.6673049320341398}}, \"EndTime\": 1575870417.825243, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825226}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672343759888321, \"sum\": 0.6672343759888321, \"min\": 0.6672343759888321}}, \"EndTime\": 1575870417.825303, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825287}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6673049324478348, \"sum\": 0.6673049324478348, \"min\": 0.6673049324478348}}, \"EndTime\": 1575870417.825368, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825351}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672331720151317, \"sum\": 0.6672331720151317, \"min\": 0.6672331720151317}}, \"EndTime\": 1575870417.82543, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825413}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6682006594178144, \"sum\": 0.6682006594178144, \"min\": 0.6682006594178144}}, \"EndTime\": 1575870417.825492, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825475}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672345311850196, \"sum\": 0.6672345311850196, \"min\": 0.6672345311850196}}, \"EndTime\": 1575870417.825553, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825536}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6682006598315094, \"sum\": 0.6682006598315094, \"min\": 0.6682006598315094}}, \"EndTime\": 1575870417.825614, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825599}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6677937067819836, \"sum\": 0.6677937067819836, \"min\": 0.6677937067819836}}, \"EndTime\": 1575870417.825678, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825661}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6678636902733309, \"sum\": 0.6678636902733309, \"min\": 0.6678636902733309}}, \"EndTime\": 1575870417.825738, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825722}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6677953296269754, \"sum\": 0.6677953296269754, \"min\": 0.6677953296269754}}, \"EndTime\": 1575870417.825798, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825782}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.667863693300368, \"sum\": 0.667863693300368, \"min\": 0.667863693300368}}, \"EndTime\": 1575870417.82586, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825842}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6677942339001184, \"sum\": 0.6677942339001184, \"min\": 0.6677942339001184}}, \"EndTime\": 1575870417.82592, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825904}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.668728090568463, \"sum\": 0.668728090568463, \"min\": 0.668728090568463}}, \"EndTime\": 1575870417.825981, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.825964}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6677956686753008, \"sum\": 0.6677956686753008, \"min\": 0.6677956686753008}}, \"EndTime\": 1575870417.826039, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.826022}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6687280906794544, \"sum\": 0.6687280906794544, \"min\": 0.6687280906794544}}, \"EndTime\": 1575870417.826139, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870417.826121}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:46:57 INFO 140431902009152] #quality_metric: host=algo-1, epoch=1, train binary_classification_cross_entropy_objective <loss>=0.666060702283\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:46:57 INFO 140431902009152] #early_stopping_criteria_metric: host=algo-1, epoch=1, criteria=binary_classification_cross_entropy_objective, value=0.666060702283\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:46:57 INFO 140431902009152] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12112, \"sum\": 12112.0, \"min\": 12112}, \"Total Records Seen\": {\"count\": 1, \"max\": 12110340, \"sum\": 12110340.0, \"min\": 12110340}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1575870417.828395, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575870233.553677}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:46:57 INFO 140431902009152] #throughput_metric: host=algo-1, train throughput=32826.8868136 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 05:50:03.541] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 7, \"duration\": 185708, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660605334248656, \"sum\": 0.6660605334248656, \"min\": 0.6660605334248656}}, \"EndTime\": 1575870603.541747, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.541638}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6661087811558793, \"sum\": 0.6661087811558793, \"min\": 0.6661087811558793}}, \"EndTime\": 1575870603.541836, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.541815}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660633007421003, \"sum\": 0.6660633007421003, \"min\": 0.6660633007421003}}, \"EndTime\": 1575870603.541902, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.541884}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.66610878120633, \"sum\": 0.66610878120633, \"min\": 0.66610878120633}}, \"EndTime\": 1575870603.541965, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.541948}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660605057072967, \"sum\": 0.6660605057072967, \"min\": 0.6660605057072967}}, \"EndTime\": 1575870603.542005, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.541995}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6668316038424643, \"sum\": 0.6668316038424643, \"min\": 0.6668316038424643}}, \"EndTime\": 1575870603.542082, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.542045}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660633001669632, \"sum\": 0.6660633001669632, \"min\": 0.6660633001669632}}, \"EndTime\": 1575870603.542143, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.542128}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6668316035902112, \"sum\": 0.6668316035902112, \"min\": 0.6668316035902112}}, \"EndTime\": 1575870603.542186, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.542176}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.66606376201209, \"sum\": 0.66606376201209, \"min\": 0.66606376201209}}, \"EndTime\": 1575870603.54223, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.542215}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.666111727330641, \"sum\": 0.666111727330641, \"min\": 0.666111727330641}}, \"EndTime\": 1575870603.542291, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.542275}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660666313303819, \"sum\": 0.6660666313303819, \"min\": 0.6660666313303819}}, \"EndTime\": 1575870603.542352, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.542335}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6661117274315421, \"sum\": 0.6661117274315421, \"min\": 0.6661117274315421}}, \"EndTime\": 1575870603.542394, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.542384}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660638258926611, \"sum\": 0.6660638258926611, \"min\": 0.6660638258926611}}, \"EndTime\": 1575870603.542445, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.54243}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6668273712778746, \"sum\": 0.6668273712778746, \"min\": 0.6668273712778746}}, \"EndTime\": 1575870603.542498, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.542483}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.666066631501914, \"sum\": 0.666066631501914, \"min\": 0.666066631501914}}, \"EndTime\": 1575870603.542558, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.54254}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6668273720850845, \"sum\": 0.6668273720850845, \"min\": 0.6668273720850845}}, \"EndTime\": 1575870603.542618, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.542602}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672325254097559, \"sum\": 0.6672325254097559, \"min\": 0.6672325254097559}}, \"EndTime\": 1575870603.542679, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.542663}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672737569936663, \"sum\": 0.6672737569936663, \"min\": 0.6672737569936663}}, \"EndTime\": 1575870603.54275, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.542732}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672343769070334, \"sum\": 0.6672343769070334, \"min\": 0.6672343769070334}}, \"EndTime\": 1575870603.542814, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.542796}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672737569936663, \"sum\": 0.6672737569936663, \"min\": 0.6672737569936663}}, \"EndTime\": 1575870603.54288, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.542862}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.667232542189631, \"sum\": 0.667232542189631, \"min\": 0.667232542189631}}, \"EndTime\": 1575870603.542955, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.542937}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.667864434964796, \"sum\": 0.667864434964796, \"min\": 0.667864434964796}}, \"EndTime\": 1575870603.543031, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.543012}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672343769877543, \"sum\": 0.6672343769877543, \"min\": 0.6672343769877543}}, \"EndTime\": 1575870603.543106, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.543088}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6678644350556071, \"sum\": 0.6678644350556071, \"min\": 0.6678644350556071}}, \"EndTime\": 1575870603.543181, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.543162}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6677936022180352, \"sum\": 0.6677936022180352, \"min\": 0.6677936022180352}}, \"EndTime\": 1575870603.543255, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.543237}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6678330068942988, \"sum\": 0.6678330068942988, \"min\": 0.6678330068942988}}, \"EndTime\": 1575870603.54334, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.543322}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6677953355902383, \"sum\": 0.6677953355902383, \"min\": 0.6677953355902383}}, \"EndTime\": 1575870603.5434, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.543384}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6678330068741185, \"sum\": 0.6678330068741185, \"min\": 0.6678330068741185}}, \"EndTime\": 1575870603.543459, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.543442}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6677936019758721, \"sum\": 0.6677936019758721, \"min\": 0.6677936019758721}}, \"EndTime\": 1575870603.543521, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.543504}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.668402678348581, \"sum\": 0.668402678348581, \"min\": 0.668402678348581}}, \"EndTime\": 1575870603.543584, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.543567}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6677953356104186, \"sum\": 0.6677953356104186, \"min\": 0.6677953356104186}}, \"EndTime\": 1575870603.543645, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.543628}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6684026783990316, \"sum\": 0.6684026783990316, \"min\": 0.6684026783990316}}, \"EndTime\": 1575870603.543698, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870603.543683}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:50:03 INFO 140431902009152] #quality_metric: host=algo-1, epoch=2, train binary_classification_cross_entropy_objective <loss>=0.666060533425\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:50:03 INFO 140431902009152] #early_stopping_criteria_metric: host=algo-1, epoch=2, criteria=binary_classification_cross_entropy_objective, value=0.666060505707\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:50:03 INFO 140431902009152] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 18162, \"sum\": 18162.0, \"min\": 18162}, \"Total Records Seen\": {\"count\": 1, \"max\": 18159510, \"sum\": 18159510.0, \"min\": 18159510}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1575870603.546085, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575870417.833268}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:50:03 INFO 140431902009152] #throughput_metric: host=algo-1, train throughput=32572.6881205 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 05:53:09.179] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 9, \"duration\": 185621, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660605191069805, \"sum\": 0.6660605191069805, \"min\": 0.6660605191069805}}, \"EndTime\": 1575870789.180147, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.180026}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660863140634041, \"sum\": 0.6660863140634041, \"min\": 0.6660863140634041}}, \"EndTime\": 1575870789.180314, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.180288}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660633009338127, \"sum\": 0.6660633009338127, \"min\": 0.6660633009338127}}, \"EndTime\": 1575870789.180386, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.180368}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660863140634041, \"sum\": 0.6660863140634041, \"min\": 0.6660863140634041}}, \"EndTime\": 1575870789.180454, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.180435}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660605082903682, \"sum\": 0.6660605082903682, \"min\": 0.6660605082903682}}, \"EndTime\": 1575870789.180524, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.180506}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6665119506412153, \"sum\": 0.6665119506412153, \"min\": 0.6665119506412153}}, \"EndTime\": 1575870789.180576, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.180565}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660633008329114, \"sum\": 0.6660633008329114, \"min\": 0.6660633008329114}}, \"EndTime\": 1575870789.180629, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.180613}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6665119508127473, \"sum\": 0.6665119508127473, \"min\": 0.6665119508127473}}, \"EndTime\": 1575870789.180682, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.180672}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660638240562586, \"sum\": 0.6660638240562586, \"min\": 0.6660638240562586}}, \"EndTime\": 1575870789.180715, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.180706}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660894049305347, \"sum\": 0.6660894049305347, \"min\": 0.6660894049305347}}, \"EndTime\": 1575870789.180745, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.180737}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660666311083993, \"sum\": 0.6660666311083993, \"min\": 0.6660666311083993}}, \"EndTime\": 1575870789.18079, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.180774}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660894049305347, \"sum\": 0.6660894049305347, \"min\": 0.6660894049305347}}, \"EndTime\": 1575870789.180851, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.180835}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660638498869744, \"sum\": 0.6660638498869744, \"min\": 0.6660638498869744}}, \"EndTime\": 1575870789.180914, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.180898}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6665105891707791, \"sum\": 0.6665105891707791, \"min\": 0.6665105891707791}}, \"EndTime\": 1575870789.180978, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.180961}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660666310680388, \"sum\": 0.6660666310680388, \"min\": 0.6660666310680388}}, \"EndTime\": 1575870789.181079, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.181061}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6665105895542038, \"sum\": 0.6665105895542038, \"min\": 0.6665105895542038}}, \"EndTime\": 1575870789.181144, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.181126}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672325284367929, \"sum\": 0.6672325284367929, \"min\": 0.6672325284367929}}, \"EndTime\": 1575870789.181207, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.18119}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672544447598842, \"sum\": 0.6672544447598842, \"min\": 0.6672544447598842}}, \"EndTime\": 1575870789.181259, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.181244}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.667234376886853, \"sum\": 0.667234376886853, \"min\": 0.667234376886853}}, \"EndTime\": 1575870789.181311, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.181295}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672544447598842, \"sum\": 0.6672544447598842, \"min\": 0.6672544447598842}}, \"EndTime\": 1575870789.181372, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.181355}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672325287899472, \"sum\": 0.6672325287899472, \"min\": 0.6672325287899472}}, \"EndTime\": 1575870789.181431, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.181415}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6676069191446934, \"sum\": 0.6676069191446934, \"min\": 0.6676069191446934}}, \"EndTime\": 1575870789.181501, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.181483}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672343768767629, \"sum\": 0.6672343768767629, \"min\": 0.6672343768767629}}, \"EndTime\": 1575870789.181566, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.181549}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6676069188823501, \"sum\": 0.6676069188823501, \"min\": 0.6676069188823501}}, \"EndTime\": 1575870789.181639, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.18162}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.667793602127224, \"sum\": 0.667793602127224, \"min\": 0.667793602127224}}, \"EndTime\": 1575870789.181713, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.181695}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6678145232311772, \"sum\": 0.6678145232311772, \"min\": 0.6678145232311772}}, \"EndTime\": 1575870789.181787, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.181769}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.667795335559968, \"sum\": 0.667795335559968, \"min\": 0.667795335559968}}, \"EndTime\": 1575870789.18186, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.181842}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6678145232311772, \"sum\": 0.6678145232311772, \"min\": 0.6678145232311772}}, \"EndTime\": 1575870789.181933, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.181914}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6677936020565932, \"sum\": 0.6677936020565932, \"min\": 0.6677936020565932}}, \"EndTime\": 1575870789.182005, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.181986}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6681522829973672, \"sum\": 0.6681522829973672, \"min\": 0.6681522829973672}}, \"EndTime\": 1575870789.182091, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.182073}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6677953356205086, \"sum\": 0.6677953356205086, \"min\": 0.6677953356205086}}, \"EndTime\": 1575870789.18216, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.182142}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6681522831890796, \"sum\": 0.6681522831890796, \"min\": 0.6681522831890796}}, \"EndTime\": 1575870789.182222, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870789.182205}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:53:09 INFO 140431902009152] #quality_metric: host=algo-1, epoch=3, train binary_classification_cross_entropy_objective <loss>=0.666060519107\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:53:09 INFO 140431902009152] #early_stopping_criteria_metric: host=algo-1, epoch=3, criteria=binary_classification_cross_entropy_objective, value=0.66606050829\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:53:09 INFO 140431902009152] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 24212, \"sum\": 24212.0, \"min\": 24212}, \"Total Records Seen\": {\"count\": 1, \"max\": 24208680, \"sum\": 24208680.0, \"min\": 24208680}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1575870789.18459, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575870603.558476}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:53:09 INFO 140431902009152] #throughput_metric: host=algo-1, train throughput=32587.899123 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 05:56:12.121] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 182924, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660605142839016, \"sum\": 0.6660605142839016, \"min\": 0.6660605142839016}}, \"EndTime\": 1575870972.121721, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.121623}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660737636551658, \"sum\": 0.6660737636551658, \"min\": 0.6660737636551658}}, \"EndTime\": 1575870972.1218, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.121786}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660633009338127, \"sum\": 0.6660633009338127, \"min\": 0.6660633009338127}}, \"EndTime\": 1575870972.121865, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.121849}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660737636551658, \"sum\": 0.6660737636551658, \"min\": 0.6660737636551658}}, \"EndTime\": 1575870972.121922, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.12191}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660605095314535, \"sum\": 0.6660605095314535, \"min\": 0.6660605095314535}}, \"EndTime\": 1575870972.121957, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.121948}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6663312592950217, \"sum\": 0.6663312592950217, \"min\": 0.6663312592950217}}, \"EndTime\": 1575870972.12199, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.121982}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660633009338127, \"sum\": 0.6660633009338127, \"min\": 0.6660633009338127}}, \"EndTime\": 1575870972.122038, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122022}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6663312589418673, \"sum\": 0.6663312589418673, \"min\": 0.6663312589418673}}, \"EndTime\": 1575870972.122124, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122107}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660638514509436, \"sum\": 0.6660638514509436, \"min\": 0.6660638514509436}}, \"EndTime\": 1575870972.122187, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.12217}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660769819807348, \"sum\": 0.6660769819807348, \"min\": 0.6660769819807348}}, \"EndTime\": 1575870972.122233, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122217}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660666311083993, \"sum\": 0.6660666311083993, \"min\": 0.6660666311083993}}, \"EndTime\": 1575870972.122283, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122273}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660769819807348, \"sum\": 0.6660769819807348, \"min\": 0.6660769819807348}}, \"EndTime\": 1575870972.122329, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122313}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660638624996287, \"sum\": 0.6660638624996287, \"min\": 0.6660638624996287}}, \"EndTime\": 1575870972.122386, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122371}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6663321816130184, \"sum\": 0.6663321816130184, \"min\": 0.6663321816130184}}, \"EndTime\": 1575870972.122441, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122425}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6660666311083993, \"sum\": 0.6660666311083993, \"min\": 0.6660666311083993}}, \"EndTime\": 1575870972.122479, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.12247}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6663321817542801, \"sum\": 0.6663321817542801, \"min\": 0.6663321817542801}}, \"EndTime\": 1575870972.122525, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122514}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672325284166126, \"sum\": 0.6672325284166126, \"min\": 0.6672325284166126}}, \"EndTime\": 1575870972.122572, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122561}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672432697666324, \"sum\": 0.6672432697666324, \"min\": 0.6672432697666324}}, \"EndTime\": 1575870972.122625, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122609}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.667234376886853, \"sum\": 0.667234376886853, \"min\": 0.667234376886853}}, \"EndTime\": 1575870972.122685, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122669}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672432697666324, \"sum\": 0.6672432697666324, \"min\": 0.6672432697666324}}, \"EndTime\": 1575870972.122727, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122718}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6672325284166126, \"sum\": 0.6672325284166126, \"min\": 0.6672325284166126}}, \"EndTime\": 1575870972.12276, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122751}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6674567306450446, \"sum\": 0.6674567306450446, \"min\": 0.6674567306450446}}, \"EndTime\": 1575870972.122804, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122789}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.667234376886853, \"sum\": 0.667234376886853, \"min\": 0.667234376886853}}, \"EndTime\": 1575870972.122872, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122855}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6674567310284694, \"sum\": 0.6674567310284694, \"min\": 0.6674567310284694}}, \"EndTime\": 1575870972.122935, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122918}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6677936021171339, \"sum\": 0.6677936021171339, \"min\": 0.6677936021171339}}, \"EndTime\": 1575870972.122996, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.122979}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6678039138902414, \"sum\": 0.6678039138902414, \"min\": 0.6678039138902414}}, \"EndTime\": 1575870972.123064, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.123047}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.667795335559968, \"sum\": 0.667795335559968, \"min\": 0.667795335559968}}, \"EndTime\": 1575870972.123124, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.123108}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6678039138902414, \"sum\": 0.6678039138902414, \"min\": 0.6678039138902414}}, \"EndTime\": 1575870972.12319, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.123173}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6677936021171339, \"sum\": 0.6677936021171339, \"min\": 0.6677936021171339}}, \"EndTime\": 1575870972.123254, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.123237}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6680105570236461, \"sum\": 0.6680105570236461, \"min\": 0.6680105570236461}}, \"EndTime\": 1575870972.123306, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.12329}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.667795335559968, \"sum\": 0.667795335559968, \"min\": 0.667795335559968}}, \"EndTime\": 1575870972.123369, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.123352}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_cross_entropy_objective\": {\"count\": 1, \"max\": 0.6680105570740967, \"sum\": 0.6680105570740967, \"min\": 0.6680105570740967}}, \"EndTime\": 1575870972.123423, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870972.123406}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:12 INFO 140431902009152] #quality_metric: host=algo-1, epoch=4, train binary_classification_cross_entropy_objective <loss>=0.666060514284\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:12 INFO 140431902009152] #early_stopping_criteria_metric: host=algo-1, epoch=4, criteria=binary_classification_cross_entropy_objective, value=0.666060509531\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:12 INFO 140431902009152] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:12 INFO 140431902009152] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 30262, \"sum\": 30262.0, \"min\": 30262}, \"Total Records Seen\": {\"count\": 1, \"max\": 30257850, \"sum\": 30257850.0, \"min\": 30257850}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1575870972.125797, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575870789.197133}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:12 INFO 140431902009152] #throughput_metric: host=algo-1, train throughput=33068.4401114 records/second\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:12 WARNING 140431902009152] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:12 WARNING 140431902009152] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[2019-12-09 05:56:12.133] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 13, \"duration\": 5, \"num_examples\": 1, \"num_bytes\": 52000}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 05:56:30.727] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 18590, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m[2019-12-09 05:56:53.377] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 20651, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:53 INFO 140431902009152] #train_score (algo-1) : ('binary_classification_cross_entropy_objective', 0.6660646052747393)\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:53 INFO 140431902009152] #train_score (algo-1) : ('binary_classification_accuracy', 0.6120975274293828)\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:53 INFO 140431902009152] #train_score (algo-1) : ('binary_f_1.000', 0.0)\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:53 INFO 140431902009152] #train_score (algo-1) : ('precision', nan)\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:53 INFO 140431902009152] #train_score (algo-1) : ('recall', 0.0)\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:53 INFO 140431902009152] #quality_metric: host=algo-1, train binary_classification_cross_entropy_objective <loss>=0.666064605275\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:53 INFO 140431902009152] #quality_metric: host=algo-1, train binary_classification_accuracy <score>=0.612097527429\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:53 INFO 140431902009152] #quality_metric: host=algo-1, train binary_f_1.000 <score>=0.0\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:53 INFO 140431902009152] #quality_metric: host=algo-1, train precision <score>=nan\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:53 INFO 140431902009152] #quality_metric: host=algo-1, train recall <score>=0.0\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:53 INFO 140431902009152] Best model found for hyperparameters: {\"lr_scheduler_step\": 10, \"wd\": 0.0001, \"optimizer\": \"adam\", \"lr_scheduler_factor\": 0.99, \"l1\": 0.0, \"learning_rate\": 0.005, \"lr_scheduler_minimum_lr\": 1e-05}\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:53 INFO 140431902009152] Saved checkpoint to \"/tmp/tmpUDEkE8/mx-mod-0000.params\"\u001b[0m\n",
      "\u001b[34m[12/09/2019 05:56:53 INFO 140431902009152] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 963427.0398616791, \"sum\": 963427.0398616791, \"min\": 963427.0398616791}, \"finalize.time\": {\"count\": 1, \"max\": 41252.893924713135, \"sum\": 41252.893924713135, \"min\": 41252.893924713135}, \"initialize.time\": {\"count\": 1, \"max\": 178.7729263305664, \"sum\": 178.7729263305664, \"min\": 178.7729263305664}, \"check_early_stopping.time\": {\"count\": 5, \"max\": 1.1129379272460938, \"sum\": 1.9431114196777344, \"min\": 0.20313262939453125}, \"setuptime\": {\"count\": 1, \"max\": 28.00893783569336, \"sum\": 28.00893783569336, \"min\": 28.00893783569336}, \"update.time\": {\"count\": 5, \"max\": 185712.6281261444, \"sum\": 921862.802028656, \"min\": 182928.386926651}, \"epochs\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1575871013.386461, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\"}, \"StartTime\": 1575870050.04526}\n",
      "\u001b[0m\n",
      "\n",
      "2019-12-09 05:56:59 Uploading - Uploading generated training model\n",
      "2019-12-09 05:56:59 Completed - Training job completed\n",
      "Training seconds: 1032\n",
      "Billable seconds: 1032\n",
      "---------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# Training a binary classifier with default settings: logistic regression\n",
    "defaults_hyperparams = {\n",
    "    'feature_dim': 2,\n",
    "    'predictor_type': 'binary_classifier',\n",
    "    'epochs': 10\n",
    "}\n",
    "defaults_output_path = 's3://{}/{}/defaults/output'.format(bucket, prefix)\n",
    "defaults_predictor = predictor_from_hyperparams(s3_train_path, defaults_hyperparams, defaults_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we were trying to examine the results for the default logistic regression, there existed an error where precision cannot be calculated due to *float divided by 0*. Therefore, we employed binary classifier with class weights for our next step. This time, it also gave us some interesting results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-09 06:06:15 Starting - Starting the training job...\n",
      "2019-12-09 06:06:17 Starting - Launching requested ML instances......\n",
      "2019-12-09 06:07:24 Starting - Preparing the instances for training............\n",
      "2019-12-09 06:09:39 Downloading - Downloading input data\n",
      "2019-12-09 06:09:39 Training - Downloading the training image..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:09:55 INFO 140494632019776] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'loss_insensitivity': u'0.01', u'epochs': u'15', u'feature_dim': u'auto', u'init_bias': u'0.0', u'lr_scheduler_factor': u'auto', u'num_calibration_samples': u'10000000', u'accuracy_top_k': u'3', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'num_point_for_scaler': u'10000', u'_log_level': u'info', u'quantile': u'0.5', u'bias_lr_mult': u'auto', u'lr_scheduler_step': u'auto', u'init_method': u'uniform', u'init_sigma': u'0.01', u'lr_scheduler_minimum_lr': u'auto', u'target_recall': u'0.8', u'num_models': u'auto', u'early_stopping_patience': u'3', u'momentum': u'auto', u'unbias_label': u'auto', u'wd': u'auto', u'optimizer': u'auto', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.001', u'learning_rate': u'auto', u'_kvstore': u'auto', u'normalize_data': u'true', u'binary_classifier_model_selection_criteria': u'accuracy', u'use_lr_scheduler': u'true', u'target_precision': u'0.8', u'unbias_data': u'auto', u'init_scale': u'0.07', u'bias_wd_mult': u'auto', u'f_beta': u'1.0', u'mini_batch_size': u'1000', u'huber_delta': u'1.0', u'num_classes': u'1', u'beta_1': u'auto', u'loss': u'auto', u'beta_2': u'auto', u'_enable_profiler': u'false', u'normalize_label': u'auto', u'_num_gpus': u'auto', u'balance_multiclass_weights': u'false', u'positive_example_weight_mult': u'1.0', u'l1': u'auto', u'margin': u'1.0'}\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:09:55 INFO 140494632019776] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'predictor_type': u'binary_classifier', u'feature_dim': u'2', u'binary_classifier_model_selection_criteria': u'precision_at_target_recall', u'epochs': u'30', u'positive_example_weight_mult': u'balanced', u'target_recall': u'0.9'}\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:09:55 INFO 140494632019776] Final configuration: {u'loss_insensitivity': u'0.01', u'epochs': u'30', u'feature_dim': u'2', u'init_bias': u'0.0', u'lr_scheduler_factor': u'auto', u'num_calibration_samples': u'10000000', u'accuracy_top_k': u'3', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'num_point_for_scaler': u'10000', u'_log_level': u'info', u'quantile': u'0.5', u'bias_lr_mult': u'auto', u'lr_scheduler_step': u'auto', u'init_method': u'uniform', u'init_sigma': u'0.01', u'lr_scheduler_minimum_lr': u'auto', u'target_recall': u'0.9', u'num_models': u'auto', u'early_stopping_patience': u'3', u'momentum': u'auto', u'unbias_label': u'auto', u'wd': u'auto', u'optimizer': u'auto', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.001', u'learning_rate': u'auto', u'_kvstore': u'auto', u'normalize_data': u'true', u'binary_classifier_model_selection_criteria': u'precision_at_target_recall', u'use_lr_scheduler': u'true', u'target_precision': u'0.8', u'unbias_data': u'auto', u'init_scale': u'0.07', u'bias_wd_mult': u'auto', u'f_beta': u'1.0', u'mini_batch_size': u'1000', u'huber_delta': u'1.0', u'num_classes': u'1', u'predictor_type': u'binary_classifier', u'beta_1': u'auto', u'loss': u'auto', u'beta_2': u'auto', u'_enable_profiler': u'false', u'normalize_label': u'auto', u'_num_gpus': u'auto', u'balance_multiclass_weights': u'false', u'positive_example_weight_mult': u'balanced', u'l1': u'auto', u'margin': u'1.0'}\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:09:55 WARNING 140494632019776] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:09:55 INFO 140494632019776] Using default worker.\u001b[0m\n",
      "\u001b[34m[2019-12-09 06:09:55.197] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 22, \"num_examples\": 1, \"num_bytes\": 52000}\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:09:55 INFO 140494632019776] Create Store: local\u001b[0m\n",
      "\u001b[34m[2019-12-09 06:09:55.374] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 175, \"num_examples\": 11, \"num_bytes\": 572000}\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:09:55 INFO 140494632019776] Scaler algorithm parameters\n",
      " <algorithm.scaler.ScalerAlgorithmStable object at 0x7fc7212c55d0>\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:09:55 INFO 140494632019776] Scaling model computed with parameters:\n",
      " {'stdev_weight': \u001b[0m\n",
      "\u001b[34m[0.4349889  0.45223308]\u001b[0m\n",
      "\u001b[34m<NDArray 2 @cpu(0)>, 'stdev_label': None, 'mean_label': None, 'mean_weight': \u001b[0m\n",
      "\u001b[34m[0.25345454 0.28672728]\u001b[0m\n",
      "\u001b[34m<NDArray 2 @cpu(0)>}\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:09:55 INFO 140494632019776] nvidia-smi took: 0.0252549648285 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:09:55 INFO 140494632019776] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}, \"Total Records Seen\": {\"count\": 1, \"max\": 12000, \"sum\": 12000.0, \"min\": 12000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 11000, \"sum\": 11000.0, \"min\": 11000}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1575871795.510386, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\"}, \"StartTime\": 1575871795.510355}\n",
      "\u001b[0m\n",
      "\n",
      "2019-12-09 06:09:52 Training - Training image download completed. Training in progress.\u001b[34m[2019-12-09 06:11:33.455] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 3, \"duration\": 97944, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344262253365097, \"sum\": 0.8344262253365097, \"min\": 0.8344262253365097}}, \"EndTime\": 1575871893.455293, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455236}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8345857091907786, \"sum\": 0.8345857091907786, \"min\": 0.8345857091907786}}, \"EndTime\": 1575871893.455346, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455338}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344269575666723, \"sum\": 0.8344269575666723, \"min\": 0.8344269575666723}}, \"EndTime\": 1575871893.455373, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455367}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8345855517646737, \"sum\": 0.8345855517646737, \"min\": 0.8345855517646737}}, \"EndTime\": 1575871893.455396, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455391}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8349631635022964, \"sum\": 0.8349631635022964, \"min\": 0.8349631635022964}}, \"EndTime\": 1575871893.455418, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455413}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8371118007463786, \"sum\": 0.8371118007463786, \"min\": 0.8371118007463786}}, \"EndTime\": 1575871893.455439, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455434}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.834961144922666, \"sum\": 0.834961144922666, \"min\": 0.834961144922666}}, \"EndTime\": 1575871893.45546, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455455}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8370925609395019, \"sum\": 0.8370925609395019, \"min\": 0.8370925609395019}}, \"EndTime\": 1575871893.455484, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455478}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344321117025897, \"sum\": 0.8344321117025897, \"min\": 0.8344321117025897}}, \"EndTime\": 1575871893.455511, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455505}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8345875953476297, \"sum\": 0.8345875953476297, \"min\": 0.8345875953476297}}, \"EndTime\": 1575871893.455537, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455531}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344294512700318, \"sum\": 0.8344294512700318, \"min\": 0.8344294512700318}}, \"EndTime\": 1575871893.455562, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455556}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8345861292224341, \"sum\": 0.8345861292224341, \"min\": 0.8345861292224341}}, \"EndTime\": 1575871893.455587, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455581}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8349574728341591, \"sum\": 0.8349574728341591, \"min\": 0.8349574728341591}}, \"EndTime\": 1575871893.455611, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455605}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8370712786091322, \"sum\": 0.8370712786091322, \"min\": 0.8370712786091322}}, \"EndTime\": 1575871893.455636, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.45563}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8349589323604141, \"sum\": 0.8349589323604141, \"min\": 0.8349589323604141}}, \"EndTime\": 1575871893.45566, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455654}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8370707185265643, \"sum\": 0.8370707185265643, \"min\": 0.8370707185265643}}, \"EndTime\": 1575871893.455685, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455679}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357825231515863, \"sum\": 0.8357825231515863, \"min\": 0.8357825231515863}}, \"EndTime\": 1575871893.455709, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455703}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.835913026153519, \"sum\": 0.835913026153519, \"min\": 0.835913026153519}}, \"EndTime\": 1575871893.455733, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455727}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357774396566227, \"sum\": 0.8357774396566227, \"min\": 0.8357774396566227}}, \"EndTime\": 1575871893.455757, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455751}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8359109582029152, \"sum\": 0.8359109582029152, \"min\": 0.8359109582029152}}, \"EndTime\": 1575871893.455781, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455775}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8362102001844152, \"sum\": 0.8362102001844152, \"min\": 0.8362102001844152}}, \"EndTime\": 1575871893.455805, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455799}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8379433842168837, \"sum\": 0.8379433842168837, \"min\": 0.8379433842168837}}, \"EndTime\": 1575871893.455829, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455823}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8362099708863616, \"sum\": 0.8362099708863616, \"min\": 0.8362099708863616}}, \"EndTime\": 1575871893.455853, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455847}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8379243987618369, \"sum\": 0.8379243987618369, \"min\": 0.8379243987618369}}, \"EndTime\": 1575871893.455877, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455871}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8366218019051282, \"sum\": 0.8366218019051282, \"min\": 0.8366218019051282}}, \"EndTime\": 1575871893.455901, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455896}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8367490628697021, \"sum\": 0.8367490628697021, \"min\": 0.8367490628697021}}, \"EndTime\": 1575871893.455925, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.45592}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8366242841561977, \"sum\": 0.8366242841561977, \"min\": 0.8366242841561977}}, \"EndTime\": 1575871893.455949, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455944}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8367496934015114, \"sum\": 0.8367496934015114, \"min\": 0.8367496934015114}}, \"EndTime\": 1575871893.455973, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455968}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8370563309490238, \"sum\": 0.8370563309490238, \"min\": 0.8370563309490238}}, \"EndTime\": 1575871893.455997, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.455992}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8386797449935229, \"sum\": 0.8386797449935229, \"min\": 0.8386797449935229}}, \"EndTime\": 1575871893.45602, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.456015}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8370590073744272, \"sum\": 0.8370590073744272, \"min\": 0.8370590073744272}}, \"EndTime\": 1575871893.456044, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.456038}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8386605403103657, \"sum\": 0.8386605403103657, \"min\": 0.8386605403103657}}, \"EndTime\": 1575871893.456068, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871893.456062}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:11:33 INFO 140494632019776] #quality_metric: host=algo-1, epoch=0, train binary_classification_weighted_cross_entropy_objective <loss>=0.834426225337\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:11:33 INFO 140494632019776] #early_stopping_criteria_metric: host=algo-1, epoch=0, criteria=binary_classification_weighted_cross_entropy_objective, value=0.834426225337\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:11:33 INFO 140494632019776] Epoch 0: Loss improved. Updating best model\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:11:33 INFO 140494632019776] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6062, \"sum\": 6062.0, \"min\": 6062}, \"Total Records Seen\": {\"count\": 1, \"max\": 6061170, \"sum\": 6061170.0, \"min\": 6061170}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1575871893.458378, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575871795.510526}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:11:33 INFO 140494632019776] #throughput_metric: host=algo-1, train throughput=61759.0463109 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 06:13:12.602] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 99142, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.834380998003205, \"sum\": 0.834380998003205, \"min\": 0.834380998003205}}, \"EndTime\": 1575871992.602748, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.602695}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.834491566311645, \"sum\": 0.834491566311645, \"min\": 0.834491566311645}}, \"EndTime\": 1575871992.602798, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.60279}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343843147982879, \"sum\": 0.8343843147982879, \"min\": 0.8343843147982879}}, \"EndTime\": 1575871992.602824, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.602819}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344915667657006, \"sum\": 0.8344915667657006, \"min\": 0.8344915667657006}}, \"EndTime\": 1575871992.602848, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.602843}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.834382134302448, \"sum\": 0.834382134302448, \"min\": 0.834382134302448}}, \"EndTime\": 1575871992.60287, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.602865}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8359201800408965, \"sum\": 0.8359201800408965, \"min\": 0.8359201800408965}}, \"EndTime\": 1575871992.602891, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.602886}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343848265188946, \"sum\": 0.8343848265188946, \"min\": 0.8343848265188946}}, \"EndTime\": 1575871992.602912, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.602907}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8359201806059434, \"sum\": 0.8359201806059434, \"min\": 0.8359201806059434}}, \"EndTime\": 1575871992.602932, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.602927}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343833786870926, \"sum\": 0.8343833786870926, \"min\": 0.8343833786870926}}, \"EndTime\": 1575871992.602953, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.602948}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344935728133976, \"sum\": 0.8344935728133976, \"min\": 0.8344935728133976}}, \"EndTime\": 1575871992.602973, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.602968}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343870051985123, \"sum\": 0.8343870051985123, \"min\": 0.8343870051985123}}, \"EndTime\": 1575871992.602994, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.602989}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344935732775433, \"sum\": 0.8344935732775433, \"min\": 0.8344935732775433}}, \"EndTime\": 1575871992.603014, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.603009}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343854498059952, \"sum\": 0.8343854498059952, \"min\": 0.8343854498059952}}, \"EndTime\": 1575871992.603035, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.60303}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8359104753400614, \"sum\": 0.8359104753400614, \"min\": 0.8359104753400614}}, \"EndTime\": 1575871992.603055, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.603051}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343878939567587, \"sum\": 0.8343878939567587, \"min\": 0.8343878939567587}}, \"EndTime\": 1575871992.603076, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.603071}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8359104756932156, \"sum\": 0.8359104756932156, \"min\": 0.8359104756932156}}, \"EndTime\": 1575871992.603096, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.603091}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357407987256901, \"sum\": 0.8357407987256901, \"min\": 0.8357407987256901}}, \"EndTime\": 1575871992.603116, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.603111}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8358347045474652, \"sum\": 0.8358347045474652, \"min\": 0.8358347045474652}}, \"EndTime\": 1575871992.603136, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.603131}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357437768861428, \"sum\": 0.8357437768861428, \"min\": 0.8357437768861428}}, \"EndTime\": 1575871992.603156, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.603151}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.835834704608006, \"sum\": 0.835834704608006, \"min\": 0.835834704608006}}, \"EndTime\": 1575871992.603176, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.603171}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357421849471988, \"sum\": 0.8357421849471988, \"min\": 0.8357421849471988}}, \"EndTime\": 1575871992.603196, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.603191}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8369815002431316, \"sum\": 0.8369815002431316, \"min\": 0.8369815002431316}}, \"EndTime\": 1575871992.603215, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.603211}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357439974360593, \"sum\": 0.8357439974360593, \"min\": 0.8357439974360593}}, \"EndTime\": 1575871992.603235, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.603231}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8369815006669168, \"sum\": 0.8369815006669168, \"min\": 0.8369815006669168}}, \"EndTime\": 1575871992.603255, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.60325}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365872700016919, \"sum\": 0.8365872700016919, \"min\": 0.8365872700016919}}, \"EndTime\": 1575871992.603275, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.60327}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8366761728880484, \"sum\": 0.8366761728880484, \"min\": 0.8366761728880484}}, \"EndTime\": 1575871992.603295, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.60329}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365893273677564, \"sum\": 0.8365893273677564, \"min\": 0.8365893273677564}}, \"EndTime\": 1575871992.603314, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.60331}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8366761708801138, \"sum\": 0.8366761708801138, \"min\": 0.8366761708801138}}, \"EndTime\": 1575871992.603334, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.60333}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365879341437021, \"sum\": 0.8365879341437021, \"min\": 0.8365879341437021}}, \"EndTime\": 1575871992.603354, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.603349}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8377739366886734, \"sum\": 0.8377739366886734, \"min\": 0.8377739366886734}}, \"EndTime\": 1575871992.603373, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.603369}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365897628070303, \"sum\": 0.8365897628070303, \"min\": 0.8365897628070303}}, \"EndTime\": 1575871992.603393, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.603389}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8377739373344413, \"sum\": 0.8377739373344413, \"min\": 0.8377739373344413}}, \"EndTime\": 1575871992.603413, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871992.603408}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:13:12 INFO 140494632019776] #quality_metric: host=algo-1, epoch=1, train binary_classification_weighted_cross_entropy_objective <loss>=0.834380998003\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:13:12 INFO 140494632019776] #early_stopping_criteria_metric: host=algo-1, epoch=1, criteria=binary_classification_weighted_cross_entropy_objective, value=0.834380998003\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:13:12 INFO 140494632019776] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12112, \"sum\": 12112.0, \"min\": 12112}, \"Total Records Seen\": {\"count\": 1, \"max\": 12110340, \"sum\": 12110340.0, \"min\": 12110340}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1575871992.604851, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575871893.46002}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:13:12 INFO 140494632019776] #throughput_metric: host=algo-1, train throughput=61013.4190034 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 06:14:50.433] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 7, \"duration\": 97823, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.83438078330556, \"sum\": 0.83438078330556, \"min\": 0.83438078330556}}, \"EndTime\": 1575872090.434048, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.433996}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.834442166803139, \"sum\": 0.834442166803139, \"min\": 0.834442166803139}}, \"EndTime\": 1575872090.4341, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434092}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.834384314475404, \"sum\": 0.834384314475404, \"min\": 0.834384314475404}}, \"EndTime\": 1575872090.434128, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434122}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344421667123278, \"sum\": 0.8344421667123278, \"min\": 0.8344421667123278}}, \"EndTime\": 1575872090.434151, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434146}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343807465775108, \"sum\": 0.8343807465775108, \"min\": 0.8343807465775108}}, \"EndTime\": 1575872090.434174, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434169}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8353605555646142, \"sum\": 0.8353605555646142, \"min\": 0.8353605555646142}}, \"EndTime\": 1575872090.434195, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434191}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343843144854941, \"sum\": 0.8343843144854941, \"min\": 0.8343843144854941}}, \"EndTime\": 1575872090.434216, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434212}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8353605554334427, \"sum\": 0.8353605554334427, \"min\": 0.8353605554334427}}, \"EndTime\": 1575872090.434237, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434233}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343833668412877, \"sum\": 0.8343833668412877, \"min\": 0.8343833668412877}}, \"EndTime\": 1575872090.434258, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434253}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344444659689199, \"sum\": 0.8344444659689199, \"min\": 0.8344444659689199}}, \"EndTime\": 1575872090.434279, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434274}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343870083062703, \"sum\": 0.8343870083062703, \"min\": 0.8343870083062703}}, \"EndTime\": 1575872090.434299, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434295}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344444659184692, \"sum\": 0.8344444659184692, \"min\": 0.8344444659184692}}, \"EndTime\": 1575872090.43432, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434316}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343834360393537, \"sum\": 0.8343834360393537, \"min\": 0.8343834360393537}}, \"EndTime\": 1575872090.434341, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434336}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8353555478969277, \"sum\": 0.8353555478969277, \"min\": 0.8353555478969277}}, \"EndTime\": 1575872090.434362, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434357}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343870080237468, \"sum\": 0.8343870080237468, \"min\": 0.8343870080237468}}, \"EndTime\": 1575872090.434383, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434378}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8353555476850351, \"sum\": 0.8353555476850351, \"min\": 0.8353555476850351}}, \"EndTime\": 1575872090.434403, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434398}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357413410496408, \"sum\": 0.8357413410496408, \"min\": 0.8357413410496408}}, \"EndTime\": 1575872090.434423, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434419}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357943289591707, \"sum\": 0.8357943289591707, \"min\": 0.8357943289591707}}, \"EndTime\": 1575872090.434444, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434439}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357437783290304, \"sum\": 0.8357437783290304, \"min\": 0.8357437783290304}}, \"EndTime\": 1575872090.434464, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434459}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357943290398917, \"sum\": 0.8357943290398917, \"min\": 0.8357943290398917}}, \"EndTime\": 1575872090.434485, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.43448}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357413566590617, \"sum\": 0.8357413566590617, \"min\": 0.8357413566590617}}, \"EndTime\": 1575872090.434505, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.4345}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365538895326836, \"sum\": 0.8365538895326836, \"min\": 0.8365538895326836}}, \"EndTime\": 1575872090.434525, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.43452}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357437780364168, \"sum\": 0.8357437780364168, \"min\": 0.8357437780364168}}, \"EndTime\": 1575872090.434545, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434541}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365538893308812, \"sum\": 0.8365538893308812, \"min\": 0.8365538893308812}}, \"EndTime\": 1575872090.434566, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434561}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365871371753079, \"sum\": 0.8365871371753079, \"min\": 0.8365871371753079}}, \"EndTime\": 1575872090.434586, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434581}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8366372988211023, \"sum\": 0.8366372988211023, \"min\": 0.8366372988211023}}, \"EndTime\": 1575872090.434607, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434602}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365893324430885, \"sum\": 0.8365893324430885, \"min\": 0.8365893324430885}}, \"EndTime\": 1575872090.434627, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434622}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8366372988412826, \"sum\": 0.8366372988412826, \"min\": 0.8366372988412826}}, \"EndTime\": 1575872090.434647, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434642}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365871361763857, \"sum\": 0.8365871361763857, \"min\": 0.8365871361763857}}, \"EndTime\": 1575872090.434667, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434663}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8373598695472166, \"sum\": 0.8373598695472166, \"min\": 0.8373598695472166}}, \"EndTime\": 1575872090.434688, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434683}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.836589331989033, \"sum\": 0.836589331989033, \"min\": 0.836589331989033}}, \"EndTime\": 1575872090.434708, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434703}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8373598694564055, \"sum\": 0.8373598694564055, \"min\": 0.8373598694564055}}, \"EndTime\": 1575872090.434728, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575872090.434723}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:14:50 INFO 140494632019776] #quality_metric: host=algo-1, epoch=2, train binary_classification_weighted_cross_entropy_objective <loss>=0.834380783306\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:14:50 INFO 140494632019776] #early_stopping_criteria_metric: host=algo-1, epoch=2, criteria=binary_classification_weighted_cross_entropy_objective, value=0.834380746578\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:14:50 INFO 140494632019776] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 18162, \"sum\": 18162.0, \"min\": 18162}, \"Total Records Seen\": {\"count\": 1, \"max\": 18159510, \"sum\": 18159510.0, \"min\": 18159510}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1575872090.436204, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575871992.610278}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:14:50 INFO 140494632019776] #throughput_metric: host=algo-1, train throughput=61836.0104599 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 06:16:29.291] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 9, \"duration\": 98853, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343807633069354, \"sum\": 0.8343807633069354, \"min\": 0.8343807633069354}}, \"EndTime\": 1575872189.291939, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.291887}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344135619895204, \"sum\": 0.8344135619895204, \"min\": 0.8344135619895204}}, \"EndTime\": 1575872189.291991, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.291983}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343843145258546, \"sum\": 0.8343843145258546, \"min\": 0.8343843145258546}}, \"EndTime\": 1575872189.292016, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292011}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344135621610526, \"sum\": 0.8344135621610526, \"min\": 0.8344135621610526}}, \"EndTime\": 1575872189.29204, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292035}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343807494229256, \"sum\": 0.8343807494229256, \"min\": 0.8343807494229256}}, \"EndTime\": 1575872189.292062, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292057}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8349553628279209, \"sum\": 0.8349553628279209, \"min\": 0.8349553628279209}}, \"EndTime\": 1575872189.292082, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292078}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343843145056744, \"sum\": 0.8343843145056744, \"min\": 0.8343843145056744}}, \"EndTime\": 1575872189.292103, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292098}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8349553631709852, \"sum\": 0.8349553631709852, \"min\": 0.8349553631709852}}, \"EndTime\": 1575872189.292123, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292118}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343834295816748, \"sum\": 0.8343834295816748, \"min\": 0.8343834295816748}}, \"EndTime\": 1575872189.292144, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292139}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344160104588568, \"sum\": 0.8344160104588568, \"min\": 0.8344160104588568}}, \"EndTime\": 1575872189.292164, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292159}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343870080943777, \"sum\": 0.8343870080943777, \"min\": 0.8343870080943777}}, \"EndTime\": 1575872189.292184, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.29218}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344160104285865, \"sum\": 0.8344160104285865, \"min\": 0.8344160104285865}}, \"EndTime\": 1575872189.292205, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.2922}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343834573496943, \"sum\": 0.8343834573496943, \"min\": 0.8343834573496943}}, \"EndTime\": 1575872189.292225, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.29222}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8349532706812012, \"sum\": 0.8349532706812012, \"min\": 0.8349532706812012}}, \"EndTime\": 1575872189.292245, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.29224}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.834387008124648, \"sum\": 0.834387008124648, \"min\": 0.834387008124648}}, \"EndTime\": 1575872189.292265, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.29226}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8349532708123728, \"sum\": 0.8349532708123728, \"min\": 0.8349532708123728}}, \"EndTime\": 1575872189.292285, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292281}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357413530165272, \"sum\": 0.8357413530165272, \"min\": 0.8357413530165272}}, \"EndTime\": 1575872189.292305, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292301}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357693791313001, \"sum\": 0.8357693791313001, \"min\": 0.8357693791313001}}, \"EndTime\": 1575872189.292325, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292321}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357437782382192, \"sum\": 0.8357437782382192, \"min\": 0.8357437782382192}}, \"EndTime\": 1575872189.292345, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292341}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357693791313001, \"sum\": 0.8357693791313001, \"min\": 0.8357693791313001}}, \"EndTime\": 1575872189.292365, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.29236}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357413534201321, \"sum\": 0.8357413534201321, \"min\": 0.8357413534201321}}, \"EndTime\": 1575872189.292384, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.29238}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8362215657800973, \"sum\": 0.8362215657800973, \"min\": 0.8362215657800973}}, \"EndTime\": 1575872189.292404, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292399}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357437782079489, \"sum\": 0.8357437782079489, \"min\": 0.8357437782079489}}, \"EndTime\": 1575872189.292427, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292419}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.836221565840638, \"sum\": 0.836221565840638, \"min\": 0.836221565840638}}, \"EndTime\": 1575872189.292455, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292449}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365871369028746, \"sum\": 0.8365871369028746, \"min\": 0.8365871369028746}}, \"EndTime\": 1575872189.292488, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292479}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8366136168979538, \"sum\": 0.8366136168979538, \"min\": 0.8366136168979538}}, \"EndTime\": 1575872189.292522, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292513}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365893321403848, \"sum\": 0.8365893321403848, \"min\": 0.8365893321403848}}, \"EndTime\": 1575872189.292557, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292548}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8366136168979538, \"sum\": 0.8366136168979538, \"min\": 0.8366136168979538}}, \"EndTime\": 1575872189.292593, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292584}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.836587136943235, \"sum\": 0.836587136943235, \"min\": 0.836587136943235}}, \"EndTime\": 1575872189.292627, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.29262}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8370435709487428, \"sum\": 0.8370435709487428, \"min\": 0.8370435709487428}}, \"EndTime\": 1575872189.292661, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292652}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365893320596638, \"sum\": 0.8365893320596638, \"min\": 0.8365893320596638}}, \"EndTime\": 1575872189.292698, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292688}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8370435707267601, \"sum\": 0.8370435707267601, \"min\": 0.8370435707267601}}, \"EndTime\": 1575872189.292728, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872189.292721}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:16:29 INFO 140494632019776] #quality_metric: host=algo-1, epoch=3, train binary_classification_weighted_cross_entropy_objective <loss>=0.834380763307\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:16:29 INFO 140494632019776] #early_stopping_criteria_metric: host=algo-1, epoch=3, criteria=binary_classification_weighted_cross_entropy_objective, value=0.834380749423\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:16:29 INFO 140494632019776] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 24212, \"sum\": 24212.0, \"min\": 24212}, \"Total Records Seen\": {\"count\": 1, \"max\": 24208680, \"sum\": 24208680.0, \"min\": 24208680}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1575872189.294296, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575872090.437903}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:16:29 INFO 140494632019776] #throughput_metric: host=algo-1, train throughput=61191.4387461 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 06:18:03.052] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 93749, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.834380757787638, \"sum\": 0.834380757787638, \"min\": 0.834380757787638}}, \"EndTime\": 1575872283.052863, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.052807}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343976143981863, \"sum\": 0.8343976143981863, \"min\": 0.8343976143981863}}, \"EndTime\": 1575872283.052914, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.052906}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343843145056744, \"sum\": 0.8343843145056744, \"min\": 0.8343843145056744}}, \"EndTime\": 1575872283.05294, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.052935}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343976143981863, \"sum\": 0.8343976143981863, \"min\": 0.8343976143981863}}, \"EndTime\": 1575872283.052964, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.052959}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343807522683404, \"sum\": 0.8343807522683404, \"min\": 0.8343807522683404}}, \"EndTime\": 1575872283.052986, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.052981}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8347252852316434, \"sum\": 0.8347252852316434, \"min\": 0.8347252852316434}}, \"EndTime\": 1575872283.053008, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053003}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343843145056744, \"sum\": 0.8343843145056744, \"min\": 0.8343843145056744}}, \"EndTime\": 1575872283.053029, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053024}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8347252849491199, \"sum\": 0.8347252849491199, \"min\": 0.8347252849491199}}, \"EndTime\": 1575872283.053049, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053044}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.834383456754377, \"sum\": 0.834383456754377, \"min\": 0.834383456754377}}, \"EndTime\": 1575872283.05307, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053065}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344001945638129, \"sum\": 0.8344001945638129, \"min\": 0.8344001945638129}}, \"EndTime\": 1575872283.053091, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053086}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343870080943777, \"sum\": 0.8343870080943777, \"min\": 0.8343870080943777}}, \"EndTime\": 1575872283.053112, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053107}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8344001945638129, \"sum\": 0.8344001945638129, \"min\": 0.8344001945638129}}, \"EndTime\": 1575872283.053133, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053128}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.834383467883783, \"sum\": 0.834383467883783, \"min\": 0.834383467883783}}, \"EndTime\": 1575872283.053154, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053149}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8347255232072031, \"sum\": 0.8347255232072031, \"min\": 0.8347255232072031}}, \"EndTime\": 1575872283.053174, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.05317}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8343870080943777, \"sum\": 0.8343870080943777, \"min\": 0.8343870080943777}}, \"EndTime\": 1575872283.053195, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.05319}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8347255233989154, \"sum\": 0.8347255233989154, \"min\": 0.8347255233989154}}, \"EndTime\": 1575872283.053215, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053211}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.835741353490763, \"sum\": 0.835741353490763, \"min\": 0.835741353490763}}, \"EndTime\": 1575872283.053236, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053231}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357550536988293, \"sum\": 0.8357550536988293, \"min\": 0.8357550536988293}}, \"EndTime\": 1575872283.053256, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053251}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357437782382192, \"sum\": 0.8357437782382192, \"min\": 0.8357437782382192}}, \"EndTime\": 1575872283.053277, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053272}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357550536988293, \"sum\": 0.8357550536988293, \"min\": 0.8357550536988293}}, \"EndTime\": 1575872283.053297, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053292}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357413535513036, \"sum\": 0.8357413535513036, \"min\": 0.8357413535513036}}, \"EndTime\": 1575872283.053317, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053312}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8360284276411423, \"sum\": 0.8360284276411423, \"min\": 0.8360284276411423}}, \"EndTime\": 1575872283.053337, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053332}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8357437782382192, \"sum\": 0.8357437782382192, \"min\": 0.8357437782382192}}, \"EndTime\": 1575872283.053357, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053353}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8360284274897904, \"sum\": 0.8360284274897904, \"min\": 0.8360284274897904}}, \"EndTime\": 1575872283.053378, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053373}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365871369129647, \"sum\": 0.8365871369129647, \"min\": 0.8365871369129647}}, \"EndTime\": 1575872283.053398, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053393}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8366002657072547, \"sum\": 0.8366002657072547, \"min\": 0.8366002657072547}}, \"EndTime\": 1575872283.053419, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053414}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365893321403848, \"sum\": 0.8365893321403848, \"min\": 0.8365893321403848}}, \"EndTime\": 1575872283.053439, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053434}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8366002657072547, \"sum\": 0.8366002657072547, \"min\": 0.8366002657072547}}, \"EndTime\": 1575872283.05346, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053455}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365871369129647, \"sum\": 0.8365871369129647, \"min\": 0.8365871369129647}}, \"EndTime\": 1575872283.05348, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053475}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8368623560159576, \"sum\": 0.8368623560159576, \"min\": 0.8368623560159576}}, \"EndTime\": 1575872283.053501, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053496}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8365893321403848, \"sum\": 0.8365893321403848, \"min\": 0.8365893321403848}}, \"EndTime\": 1575872283.053521, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053516}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_weighted_cross_entropy_objective\": {\"count\": 1, \"max\": 0.8368623556224427, \"sum\": 0.8368623556224427, \"min\": 0.8368623556224427}}, \"EndTime\": 1575872283.053555, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872283.053537}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:03 INFO 140494632019776] #quality_metric: host=algo-1, epoch=4, train binary_classification_weighted_cross_entropy_objective <loss>=0.834380757788\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:03 INFO 140494632019776] #early_stopping_criteria_metric: host=algo-1, epoch=4, criteria=binary_classification_weighted_cross_entropy_objective, value=0.834380752268\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:03 INFO 140494632019776] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:03 INFO 140494632019776] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 30262, \"sum\": 30262.0, \"min\": 30262}, \"Total Records Seen\": {\"count\": 1, \"max\": 30257850, \"sum\": 30257850.0, \"min\": 30257850}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1575872283.05517, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575872189.303595}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:03 INFO 140494632019776] #throughput_metric: host=algo-1, train throughput=64523.335463 records/second\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:03 WARNING 140494632019776] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:03 WARNING 140494632019776] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[2019-12-09 06:18:03.069] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 13, \"duration\": 4, \"num_examples\": 1, \"num_bytes\": 52000}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 06:18:14.717] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 11645, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m[2019-12-09 06:18:28.735] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 12314, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:28 INFO 140494632019776] #train_score (algo-1) : ('binary_classification_weighted_cross_entropy_objective', 0.8343861437185272)\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:28 INFO 140494632019776] #train_score (algo-1) : ('binary_classification_accuracy', 0.3879024725706171)\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:28 INFO 140494632019776] #train_score (algo-1) : ('binary_f_1.000', 0.5589765566915661)\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:28 INFO 140494632019776] #train_score (algo-1) : ('precision', 0.3879024725706171)\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:28 INFO 140494632019776] #train_score (algo-1) : ('recall', 1.0)\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:28 INFO 140494632019776] #quality_metric: host=algo-1, train binary_classification_weighted_cross_entropy_objective <loss>=0.834386143719\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:28 INFO 140494632019776] #quality_metric: host=algo-1, train binary_classification_accuracy <score>=0.387902472571\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:28 INFO 140494632019776] #quality_metric: host=algo-1, train binary_f_1.000 <score>=0.558976556692\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:28 INFO 140494632019776] #quality_metric: host=algo-1, train precision <score>=0.387902472571\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:28 INFO 140494632019776] #quality_metric: host=algo-1, train recall <score>=1.0\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:28 INFO 140494632019776] Best model found for hyperparameters: {\"lr_scheduler_step\": 10, \"wd\": 0.0001, \"optimizer\": \"adam\", \"lr_scheduler_factor\": 0.99, \"l1\": 0.0, \"learning_rate\": 0.005, \"lr_scheduler_minimum_lr\": 1e-05}\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:28 INFO 140494632019776] Saved checkpoint to \"/tmp/tmpseC95J/mx-mod-0000.params\"\u001b[0m\n",
      "\u001b[34m[12/09/2019 06:18:28 INFO 140494632019776] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 513652.1511077881, \"sum\": 513652.1511077881, \"min\": 513652.1511077881}, \"finalize.time\": {\"count\": 1, \"max\": 25680.9401512146, \"sum\": 25680.9401512146, \"min\": 25680.9401512146}, \"initialize.time\": {\"count\": 1, \"max\": 327.0537853240967, \"sum\": 327.0537853240967, \"min\": 327.0537853240967}, \"check_early_stopping.time\": {\"count\": 5, \"max\": 0.49996376037597656, \"sum\": 1.0619163513183594, \"min\": 0.12493133544921875}, \"setuptime\": {\"count\": 1, \"max\": 23.077011108398438, \"sum\": 23.077011108398438, \"min\": 23.077011108398438}, \"update.time\": {\"count\": 5, \"max\": 99144.74391937256, \"sum\": 487526.03101730347, \"min\": 93751.41787528992}, \"epochs\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1575872308.748645, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\"}, \"StartTime\": 1575871795.175162}\n",
      "\u001b[0m\n",
      "\n",
      "2019-12-09 06:18:37 Uploading - Uploading generated training model\n",
      "2019-12-09 06:18:37 Completed - Training job completed\n",
      "Training seconds: 559\n",
      "Billable seconds: 559\n",
      "---------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# Training a binary classifier with class weights and automated threshold tuning\n",
    "class_weights_hyperparams = {\n",
    "    'feature_dim': 2,\n",
    "    'predictor_type': 'binary_classifier',\n",
    "    'binary_classifier_model_selection_criteria': 'precision_at_target_recall', \n",
    "    'target_recall': 0.9,\n",
    "    'positive_example_weight_mult': 'balanced',\n",
    "    'epochs': 30\n",
    "}\n",
    "class_weights_output_path = 's3://{}/{}/class_weights/output'.format(bucket, prefix)\n",
    "class_weights_predictor = predictor_from_hyperparams(s3_train_path, class_weights_hyperparams, class_weights_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we examined the hinge loss, also known as the SVM (Support Vector Machine) model because we think that our focus of this project is to gain better accuracy, precision and possibly recall values. Therefore, according to the introduction page in the lienar learner notebook, hinge loss was taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-09 07:37:50 Starting - Starting the training job...\n",
      "2019-12-09 07:37:51 Starting - Launching requested ML instances.........\n",
      "2019-12-09 07:39:26 Starting - Preparing the instances for training...\n",
      "2019-12-09 07:40:16 Downloading - Downloading input data\n",
      "2019-12-09 07:40:16 Training - Downloading the training image..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:40:31 INFO 139896882931520] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'loss_insensitivity': u'0.01', u'epochs': u'15', u'feature_dim': u'auto', u'init_bias': u'0.0', u'lr_scheduler_factor': u'auto', u'num_calibration_samples': u'10000000', u'accuracy_top_k': u'3', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'num_point_for_scaler': u'10000', u'_log_level': u'info', u'quantile': u'0.5', u'bias_lr_mult': u'auto', u'lr_scheduler_step': u'auto', u'init_method': u'uniform', u'init_sigma': u'0.01', u'lr_scheduler_minimum_lr': u'auto', u'target_recall': u'0.8', u'num_models': u'auto', u'early_stopping_patience': u'3', u'momentum': u'auto', u'unbias_label': u'auto', u'wd': u'auto', u'optimizer': u'auto', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.001', u'learning_rate': u'auto', u'_kvstore': u'auto', u'normalize_data': u'true', u'binary_classifier_model_selection_criteria': u'accuracy', u'use_lr_scheduler': u'true', u'target_precision': u'0.8', u'unbias_data': u'auto', u'init_scale': u'0.07', u'bias_wd_mult': u'auto', u'f_beta': u'1.0', u'mini_batch_size': u'1000', u'huber_delta': u'1.0', u'num_classes': u'1', u'beta_1': u'auto', u'loss': u'auto', u'beta_2': u'auto', u'_enable_profiler': u'false', u'normalize_label': u'auto', u'_num_gpus': u'auto', u'balance_multiclass_weights': u'false', u'positive_example_weight_mult': u'1.0', u'l1': u'auto', u'margin': u'1.0'}\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:40:31 INFO 139896882931520] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'loss': u'hinge_loss', u'predictor_type': u'binary_classifier', u'binary_classifier_model_selection_criteria': u'recall_at_target_precision', u'epochs': u'30', u'feature_dim': u'2', u'target_recall': u'0.8', u'margin': u'1.0'}\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:40:31 INFO 139896882931520] Final configuration: {u'loss_insensitivity': u'0.01', u'epochs': u'30', u'feature_dim': u'2', u'init_bias': u'0.0', u'lr_scheduler_factor': u'auto', u'num_calibration_samples': u'10000000', u'accuracy_top_k': u'3', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'num_point_for_scaler': u'10000', u'_log_level': u'info', u'quantile': u'0.5', u'bias_lr_mult': u'auto', u'lr_scheduler_step': u'auto', u'init_method': u'uniform', u'init_sigma': u'0.01', u'lr_scheduler_minimum_lr': u'auto', u'target_recall': u'0.8', u'num_models': u'auto', u'early_stopping_patience': u'3', u'momentum': u'auto', u'unbias_label': u'auto', u'wd': u'auto', u'optimizer': u'auto', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.001', u'learning_rate': u'auto', u'_kvstore': u'auto', u'normalize_data': u'true', u'binary_classifier_model_selection_criteria': u'recall_at_target_precision', u'use_lr_scheduler': u'true', u'target_precision': u'0.8', u'unbias_data': u'auto', u'init_scale': u'0.07', u'bias_wd_mult': u'auto', u'f_beta': u'1.0', u'mini_batch_size': u'1000', u'huber_delta': u'1.0', u'num_classes': u'1', u'predictor_type': u'binary_classifier', u'beta_1': u'auto', u'loss': u'hinge_loss', u'beta_2': u'auto', u'_enable_profiler': u'false', u'normalize_label': u'auto', u'_num_gpus': u'auto', u'balance_multiclass_weights': u'false', u'positive_example_weight_mult': u'1.0', u'l1': u'auto', u'margin': u'1.0'}\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:40:31 WARNING 139896882931520] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:40:31 INFO 139896882931520] Using default worker.\u001b[0m\n",
      "\u001b[34m[2019-12-09 07:40:31.331] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 19, \"num_examples\": 1, \"num_bytes\": 52000}\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:40:31 INFO 139896882931520] Create Store: local\u001b[0m\n",
      "\u001b[34m[2019-12-09 07:40:31.358] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 26, \"num_examples\": 11, \"num_bytes\": 572000}\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:40:31 INFO 139896882931520] Scaler algorithm parameters\n",
      " <algorithm.scaler.ScalerAlgorithmStable object at 0x7f3bf46f0150>\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:40:31 INFO 139896882931520] Scaling model computed with parameters:\n",
      " {'stdev_weight': \u001b[0m\n",
      "\u001b[34m[0.4349889  0.45223308]\u001b[0m\n",
      "\u001b[34m<NDArray 2 @cpu(0)>, 'stdev_label': None, 'mean_label': None, 'mean_weight': \u001b[0m\n",
      "\u001b[34m[0.25345454 0.28672728]\u001b[0m\n",
      "\u001b[34m<NDArray 2 @cpu(0)>}\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:40:31 INFO 139896882931520] nvidia-smi took: 0.0252258777618 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:40:31 INFO 139896882931520] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}, \"Total Records Seen\": {\"count\": 1, \"max\": 12000, \"sum\": 12000.0, \"min\": 12000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 11000, \"sum\": 11000.0, \"min\": 11000}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1575877231.474446, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\"}, \"StartTime\": 1575877231.474415}\n",
      "\u001b[0m\n",
      "\n",
      "2019-12-09 07:40:29 Training - Training image download completed. Training in progress.\u001b[34m[2019-12-09 07:42:06.096] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 3, \"duration\": 94621, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3881770194897594, \"sum\": 0.3881770194897594, \"min\": 0.3881770194897594}}, \"EndTime\": 1575877326.096287, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096229}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3889996649826789, \"sum\": 0.3889996649826789, \"min\": 0.3889996649826789}}, \"EndTime\": 1575877326.096341, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096333}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38816471796954916, \"sum\": 0.38816471796954916, \"min\": 0.38816471796954916}}, \"EndTime\": 1575877326.096368, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096362}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3890003362533612, \"sum\": 0.3890003362533612, \"min\": 0.3890003362533612}}, \"EndTime\": 1575877326.096394, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096388}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39290762636850324, \"sum\": 0.39290762636850324, \"min\": 0.39290762636850324}}, \"EndTime\": 1575877326.096416, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096411}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.40965366037992784, \"sum\": 0.40965366037992784, \"min\": 0.40965366037992784}}, \"EndTime\": 1575877326.096438, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096433}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39308715885393797, \"sum\": 0.39308715885393797, \"min\": 0.39308715885393797}}, \"EndTime\": 1575877326.096459, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096454}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.4110972277275056, \"sum\": 0.4110972277275056, \"min\": 0.4110972277275056}}, \"EndTime\": 1575877326.09648, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096475}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38817560522779787, \"sum\": 0.38817560522779787, \"min\": 0.38817560522779787}}, \"EndTime\": 1575877326.096501, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096496}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38905705534925067, \"sum\": 0.38905705534925067, \"min\": 0.38905705534925067}}, \"EndTime\": 1575877326.096522, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096517}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3881550845655257, \"sum\": 0.3881550845655257, \"min\": 0.3881550845655257}}, \"EndTime\": 1575877326.096543, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096538}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3890377652885146, \"sum\": 0.3890377652885146, \"min\": 0.3890377652885146}}, \"EndTime\": 1575877326.096564, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096559}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39269922260442713, \"sum\": 0.39269922260442713, \"min\": 0.39269922260442713}}, \"EndTime\": 1575877326.096584, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096579}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.41007509293306327, \"sum\": 0.41007509293306327, \"min\": 0.41007509293306327}}, \"EndTime\": 1575877326.096605, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.0966}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3927924951196091, \"sum\": 0.3927924951196091, \"min\": 0.3927924951196091}}, \"EndTime\": 1575877326.096626, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096621}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.4097836160380735, \"sum\": 0.4097836160380735, \"min\": 0.4097836160380735}}, \"EndTime\": 1575877326.096646, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096641}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3881843289668392, \"sum\": 0.3881843289668392, \"min\": 0.3881843289668392}}, \"EndTime\": 1575877326.096666, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096662}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38906229355103084, \"sum\": 0.38906229355103084, \"min\": 0.38906229355103084}}, \"EndTime\": 1575877326.096686, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096681}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3881780307875091, \"sum\": 0.3881780307875091, \"min\": 0.3881780307875091}}, \"EndTime\": 1575877326.096706, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096701}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3889430016145408, \"sum\": 0.3889430016145408, \"min\": 0.3889430016145408}}, \"EndTime\": 1575877326.096726, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096721}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3928382777023126, \"sum\": 0.3928382777023126, \"min\": 0.3928382777023126}}, \"EndTime\": 1575877326.096746, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096742}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.4107010197411649, \"sum\": 0.4107010197411649, \"min\": 0.4107010197411649}}, \"EndTime\": 1575877326.096766, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096761}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39320785389271307, \"sum\": 0.39320785389271307, \"min\": 0.39320785389271307}}, \"EndTime\": 1575877326.096786, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096781}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.4092402164549409, \"sum\": 0.4092402164549409, \"min\": 0.4092402164549409}}, \"EndTime\": 1575877326.096806, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096802}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3882024056044743, \"sum\": 0.3882024056044743, \"min\": 0.3882024056044743}}, \"EndTime\": 1575877326.096826, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096821}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3891194421986072, \"sum\": 0.3891194421986072, \"min\": 0.3891194421986072}}, \"EndTime\": 1575877326.096846, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096841}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3881887273172009, \"sum\": 0.3881887273172009, \"min\": 0.3881887273172009}}, \"EndTime\": 1575877326.096866, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096861}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3890410349526313, \"sum\": 0.3890410349526313, \"min\": 0.3890410349526313}}, \"EndTime\": 1575877326.096886, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096882}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39364501233699206, \"sum\": 0.39364501233699206, \"min\": 0.39364501233699206}}, \"EndTime\": 1575877326.096906, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096901}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.41573939030590995, \"sum\": 0.41573939030590995, \"min\": 0.41573939030590995}}, \"EndTime\": 1575877326.096926, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096922}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39447557905603586, \"sum\": 0.39447557905603586, \"min\": 0.39447557905603586}}, \"EndTime\": 1575877326.096946, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096942}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.4174938970998725, \"sum\": 0.4174938970998725, \"min\": 0.4174938970998725}}, \"EndTime\": 1575877326.096966, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877326.096962}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:42:06 INFO 139896882931520] #quality_metric: host=algo-1, epoch=0, train binary_classification_hinge_loss_objective <loss>=0.38817701949\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:42:06 INFO 139896882931520] #early_stopping_criteria_metric: host=algo-1, epoch=0, criteria=binary_classification_hinge_loss_objective, value=0.388155084566\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:42:06 INFO 139896882931520] Epoch 0: Loss improved. Updating best model\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:42:06 INFO 139896882931520] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6062, \"sum\": 6062.0, \"min\": 6062}, \"Total Records Seen\": {\"count\": 1, \"max\": 6061170, \"sum\": 6061170.0, \"min\": 6061170}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1575877326.099274, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575877231.474572}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:42:06 INFO 139896882931520] #throughput_metric: host=algo-1, train throughput=63927.9791623 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 07:43:45.896] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 99794, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38790650864517734, \"sum\": 0.38790650864517734, \"min\": 0.38790650864517734}}, \"EndTime\": 1575877425.896189, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896128}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3885221760075513, \"sum\": 0.3885221760075513, \"min\": 0.3885221760075513}}, \"EndTime\": 1575877425.896247, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896237}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38793278157074484, \"sum\": 0.38793278157074484, \"min\": 0.38793278157074484}}, \"EndTime\": 1575877425.896278, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896272}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.388498218548359, \"sum\": 0.388498218548359, \"min\": 0.388498218548359}}, \"EndTime\": 1575877425.896301, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896296}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879164543486091, \"sum\": 0.3879164543486091, \"min\": 0.3879164543486091}}, \"EndTime\": 1575877425.896322, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896317}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.399650424568175, \"sum\": 0.399650424568175, \"min\": 0.399650424568175}}, \"EndTime\": 1575877425.896352, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896346}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38793492751295733, \"sum\": 0.38793492751295733, \"min\": 0.38793492751295733}}, \"EndTime\": 1575877425.896372, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896367}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39996290917064675, \"sum\": 0.39996290917064675, \"min\": 0.39996290917064675}}, \"EndTime\": 1575877425.896392, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896388}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879065684493385, \"sum\": 0.3879065684493385, \"min\": 0.3879065684493385}}, \"EndTime\": 1575877425.896419, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896412}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3884907093473773, \"sum\": 0.3884907093473773, \"min\": 0.3884907093473773}}, \"EndTime\": 1575877425.896441, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896436}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879324555487687, \"sum\": 0.3879324555487687, \"min\": 0.3879324555487687}}, \"EndTime\": 1575877425.896461, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896456}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38854611123315674, \"sum\": 0.38854611123315674, \"min\": 0.38854611123315674}}, \"EndTime\": 1575877425.89648, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896476}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879148081399366, \"sum\": 0.3879148081399366, \"min\": 0.3879148081399366}}, \"EndTime\": 1575877425.896509, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896503}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.4001664663508108, \"sum\": 0.4001664663508108, \"min\": 0.4001664663508108}}, \"EndTime\": 1575877425.896529, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896524}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38793616315955504, \"sum\": 0.38793616315955504, \"min\": 0.38793616315955504}}, \"EndTime\": 1575877425.896549, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896544}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.4001249682716071, \"sum\": 0.4001249682716071, \"min\": 0.4001249682716071}}, \"EndTime\": 1575877425.896576, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896568}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38790652394684944, \"sum\": 0.38790652394684944, \"min\": 0.38790652394684944}}, \"EndTime\": 1575877425.896597, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896592}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38848633180789505, \"sum\": 0.38848633180789505, \"min\": 0.38848633180789505}}, \"EndTime\": 1575877425.896617, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896612}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38793272597921014, \"sum\": 0.38793272597921014, \"min\": 0.38793272597921014}}, \"EndTime\": 1575877425.896636, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896632}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38848979619625756, \"sum\": 0.38848979619625756, \"min\": 0.38848979619625756}}, \"EndTime\": 1575877425.896665, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896659}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38791605945649593, \"sum\": 0.38791605945649593, \"min\": 0.38791605945649593}}, \"EndTime\": 1575877425.896684, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.89668}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3995371801168549, \"sum\": 0.3995371801168549, \"min\": 0.3995371801168549}}, \"EndTime\": 1575877425.896704, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.8967}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38793569939225975, \"sum\": 0.38793569939225975, \"min\": 0.38793569939225975}}, \"EndTime\": 1575877425.89673, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896721}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39966594102367287, \"sum\": 0.39966594102367287, \"min\": 0.39966594102367287}}, \"EndTime\": 1575877425.896752, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896747}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38790646492971786, \"sum\": 0.38790646492971786, \"min\": 0.38790646492971786}}, \"EndTime\": 1575877425.896771, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896767}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38848226744566183, \"sum\": 0.38848226744566183, \"min\": 0.38848226744566183}}, \"EndTime\": 1575877425.896791, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896786}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38793163331975067, \"sum\": 0.38793163331975067, \"min\": 0.38793163331975067}}, \"EndTime\": 1575877425.896818, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896813}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38847456176475287, \"sum\": 0.38847456176475287, \"min\": 0.38847456176475287}}, \"EndTime\": 1575877425.896839, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896834}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38791472493173423, \"sum\": 0.38791472493173423, \"min\": 0.38791472493173423}}, \"EndTime\": 1575877425.896858, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896853}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.4029337213497001, \"sum\": 0.4029337213497001, \"min\": 0.4029337213497001}}, \"EndTime\": 1575877425.896882, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896873}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38793460214179415, \"sum\": 0.38793460214179415, \"min\": 0.38793460214179415}}, \"EndTime\": 1575877425.896905, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.8969}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.4028249515098941, \"sum\": 0.4028249515098941, \"min\": 0.4028249515098941}}, \"EndTime\": 1575877425.896926, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877425.896921}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:43:45 INFO 139896882931520] #quality_metric: host=algo-1, epoch=1, train binary_classification_hinge_loss_objective <loss>=0.387906508645\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:43:45 INFO 139896882931520] #early_stopping_criteria_metric: host=algo-1, epoch=1, criteria=binary_classification_hinge_loss_objective, value=0.38790646493\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:43:45 INFO 139896882931520] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12112, \"sum\": 12112.0, \"min\": 12112}, \"Total Records Seen\": {\"count\": 1, \"max\": 12110340, \"sum\": 12110340.0, \"min\": 12110340}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1575877425.898453, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575877326.101043}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:43:45 INFO 139896882931520] #throughput_metric: host=algo-1, train throughput=60614.4508297 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 07:45:19.582] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 7, \"duration\": 93682, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879067186256897, \"sum\": 0.3879067186256897, \"min\": 0.3879067186256897}}, \"EndTime\": 1575877519.58307, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583017}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.388234000076241, \"sum\": 0.388234000076241, \"min\": 0.388234000076241}}, \"EndTime\": 1575877519.583121, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583112}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879334645005647, \"sum\": 0.3879334645005647, \"min\": 0.3879334645005647}}, \"EndTime\": 1575877519.583148, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583142}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38823215014293266, \"sum\": 0.38823215014293266, \"min\": 0.38823215014293266}}, \"EndTime\": 1575877519.583171, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583166}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38790653580274437, \"sum\": 0.38790653580274437, \"min\": 0.38790653580274437}}, \"EndTime\": 1575877519.583194, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583189}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39457474374416385, \"sum\": 0.39457474374416385, \"min\": 0.39457474374416385}}, \"EndTime\": 1575877519.583215, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.58321}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879319683118465, \"sum\": 0.3879319683118465, \"min\": 0.3879319683118465}}, \"EndTime\": 1575877519.583236, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583232}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39447060620432, \"sum\": 0.39447060620432, \"min\": 0.39447060620432}}, \"EndTime\": 1575877519.583257, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583252}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879067134645916, \"sum\": 0.3879067134645916, \"min\": 0.3879067134645916}}, \"EndTime\": 1575877519.583286, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583281}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38824099999842593, \"sum\": 0.38824099999842593, \"min\": 0.38824099999842593}}, \"EndTime\": 1575877519.583307, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583302}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879331017454945, \"sum\": 0.3879331017454945, \"min\": 0.3879331017454945}}, \"EndTime\": 1575877519.583328, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583323}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38824255807996094, \"sum\": 0.38824255807996094, \"min\": 0.38824255807996094}}, \"EndTime\": 1575877519.583349, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583344}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38790659079391676, \"sum\": 0.38790659079391676, \"min\": 0.38790659079391676}}, \"EndTime\": 1575877519.583369, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583364}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39436186289144837, \"sum\": 0.39436186289144837, \"min\": 0.39436186289144837}}, \"EndTime\": 1575877519.58339, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583385}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879341309986187, \"sum\": 0.3879341309986187, \"min\": 0.3879341309986187}}, \"EndTime\": 1575877519.583411, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583406}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3945537144387728, \"sum\": 0.3945537144387728, \"min\": 0.3945537144387728}}, \"EndTime\": 1575877519.583431, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583426}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879065842454267, \"sum\": 0.3879065842454267, \"min\": 0.3879065842454267}}, \"EndTime\": 1575877519.583451, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583446}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3882350594433384, \"sum\": 0.3882350594433384, \"min\": 0.3882350594433384}}, \"EndTime\": 1575877519.583472, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583467}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879328881123575, \"sum\": 0.3879328881123575, \"min\": 0.3879328881123575}}, \"EndTime\": 1575877519.583492, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583487}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38823982473112645, \"sum\": 0.38823982473112645, \"min\": 0.38823982473112645}}, \"EndTime\": 1575877519.583512, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583507}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879067634157473, \"sum\": 0.3879067634157473, \"min\": 0.3879067634157473}}, \"EndTime\": 1575877519.583532, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583527}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39430956346130625, \"sum\": 0.39430956346130625, \"min\": 0.39430956346130625}}, \"EndTime\": 1575877519.583552, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583547}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38793444188540976, \"sum\": 0.38793444188540976, \"min\": 0.38793444188540976}}, \"EndTime\": 1575877519.583573, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583568}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39408397318000815, \"sum\": 0.39408397318000815, \"min\": 0.39408397318000815}}, \"EndTime\": 1575877519.583593, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583588}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38790660327035426, \"sum\": 0.38790660327035426, \"min\": 0.38790660327035426}}, \"EndTime\": 1575877519.583614, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583609}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38821593498675205, \"sum\": 0.38821593498675205, \"min\": 0.38821593498675205}}, \"EndTime\": 1575877519.583634, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583629}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879325268506255, \"sum\": 0.3879325268506255, \"min\": 0.3879325268506255}}, \"EndTime\": 1575877519.583655, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.58365}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38820845736281145, \"sum\": 0.38820845736281145, \"min\": 0.38820845736281145}}, \"EndTime\": 1575877519.583676, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583671}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879066391962385, \"sum\": 0.3879066391962385, \"min\": 0.3879066391962385}}, \"EndTime\": 1575877519.583696, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583691}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.395497844527666, \"sum\": 0.395497844527666, \"min\": 0.395497844527666}}, \"EndTime\": 1575877519.583716, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583712}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879331813363876, \"sum\": 0.3879331813363876, \"min\": 0.3879331813363876}}, \"EndTime\": 1575877519.583737, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583732}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39559561037154095, \"sum\": 0.39559561037154095, \"min\": 0.39559561037154095}}, \"EndTime\": 1575877519.583757, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877519.583752}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:45:19 INFO 139896882931520] #quality_metric: host=algo-1, epoch=2, train binary_classification_hinge_loss_objective <loss>=0.387906718626\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:45:19 INFO 139896882931520] #early_stopping_criteria_metric: host=algo-1, epoch=2, criteria=binary_classification_hinge_loss_objective, value=0.387906535803\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:45:19 INFO 139896882931520] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 18162, \"sum\": 18162.0, \"min\": 18162}, \"Total Records Seen\": {\"count\": 1, \"max\": 18159510, \"sum\": 18159510.0, \"min\": 18159510}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1575877519.585225, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575877425.900207}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:45:19 INFO 139896882931520] #throughput_metric: host=algo-1, train throughput=64569.1811724 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 07:46:50.832] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 9, \"duration\": 91245, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879065175396211, \"sum\": 0.3879065175396211, \"min\": 0.3879065175396211}}, \"EndTime\": 1575877610.832882, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.832821}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38808734845744614, \"sum\": 0.38808734845744614, \"min\": 0.38808734845744614}}, \"EndTime\": 1575877610.832941, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.832932}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879332619665634, \"sum\": 0.3879332619665634, \"min\": 0.3879332619665634}}, \"EndTime\": 1575877610.832974, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.832967}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.388083431643087, \"sum\": 0.388083431643087, \"min\": 0.388083431643087}}, \"EndTime\": 1575877610.833002, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.832995}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879065207482803, \"sum\": 0.3879065207482803, \"min\": 0.3879065207482803}}, \"EndTime\": 1575877610.833028, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833022}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3913631548712048, \"sum\": 0.3913631548712048, \"min\": 0.3913631548712048}}, \"EndTime\": 1575877610.833054, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833048}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38793241527908634, \"sum\": 0.38793241527908634, \"min\": 0.38793241527908634}}, \"EndTime\": 1575877610.83308, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833073}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3913734131515233, \"sum\": 0.3913734131515233, \"min\": 0.3913734131515233}}, \"EndTime\": 1575877610.833106, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.8331}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38790665636458344, \"sum\": 0.38790665636458344, \"min\": 0.38790665636458344}}, \"EndTime\": 1575877610.833132, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833126}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3880784379653565, \"sum\": 0.3880784379653565, \"min\": 0.3880784379653565}}, \"EndTime\": 1575877610.833158, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833151}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879330959941242, \"sum\": 0.3879330959941242, \"min\": 0.3879330959941242}}, \"EndTime\": 1575877610.833183, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833177}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38807961991231765, \"sum\": 0.38807961991231765, \"min\": 0.38807961991231765}}, \"EndTime\": 1575877610.833208, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833202}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38790655729470713, \"sum\": 0.38790655729470713, \"min\": 0.38790655729470713}}, \"EndTime\": 1575877610.833235, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833228}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39140008351696015, \"sum\": 0.39140008351696015, \"min\": 0.39140008351696015}}, \"EndTime\": 1575877610.83326, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833254}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879328378383178, \"sum\": 0.3879328378383178, \"min\": 0.3879328378383178}}, \"EndTime\": 1575877610.833285, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833279}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3915730432254024, \"sum\": 0.3915730432254024, \"min\": 0.3915730432254024}}, \"EndTime\": 1575877610.83331, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833304}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38790648128076277, \"sum\": 0.38790648128076277, \"min\": 0.38790648128076277}}, \"EndTime\": 1575877610.833335, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833329}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38808543419960134, \"sum\": 0.38808543419960134, \"min\": 0.38808543419960134}}, \"EndTime\": 1575877610.833361, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833355}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879335309793425, \"sum\": 0.3879335309793425, \"min\": 0.3879335309793425}}, \"EndTime\": 1575877610.833386, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.83338}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3880827565230226, \"sum\": 0.3880827565230226, \"min\": 0.3880827565230226}}, \"EndTime\": 1575877610.833412, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833406}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38790650239434593, \"sum\": 0.38790650239434593, \"min\": 0.38790650239434593}}, \"EndTime\": 1575877610.833437, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833431}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3914467828882728, \"sum\": 0.3914467828882728, \"min\": 0.3914467828882728}}, \"EndTime\": 1575877610.833463, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833456}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38793225203602544, \"sum\": 0.38793225203602544, \"min\": 0.38793225203602544}}, \"EndTime\": 1575877610.833488, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833482}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39148661223260367, \"sum\": 0.39148661223260367, \"min\": 0.39148661223260367}}, \"EndTime\": 1575877610.833513, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833507}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879066357504614, \"sum\": 0.3879066357504614, \"min\": 0.3879066357504614}}, \"EndTime\": 1575877610.833538, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833532}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3880700636725127, \"sum\": 0.3880700636725127, \"min\": 0.3880700636725127}}, \"EndTime\": 1575877610.833563, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833557}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879332120305429, \"sum\": 0.3879332120305429, \"min\": 0.3879332120305429}}, \"EndTime\": 1575877610.833589, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833582}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3880812560863624, \"sum\": 0.3880812560863624, \"min\": 0.3880812560863624}}, \"EndTime\": 1575877610.833614, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833607}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879065834634421, \"sum\": 0.3879065834634421, \"min\": 0.3879065834634421}}, \"EndTime\": 1575877610.833639, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833633}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3918072467084403, \"sum\": 0.3918072467084403, \"min\": 0.3918072467084403}}, \"EndTime\": 1575877610.833664, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833658}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38793249619178566, \"sum\": 0.38793249619178566, \"min\": 0.38793249619178566}}, \"EndTime\": 1575877610.833689, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833683}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.39209955492364923, \"sum\": 0.39209955492364923, \"min\": 0.39209955492364923}}, \"EndTime\": 1575877610.833715, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877610.833708}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:46:50 INFO 139896882931520] #quality_metric: host=algo-1, epoch=3, train binary_classification_hinge_loss_objective <loss>=0.38790651754\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:46:50 INFO 139896882931520] #early_stopping_criteria_metric: host=algo-1, epoch=3, criteria=binary_classification_hinge_loss_objective, value=0.387906481281\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:46:50 INFO 139896882931520] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 24212, \"sum\": 24212.0, \"min\": 24212}, \"Total Records Seen\": {\"count\": 1, \"max\": 24208680, \"sum\": 24208680.0, \"min\": 24208680}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1575877610.835684, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575877519.58704}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:46:50 INFO 139896882931520] #throughput_metric: host=algo-1, train throughput=66293.1966691 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 07:48:27.995] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 97157, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38790647360722397, \"sum\": 0.38790647360722397, \"min\": 0.38790647360722397}}, \"EndTime\": 1575877707.995678, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.995622}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38800100308943436, \"sum\": 0.38800100308943436, \"min\": 0.38800100308943436}}, \"EndTime\": 1575877707.99573, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.995722}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879349802338519, \"sum\": 0.3879349802338519, \"min\": 0.3879349802338519}}, \"EndTime\": 1575877707.995758, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.995752}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38799853884923513, \"sum\": 0.38799853884923513, \"min\": 0.38799853884923513}}, \"EndTime\": 1575877707.995781, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.995776}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879066762118561, \"sum\": 0.3879066762118561, \"min\": 0.3879066762118561}}, \"EndTime\": 1575877707.995804, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.995798}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3898233165264839, \"sum\": 0.3898233165264839, \"min\": 0.3898233165264839}}, \"EndTime\": 1575877707.995825, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.99582}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879342990597135, \"sum\": 0.3879342990597135, \"min\": 0.3879342990597135}}, \"EndTime\": 1575877707.995846, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.995841}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38984501129064625, \"sum\": 0.38984501129064625, \"min\": 0.38984501129064625}}, \"EndTime\": 1575877707.995866, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.995861}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879064196906497, \"sum\": 0.3879064196906497, \"min\": 0.3879064196906497}}, \"EndTime\": 1575877707.995887, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.995882}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879995142816413, \"sum\": 0.3879995142816413, \"min\": 0.3879995142816413}}, \"EndTime\": 1575877707.995908, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.995903}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38793353170078637, \"sum\": 0.38793353170078637, \"min\": 0.38793353170078637}}, \"EndTime\": 1575877707.995928, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.995923}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38800230980077094, \"sum\": 0.38800230980077094, \"min\": 0.38800230980077094}}, \"EndTime\": 1575877707.995949, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.995944}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38790659749880374, \"sum\": 0.38790659749880374, \"min\": 0.38790659749880374}}, \"EndTime\": 1575877707.99597, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.995965}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3898235088644155, \"sum\": 0.3898235088644155, \"min\": 0.3898235088644155}}, \"EndTime\": 1575877707.995991, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.995986}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879353466819073, \"sum\": 0.3879353466819073, \"min\": 0.3879353466819073}}, \"EndTime\": 1575877707.996011, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996006}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3899307033534404, \"sum\": 0.3899307033534404, \"min\": 0.3899307033534404}}, \"EndTime\": 1575877707.996031, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996026}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38790643931593966, \"sum\": 0.38790643931593966, \"min\": 0.38790643931593966}}, \"EndTime\": 1575877707.996051, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996046}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3880023673800599, \"sum\": 0.3880023673800599, \"min\": 0.3880023673800599}}, \"EndTime\": 1575877707.996072, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996067}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38793210367589653, \"sum\": 0.38793210367589653, \"min\": 0.38793210367589653}}, \"EndTime\": 1575877707.996092, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996087}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879997559148707, \"sum\": 0.3879997559148707, \"min\": 0.3879997559148707}}, \"EndTime\": 1575877707.996112, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996107}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38790654624602205, \"sum\": 0.38790654624602205, \"min\": 0.38790654624602205}}, \"EndTime\": 1575877707.996132, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996128}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3898787388195379, \"sum\": 0.3898787388195379, \"min\": 0.3898787388195379}}, \"EndTime\": 1575877707.996153, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996148}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879325283994594, \"sum\": 0.3879325283994594, \"min\": 0.3879325283994594}}, \"EndTime\": 1575877707.996173, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996168}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3897806689198854, \"sum\": 0.3897806689198854, \"min\": 0.3897806689198854}}, \"EndTime\": 1575877707.996193, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996188}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879065051388595, \"sum\": 0.3879065051388595, \"min\": 0.3879065051388595}}, \"EndTime\": 1575877707.996213, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996208}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.387999459618398, \"sum\": 0.387999459618398, \"min\": 0.387999459618398}}, \"EndTime\": 1575877707.996233, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996228}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.387931802975085, \"sum\": 0.387931802975085, \"min\": 0.387931802975085}}, \"EndTime\": 1575877707.996253, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996248}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879988052184019, \"sum\": 0.3879988052184019, \"min\": 0.3879988052184019}}, \"EndTime\": 1575877707.996273, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996268}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879065501811702, \"sum\": 0.3879065501811702, \"min\": 0.3879065501811702}}, \"EndTime\": 1575877707.996293, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996288}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.38983485890112746, \"sum\": 0.38983485890112746, \"min\": 0.38983485890112746}}, \"EndTime\": 1575877707.996313, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996308}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3879320456980476, \"sum\": 0.3879320456980476, \"min\": 0.3879320456980476}}, \"EndTime\": 1575877707.996333, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996329}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_binary_classification_hinge_loss_objective\": {\"count\": 1, \"max\": 0.3899193552691674, \"sum\": 0.3899193552691674, \"min\": 0.3899193552691674}}, \"EndTime\": 1575877707.996353, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877707.996349}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:27 INFO 139896882931520] #quality_metric: host=algo-1, epoch=4, train binary_classification_hinge_loss_objective <loss>=0.387906473607\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:27 INFO 139896882931520] #early_stopping_criteria_metric: host=algo-1, epoch=4, criteria=binary_classification_hinge_loss_objective, value=0.387906419691\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:27 INFO 139896882931520] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:27 INFO 139896882931520] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 30262, \"sum\": 30262.0, \"min\": 30262}, \"Total Records Seen\": {\"count\": 1, \"max\": 30257850, \"sum\": 30257850.0, \"min\": 30257850}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1575877707.997871, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575877610.837674}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:27 INFO 139896882931520] #throughput_metric: host=algo-1, train throughput=62259.6998184 records/second\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:27 WARNING 139896882931520] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:27 WARNING 139896882931520] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[2019-12-09 07:48:28.004] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 13, \"duration\": 4, \"num_examples\": 1, \"num_bytes\": 52000}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 07:48:39.535] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 11528, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\n",
      "2019-12-09 07:48:57 Uploading - Uploading generated training model\u001b[34m[2019-12-09 07:48:52.861] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 11669, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:52 INFO 139896882931520] #train_score (algo-1) : ('binary_classification_hinge_loss_objective', 0.38796461483327843)\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:52 INFO 139896882931520] #train_score (algo-1) : ('binary_classification_accuracy', 0.3879024725706171)\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:52 INFO 139896882931520] #train_score (algo-1) : ('binary_f_1.000', 0.5589765566915661)\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:52 INFO 139896882931520] #train_score (algo-1) : ('precision', 0.3879024725706171)\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:52 INFO 139896882931520] #train_score (algo-1) : ('recall', 1.0)\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:52 INFO 139896882931520] #quality_metric: host=algo-1, train binary_classification_hinge_loss_objective <loss>=0.387964614833\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:52 INFO 139896882931520] #quality_metric: host=algo-1, train binary_classification_accuracy <score>=0.387902472571\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:52 INFO 139896882931520] #quality_metric: host=algo-1, train binary_f_1.000 <score>=0.558976556692\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:52 INFO 139896882931520] #quality_metric: host=algo-1, train precision <score>=0.387902472571\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:52 INFO 139896882931520] #quality_metric: host=algo-1, train recall <score>=1.0\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:52 INFO 139896882931520] Best model found for hyperparameters: {\"lr_scheduler_step\": 10, \"wd\": 0.01, \"optimizer\": \"adam\", \"lr_scheduler_factor\": 0.99, \"l1\": 0.0, \"learning_rate\": 0.005, \"lr_scheduler_minimum_lr\": 0.0001}\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:52 INFO 139896882931520] Saved checkpoint to \"/tmp/tmpIVAhA6/mx-mod-0000.params\"\u001b[0m\n",
      "\u001b[34m[12/09/2019 07:48:52 INFO 139896882931520] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 501627.1331310272, \"sum\": 501627.1331310272, \"min\": 501627.1331310272}, \"finalize.time\": {\"count\": 1, \"max\": 24863.883018493652, \"sum\": 24863.883018493652, \"min\": 24863.883018493652}, \"initialize.time\": {\"count\": 1, \"max\": 155.52806854248047, \"sum\": 155.52806854248047, \"min\": 155.52806854248047}, \"check_early_stopping.time\": {\"count\": 5, \"max\": 0.49805641174316406, \"sum\": 1.0402202606201172, \"min\": 0.12612342834472656}, \"setuptime\": {\"count\": 1, \"max\": 21.425962448120117, \"sum\": 21.425962448120117, \"min\": 21.425962448120117}, \"update.time\": {\"count\": 5, \"max\": 99797.3039150238, \"sum\": 476515.42472839355, \"min\": 91248.54302406311}, \"epochs\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1575877732.872831, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\"}, \"StartTime\": 1575877231.311933}\n",
      "\u001b[0m\n",
      "\n",
      "2019-12-09 07:49:04 Completed - Training job completed\n",
      "Training seconds: 548\n",
      "Billable seconds: 548\n",
      "---------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# Hinge Loss\n",
    "hinge_loss_hyperparams = {\n",
    "    'feature_dim': 2,\n",
    "    'predictor_type': 'binary_classifier',\n",
    "    'loss': 'hinge_loss',\n",
    "    'margin': 1.0,\n",
    "    'binary_classifier_model_selection_criteria': 'recall_at_target_precision', \n",
    "    'target_recall': 0.8,\n",
    "    'epochs': 30\n",
    "}\n",
    "hinge_loss_output_path = 's3://{}/{}/class_weights/output'.format(bucket, prefix)\n",
    "hinge_loss_predictor = predictor_from_hyperparams(s3_train_path, hinge_loss_hyperparams, hinge_loss_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the parameters for these two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic with class weights</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hinge loss</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  Recall  Precision  Accuracy    F1\n",
       "0  Logistic with class weights   1.000      0.388     0.388 0.559\n",
       "1                   Hinge loss   1.000      0.388     0.388 0.559"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the trained models\n",
    "predictors = {'Logistic with class weights': class_weights_predictor, 'Hinge loss': hinge_loss_predictor}\n",
    "metrics = {key: evaluate(predictor, test_features, test_labels, key, False) for key, predictor in predictors.items()}\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "display(pd.DataFrame(list(metrics.values())).loc[:, ['Model', 'Recall', 'Precision', 'Accuracy', 'F1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that these two methods have exactly the same benchmarks. Therefore, lastly we examined Squared Loss in the next step because we do not think that our training data include any extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-09 15:47:04 Starting - Starting the training job...\n",
      "2019-12-09 15:47:05 Starting - Launching requested ML instances......\n",
      "2019-12-09 15:48:11 Starting - Preparing the instances for training...\n",
      "2019-12-09 15:49:00 Downloading - Downloading input data...\n",
      "2019-12-09 15:49:19 Training - Downloading the training image.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:49:35 INFO 140271297824576] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'loss_insensitivity': u'0.01', u'epochs': u'15', u'feature_dim': u'auto', u'init_bias': u'0.0', u'lr_scheduler_factor': u'auto', u'num_calibration_samples': u'10000000', u'accuracy_top_k': u'3', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'num_point_for_scaler': u'10000', u'_log_level': u'info', u'quantile': u'0.5', u'bias_lr_mult': u'auto', u'lr_scheduler_step': u'auto', u'init_method': u'uniform', u'init_sigma': u'0.01', u'lr_scheduler_minimum_lr': u'auto', u'target_recall': u'0.8', u'num_models': u'auto', u'early_stopping_patience': u'3', u'momentum': u'auto', u'unbias_label': u'auto', u'wd': u'auto', u'optimizer': u'auto', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.001', u'learning_rate': u'auto', u'_kvstore': u'auto', u'normalize_data': u'true', u'binary_classifier_model_selection_criteria': u'accuracy', u'use_lr_scheduler': u'true', u'target_precision': u'0.8', u'unbias_data': u'auto', u'init_scale': u'0.07', u'bias_wd_mult': u'auto', u'f_beta': u'1.0', u'mini_batch_size': u'1000', u'huber_delta': u'1.0', u'num_classes': u'1', u'beta_1': u'auto', u'loss': u'auto', u'beta_2': u'auto', u'_enable_profiler': u'false', u'normalize_label': u'auto', u'_num_gpus': u'auto', u'balance_multiclass_weights': u'false', u'positive_example_weight_mult': u'1.0', u'l1': u'auto', u'margin': u'1.0'}\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:49:35 INFO 140271297824576] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'loss': u'squared_loss', u'feature_dim': u'2', u'predictor_type': u'regressor'}\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:49:35 INFO 140271297824576] Final configuration: {u'loss_insensitivity': u'0.01', u'epochs': u'15', u'feature_dim': u'2', u'init_bias': u'0.0', u'lr_scheduler_factor': u'auto', u'num_calibration_samples': u'10000000', u'accuracy_top_k': u'3', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'num_point_for_scaler': u'10000', u'_log_level': u'info', u'quantile': u'0.5', u'bias_lr_mult': u'auto', u'lr_scheduler_step': u'auto', u'init_method': u'uniform', u'init_sigma': u'0.01', u'lr_scheduler_minimum_lr': u'auto', u'target_recall': u'0.8', u'num_models': u'auto', u'early_stopping_patience': u'3', u'momentum': u'auto', u'unbias_label': u'auto', u'wd': u'auto', u'optimizer': u'auto', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.001', u'learning_rate': u'auto', u'_kvstore': u'auto', u'normalize_data': u'true', u'binary_classifier_model_selection_criteria': u'accuracy', u'use_lr_scheduler': u'true', u'target_precision': u'0.8', u'unbias_data': u'auto', u'init_scale': u'0.07', u'bias_wd_mult': u'auto', u'f_beta': u'1.0', u'mini_batch_size': u'1000', u'huber_delta': u'1.0', u'num_classes': u'1', u'predictor_type': u'regressor', u'beta_1': u'auto', u'loss': u'squared_loss', u'beta_2': u'auto', u'_enable_profiler': u'false', u'normalize_label': u'auto', u'_num_gpus': u'auto', u'balance_multiclass_weights': u'false', u'positive_example_weight_mult': u'1.0', u'l1': u'auto', u'margin': u'1.0'}\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:49:35 WARNING 140271297824576] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:49:35 INFO 140271297824576] Using default worker.\u001b[0m\n",
      "\u001b[34m[2019-12-09 15:49:35.222] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 22, \"num_examples\": 1, \"num_bytes\": 52000}\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:49:35 INFO 140271297824576] Create Store: local\u001b[0m\n",
      "\u001b[34m[2019-12-09 15:49:35.251] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 28, \"num_examples\": 11, \"num_bytes\": 572000}\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:49:35 INFO 140271297824576] Scaler algorithm parameters\n",
      " <algorithm.scaler.ScalerAlgorithmStable object at 0x7f932192e190>\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:49:35 INFO 140271297824576] Scaling model computed with parameters:\n",
      " {'stdev_weight': \u001b[0m\n",
      "\u001b[34m[0.4349889  0.45223308]\u001b[0m\n",
      "\u001b[34m<NDArray 2 @cpu(0)>, 'stdev_label': \u001b[0m\n",
      "\u001b[34m[0.48877245]\u001b[0m\n",
      "\u001b[34m<NDArray 1 @cpu(0)>, 'mean_label': \u001b[0m\n",
      "\u001b[34m[0.39463636]\u001b[0m\n",
      "\u001b[34m<NDArray 1 @cpu(0)>, 'mean_weight': \u001b[0m\n",
      "\u001b[34m[0.25345454 0.28672728]\u001b[0m\n",
      "\u001b[34m<NDArray 2 @cpu(0)>}\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:49:35 INFO 140271297824576] nvidia-smi took: 0.0251929759979 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:49:35 INFO 140271297824576] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}, \"Total Records Seen\": {\"count\": 1, \"max\": 12000, \"sum\": 12000.0, \"min\": 12000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 11000, \"sum\": 11000.0, \"min\": 11000}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1575906575.330126, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\"}, \"StartTime\": 1575906575.330095}\n",
      "\u001b[0m\n",
      "\n",
      "2019-12-09 15:49:35 Training - Training image download completed. Training in progress.\u001b[34m[2019-12-09 15:50:34.729] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 3, \"duration\": 59398, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9905487607431561, \"sum\": 0.9905487607431561, \"min\": 0.9905487607431561}}, \"EndTime\": 1575906634.729373, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729326}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9910409710367448, \"sum\": 0.9910409710367448, \"min\": 0.9910409710367448}}, \"EndTime\": 1575906634.729425, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729417}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9905510789489999, \"sum\": 0.9905510789489999, \"min\": 0.9905510789489999}}, \"EndTime\": 1575906634.729451, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729446}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9910438876373548, \"sum\": 0.9910438876373548, \"min\": 0.9910438876373548}}, \"EndTime\": 1575906634.729474, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729469}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9924072685575934, \"sum\": 0.9924072685575934, \"min\": 0.9924072685575934}}, \"EndTime\": 1575906634.729496, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729491}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9978936736702347, \"sum\": 0.9978936736702347, \"min\": 0.9978936736702347}}, \"EndTime\": 1575906634.729517, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729512}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9924117819605869, \"sum\": 0.9924117819605869, \"min\": 0.9924117819605869}}, \"EndTime\": 1575906634.729537, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729533}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9978354806696645, \"sum\": 0.9978354806696645, \"min\": 0.9978354806696645}}, \"EndTime\": 1575906634.729558, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729553}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9905568535467834, \"sum\": 0.9905568535467834, \"min\": 0.9905568535467834}}, \"EndTime\": 1575906634.729578, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729573}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.99104405327682, \"sum\": 0.99104405327682, \"min\": 0.99104405327682}}, \"EndTime\": 1575906634.729598, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729593}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9905546268583586, \"sum\": 0.9905546268583586, \"min\": 0.9905546268583586}}, \"EndTime\": 1575906634.729618, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729613}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9910411154768608, \"sum\": 0.9910411154768608, \"min\": 0.9910411154768608}}, \"EndTime\": 1575906634.729638, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729633}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9924002929828761, \"sum\": 0.9924002929828761, \"min\": 0.9924002929828761}}, \"EndTime\": 1575906634.729658, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729653}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9978076152602864, \"sum\": 0.9978076152602864, \"min\": 0.9978076152602864}}, \"EndTime\": 1575906634.729678, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729673}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9924925616417745, \"sum\": 0.9924925616417745, \"min\": 0.9924925616417745}}, \"EndTime\": 1575906634.729701, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729693}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9978196986168943, \"sum\": 0.9978196986168943, \"min\": 0.9978196986168943}}, \"EndTime\": 1575906634.729727, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729722}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.991486353975896, \"sum\": 0.991486353975896, \"min\": 0.991486353975896}}, \"EndTime\": 1575906634.72976, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729752}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9919162998032345, \"sum\": 0.9919162998032345, \"min\": 0.9919162998032345}}, \"EndTime\": 1575906634.729794, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729785}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9914668497068859, \"sum\": 0.9914668497068859, \"min\": 0.9914668497068859}}, \"EndTime\": 1575906634.72983, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729821}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9919081178029168, \"sum\": 0.9919081178029168, \"min\": 0.9919081178029168}}, \"EndTime\": 1575906634.729866, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729857}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9931091916422782, \"sum\": 0.9931091916422782, \"min\": 0.9931091916422782}}, \"EndTime\": 1575906634.729903, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729893}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9978456969397543, \"sum\": 0.9978456969397543, \"min\": 0.9978456969397543}}, \"EndTime\": 1575906634.729934, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729928}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9931078741244195, \"sum\": 0.9931078741244195, \"min\": 0.9931078741244195}}, \"EndTime\": 1575906634.729969, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.72996}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9977884253187995, \"sum\": 0.9977884253187995, \"min\": 0.9977884253187995}}, \"EndTime\": 1575906634.730005, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.729997}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9939126492470508, \"sum\": 0.9939126492470508, \"min\": 0.9939126492470508}}, \"EndTime\": 1575906634.730035, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.730029}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9943114873653089, \"sum\": 0.9943114873653089, \"min\": 0.9943114873653089}}, \"EndTime\": 1575906634.73007, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.730061}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9939227055584149, \"sum\": 0.9939227055584149, \"min\": 0.9939227055584149}}, \"EndTime\": 1575906634.730101, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.730094}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9943129336735024, \"sum\": 0.9943129336735024, \"min\": 0.9943129336735024}}, \"EndTime\": 1575906634.730135, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.730126}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9954833745339977, \"sum\": 0.9954833745339977, \"min\": 0.9954833745339977}}, \"EndTime\": 1575906634.730171, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.730162}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9995271407972586, \"sum\": 0.9995271407972586, \"min\": 0.9995271407972586}}, \"EndTime\": 1575906634.7302, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.730194}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9954902610734664, \"sum\": 0.9954902610734664, \"min\": 0.9954902610734664}}, \"EndTime\": 1575906634.730236, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.730227}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9994713410730499, \"sum\": 0.9994713410730499, \"min\": 0.9994713410730499}}, \"EndTime\": 1575906634.730264, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906634.730258}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:50:34 INFO 140271297824576] #quality_metric: host=algo-1, epoch=0, train mse_objective <loss>=0.990548760743\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:50:34 INFO 140271297824576] #early_stopping_criteria_metric: host=algo-1, epoch=0, criteria=mse_objective, value=0.990548760743\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:50:34 INFO 140271297824576] Epoch 0: Loss improved. Updating best model\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:50:34 INFO 140271297824576] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6062, \"sum\": 6062.0, \"min\": 6062}, \"Total Records Seen\": {\"count\": 1, \"max\": 6061170, \"sum\": 6061170.0, \"min\": 6061170}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1575906634.732638, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 0}, \"StartTime\": 1575906575.330253}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:50:34 INFO 140271297824576] #throughput_metric: host=algo-1, train throughput=101833.668214 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 15:51:33.699] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 58965, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904149438924564, \"sum\": 0.9904149438924564, \"min\": 0.9904149438924564}}, \"EndTime\": 1575906693.699543, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699491}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9907636696854433, \"sum\": 0.9907636696854433, \"min\": 0.9907636696854433}}, \"EndTime\": 1575906693.699593, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699585}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904293970759791, \"sum\": 0.9904293970759791, \"min\": 0.9904293970759791}}, \"EndTime\": 1575906693.69962, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699614}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9907636688782333, \"sum\": 0.9907636688782333, \"min\": 0.9907636688782333}}, \"EndTime\": 1575906693.699644, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699638}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904203403729568, \"sum\": 0.9904203403729568, \"min\": 0.9904203403729568}}, \"EndTime\": 1575906693.699665, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.69966}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9949877375033418, \"sum\": 0.9949877375033418, \"min\": 0.9949877375033418}}, \"EndTime\": 1575906693.699687, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699682}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.990431006672639, \"sum\": 0.990431006672639, \"min\": 0.990431006672639}}, \"EndTime\": 1575906693.699708, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699703}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.994987737927127, \"sum\": 0.994987737927127, \"min\": 0.994987737927127}}, \"EndTime\": 1575906693.699729, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699724}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904151896273209, \"sum\": 0.9904151896273209, \"min\": 0.9904151896273209}}, \"EndTime\": 1575906693.69975, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699745}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.990763399350858, \"sum\": 0.990763399350858, \"min\": 0.990763399350858}}, \"EndTime\": 1575906693.69977, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699766}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904297605928537, \"sum\": 0.9904297605928537, \"min\": 0.9904297605928537}}, \"EndTime\": 1575906693.699791, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699786}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9907633997443728, \"sum\": 0.9907633997443728, \"min\": 0.9907633997443728}}, \"EndTime\": 1575906693.699811, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699807}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904208030555638, \"sum\": 0.9904208030555638, \"min\": 0.9904208030555638}}, \"EndTime\": 1575906693.699832, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699827}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.994975448610921, \"sum\": 0.994975448610921, \"min\": 0.994975448610921}}, \"EndTime\": 1575906693.699852, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699847}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904314260988871, \"sum\": 0.9904314260988871, \"min\": 0.9904314260988871}}, \"EndTime\": 1575906693.699873, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699868}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9949754459370382, \"sum\": 0.9949754459370382, \"min\": 0.9949754459370382}}, \"EndTime\": 1575906693.699894, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699889}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9913483790055369, \"sum\": 0.9913483790055369, \"min\": 0.9913483790055369}}, \"EndTime\": 1575906693.699914, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699909}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9916620218835402, \"sum\": 0.9916620218835402, \"min\": 0.9916620218835402}}, \"EndTime\": 1575906693.699935, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.69993}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.991362848091094, \"sum\": 0.991362848091094, \"min\": 0.991362848091094}}, \"EndTime\": 1575906693.699956, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699951}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9916620219844415, \"sum\": 0.9916620219844415, \"min\": 0.9916620219844415}}, \"EndTime\": 1575906693.699976, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699971}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9913554841071678, \"sum\": 0.9913554841071678, \"min\": 0.9913554841071678}}, \"EndTime\": 1575906693.699997, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.699992}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9952720714183775, \"sum\": 0.9952720714183775, \"min\": 0.9952720714183775}}, \"EndTime\": 1575906693.700017, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.700012}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.991364067088988, \"sum\": 0.991364067088988, \"min\": 0.991364067088988}}, \"EndTime\": 1575906693.700037, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.700032}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9952720709037812, \"sum\": 0.9952720709037812, \"min\": 0.9952720709037812}}, \"EndTime\": 1575906693.700057, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.700052}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938085489902128, \"sum\": 0.9938085489902128, \"min\": 0.9938085489902128}}, \"EndTime\": 1575906693.700078, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.700073}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9940883643865625, \"sum\": 0.9940883643865625, \"min\": 0.9940883643865625}}, \"EndTime\": 1575906693.700098, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.700093}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938194387154966, \"sum\": 0.9938194387154966, \"min\": 0.9938194387154966}}, \"EndTime\": 1575906693.700118, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.700113}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9940883633472798, \"sum\": 0.9940883633472798, \"min\": 0.9940883633472798}}, \"EndTime\": 1575906693.700139, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.700134}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938128617410128, \"sum\": 0.9938128617410128, \"min\": 0.9938128617410128}}, \"EndTime\": 1575906693.700159, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.700154}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9973037718919353, \"sum\": 0.9973037718919353, \"min\": 0.9973037718919353}}, \"EndTime\": 1575906693.700179, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.700175}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938210105851851, \"sum\": 0.9938210105851851, \"min\": 0.9938210105851851}}, \"EndTime\": 1575906693.7002, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.700195}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9973037722854501, \"sum\": 0.9973037722854501, \"min\": 0.9973037722854501}}, \"EndTime\": 1575906693.70022, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906693.700215}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:51:33 INFO 140271297824576] #quality_metric: host=algo-1, epoch=1, train mse_objective <loss>=0.990414943892\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:51:33 INFO 140271297824576] #early_stopping_criteria_metric: host=algo-1, epoch=1, criteria=mse_objective, value=0.990414943892\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:51:33 INFO 140271297824576] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12112, \"sum\": 12112.0, \"min\": 12112}, \"Total Records Seen\": {\"count\": 1, \"max\": 12110340, \"sum\": 12110340.0, \"min\": 12110340}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1575906693.701701, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 1}, \"StartTime\": 1575906634.734366}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:51:33 INFO 140271297824576] #throughput_metric: host=algo-1, train throughput=102584.968844 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 15:52:31.507] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 7, \"duration\": 57803, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904147657715086, \"sum\": 0.9904147657715086, \"min\": 0.9904147657715086}}, \"EndTime\": 1575906751.507323, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507275}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9906075484959581, \"sum\": 0.9906075484959581, \"min\": 0.9906075484959581}}, \"EndTime\": 1575906751.507372, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507364}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904294034226667, \"sum\": 0.9904294034226667, \"min\": 0.9904294034226667}}, \"EndTime\": 1575906751.507398, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507393}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9906075483546964, \"sum\": 0.9906075483546964, \"min\": 0.9906075483546964}}, \"EndTime\": 1575906751.507421, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507416}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904147421908903, \"sum\": 0.9904147421908903, \"min\": 0.9904147421908903}}, \"EndTime\": 1575906751.507443, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507438}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9930463948412993, \"sum\": 0.9930463948412993, \"min\": 0.9930463948412993}}, \"EndTime\": 1575906751.507464, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507459}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.990429403029152, \"sum\": 0.990429403029152, \"min\": 0.990429403029152}}, \"EndTime\": 1575906751.507485, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.50748}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9930463945890462, \"sum\": 0.9930463945890462, \"min\": 0.9930463945890462}}, \"EndTime\": 1575906751.507505, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507501}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.990415137451293, \"sum\": 0.990415137451293, \"min\": 0.990415137451293}}, \"EndTime\": 1575906751.507526, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507521}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.990607587736448, \"sum\": 0.990607587736448, \"min\": 0.990607587736448}}, \"EndTime\": 1575906751.507546, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507542}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904297674137771, \"sum\": 0.9904297674137771, \"min\": 0.9904297674137771}}, \"EndTime\": 1575906751.507566, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507562}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9906075876254565, \"sum\": 0.9906075876254565, \"min\": 0.9906075876254565}}, \"EndTime\": 1575906751.507587, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507582}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904151041942464, \"sum\": 0.9904151041942464, \"min\": 0.9904151041942464}}, \"EndTime\": 1575906751.507607, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507602}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9930407838044156, \"sum\": 0.9930407838044156, \"min\": 0.9930407838044156}}, \"EndTime\": 1575906751.507627, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507623}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904297669799018, \"sum\": 0.9904297669799018, \"min\": 0.9904297669799018}}, \"EndTime\": 1575906751.507647, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507643}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9930407842181107, \"sum\": 0.9930407842181107, \"min\": 0.9930407842181107}}, \"EndTime\": 1575906751.507667, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507663}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9913502174966452, \"sum\": 0.9913502174966452, \"min\": 0.9913502174966452}}, \"EndTime\": 1575906751.507687, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507683}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9915248936157933, \"sum\": 0.9915248936157933, \"min\": 0.9915248936157933}}, \"EndTime\": 1575906751.507707, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507703}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9913628538424643, \"sum\": 0.9913628538424643, \"min\": 0.9913628538424643}}, \"EndTime\": 1575906751.507727, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507723}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.991524893514892, \"sum\": 0.991524893514892, \"min\": 0.991524893514892}}, \"EndTime\": 1575906751.507747, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507742}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9913502504509881, \"sum\": 0.9913502504509881, \"min\": 0.9913502504509881}}, \"EndTime\": 1575906751.507767, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507762}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9936533151766272, \"sum\": 0.9936533151766272, \"min\": 0.9936533151766272}}, \"EndTime\": 1575906751.507787, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507782}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9913628538929149, \"sum\": 0.9913628538929149, \"min\": 0.9913628538929149}}, \"EndTime\": 1575906751.507806, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507802}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9936533147831124, \"sum\": 0.9936533147831124, \"min\": 0.9936533147831124}}, \"EndTime\": 1575906751.507826, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507822}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938085017785255, \"sum\": 0.9938085017785255, \"min\": 0.9938085017785255}}, \"EndTime\": 1575906751.507846, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507841}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9939635101686649, \"sum\": 0.9939635101686649, \"min\": 0.9939635101686649}}, \"EndTime\": 1575906751.507866, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507861}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938194407738817, \"sum\": 0.9938194407738817, \"min\": 0.9938194407738817}}, \"EndTime\": 1575906751.507885, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507881}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9939635102998365, \"sum\": 0.9939635102998365, \"min\": 0.9939635102998365}}, \"EndTime\": 1575906751.507905, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.5079}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938085021014095, \"sum\": 0.9938085021014095, \"min\": 0.9938085021014095}}, \"EndTime\": 1575906751.507924, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.50792}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9958316007447964, \"sum\": 0.9958316007447964, \"min\": 0.9958316007447964}}, \"EndTime\": 1575906751.507944, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507939}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938194409252337, \"sum\": 0.9938194409252337, \"min\": 0.9938194409252337}}, \"EndTime\": 1575906751.507963, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507958}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9958316002100198, \"sum\": 0.9958316002100198, \"min\": 0.9958316002100198}}, \"EndTime\": 1575906751.507982, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906751.507978}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:52:31 INFO 140271297824576] #quality_metric: host=algo-1, epoch=2, train mse_objective <loss>=0.990414765772\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:52:31 INFO 140271297824576] #early_stopping_criteria_metric: host=algo-1, epoch=2, criteria=mse_objective, value=0.990414742191\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:52:31 INFO 140271297824576] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 18162, \"sum\": 18162.0, \"min\": 18162}, \"Total Records Seen\": {\"count\": 1, \"max\": 18159510, \"sum\": 18159510.0, \"min\": 18159510}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1575906751.509432, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 2}, \"StartTime\": 1575906693.703481}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:52:31 INFO 140271297824576] #throughput_metric: host=algo-1, train throughput=104646.004857 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 15:53:28.692] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 9, \"duration\": 57181, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904147508078557, \"sum\": 0.9904147508078557, \"min\": 0.9904147508078557}}, \"EndTime\": 1575906808.693046, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.692997}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9905219491462626, \"sum\": 0.9905219491462626, \"min\": 0.9905219491462626}}, \"EndTime\": 1575906808.693097, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693089}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.990429403200684, \"sum\": 0.990429403200684, \"min\": 0.990429403200684}}, \"EndTime\": 1575906808.693124, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693118}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9905219491462626, \"sum\": 0.9905219491462626, \"min\": 0.9905219491462626}}, \"EndTime\": 1575906808.693148, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693143}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904147489815434, \"sum\": 0.9904147489815434, \"min\": 0.9904147489815434}}, \"EndTime\": 1575906808.69317, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693165}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9920782543352014, \"sum\": 0.9920782543352014, \"min\": 0.9920782543352014}}, \"EndTime\": 1575906808.693191, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693186}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904294031805038, \"sum\": 0.9904294031805038, \"min\": 0.9904294031805038}}, \"EndTime\": 1575906808.693212, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693207}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9920782541031284, \"sum\": 0.9920782541031284, \"min\": 0.9920782541031284}}, \"EndTime\": 1575906808.693232, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693228}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904151455334818, \"sum\": 0.9904151455334818, \"min\": 0.9904151455334818}}, \"EndTime\": 1575906808.693253, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693248}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.990522135248498, \"sum\": 0.990522135248498, \"min\": 0.990522135248498}}, \"EndTime\": 1575906808.693273, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693268}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.990429767322966, \"sum\": 0.990429767322966, \"min\": 0.990429767322966}}, \"EndTime\": 1575906808.693297, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693289}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.990522135248498, \"sum\": 0.990522135248498, \"min\": 0.990522135248498}}, \"EndTime\": 1575906808.693324, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693318}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904151406095015, \"sum\": 0.9904151406095015, \"min\": 0.9904151406095015}}, \"EndTime\": 1575906808.693357, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693348}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9920747476339065, \"sum\": 0.9920747476339065, \"min\": 0.9920747476339065}}, \"EndTime\": 1575906808.69339, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693382}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904297672926956, \"sum\": 0.9904297672926956, \"min\": 0.9904297672926956}}, \"EndTime\": 1575906808.693427, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693418}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9920747474724645, \"sum\": 0.9920747474724645, \"min\": 0.9920747474724645}}, \"EndTime\": 1575906808.693463, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693454}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9913502663227522, \"sum\": 0.9913502663227522, \"min\": 0.9913502663227522}}, \"EndTime\": 1575906808.693495, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693488}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9914479042289079, \"sum\": 0.9914479042289079, \"min\": 0.9914479042289079}}, \"EndTime\": 1575906808.69353, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693521}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9913628535296705, \"sum\": 0.9913628535296705, \"min\": 0.9913628535296705}}, \"EndTime\": 1575906808.693558, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693552}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9914479042289079, \"sum\": 0.9914479042289079, \"min\": 0.9914479042289079}}, \"EndTime\": 1575906808.693593, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693583}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9913502666153657, \"sum\": 0.9913502666153657, \"min\": 0.9913502666153657}}, \"EndTime\": 1575906808.693628, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.69362}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9928290246353995, \"sum\": 0.9928290246353995, \"min\": 0.9928290246353995}}, \"EndTime\": 1575906808.693662, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693657}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9913628535498507, \"sum\": 0.9913628535498507, \"min\": 0.9913628535498507}}, \"EndTime\": 1575906808.693696, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693687}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.992829024595039, \"sum\": 0.992829024595039, \"min\": 0.992829024595039}}, \"EndTime\": 1575906808.693731, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693724}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938085020105983, \"sum\": 0.9938085020105983, \"min\": 0.9938085020105983}}, \"EndTime\": 1575906808.69376, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693755}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938957192611253, \"sum\": 0.9938957192611253, \"min\": 0.9938957192611253}}, \"EndTime\": 1575906808.693795, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693786}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938194405518991, \"sum\": 0.9938194405518991, \"min\": 0.9938194405518991}}, \"EndTime\": 1575906808.69383, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693822}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938957192611253, \"sum\": 0.9938957192611253, \"min\": 0.9938957192611253}}, \"EndTime\": 1575906808.693858, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693853}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938085021316798, \"sum\": 0.9938085021316798, \"min\": 0.9938085021316798}}, \"EndTime\": 1575906808.693892, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693883}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9951205969123254, \"sum\": 0.9951205969123254, \"min\": 0.9951205969123254}}, \"EndTime\": 1575906808.693927, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693919}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938194405922596, \"sum\": 0.9938194405922596, \"min\": 0.9938194405922596}}, \"EndTime\": 1575906808.693956, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.69395}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9951205973462007, \"sum\": 0.9951205973462007, \"min\": 0.9951205973462007}}, \"EndTime\": 1575906808.69399, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906808.693981}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:53:28 INFO 140271297824576] #quality_metric: host=algo-1, epoch=3, train mse_objective <loss>=0.990414750808\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:53:28 INFO 140271297824576] #early_stopping_criteria_metric: host=algo-1, epoch=3, criteria=mse_objective, value=0.990414748982\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:53:28 INFO 140271297824576] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 24212, \"sum\": 24212.0, \"min\": 24212}, \"Total Records Seen\": {\"count\": 1, \"max\": 24208680, \"sum\": 24208680.0, \"min\": 24208680}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1575906808.695542, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 3}, \"StartTime\": 1575906751.511175}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:53:28 INFO 140271297824576] #throughput_metric: host=algo-1, train throughput=105783.472282 records/second\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 15:54:30.321] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 61624, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904147487393804, \"sum\": 0.9904147487393804, \"min\": 0.9904147487393804}}, \"EndTime\": 1575906870.321747, \"Dimensions\": {\"model\": 0, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.3217}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904724186982256, \"sum\": 0.9904724186982256, \"min\": 0.9904724186982256}}, \"EndTime\": 1575906870.321796, \"Dimensions\": {\"model\": 1, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.321789}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904294032814049, \"sum\": 0.9904294032814049, \"min\": 0.9904294032814049}}, \"EndTime\": 1575906870.321822, \"Dimensions\": {\"model\": 2, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.321817}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904724186982256, \"sum\": 0.9904724186982256, \"min\": 0.9904724186982256}}, \"EndTime\": 1575906870.321845, \"Dimensions\": {\"model\": 3, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.32184}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904147484064063, \"sum\": 0.9904147484064063, \"min\": 0.9904147484064063}}, \"EndTime\": 1575906870.321866, \"Dimensions\": {\"model\": 4, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.321861}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9914120477546166, \"sum\": 0.9914120477546166, \"min\": 0.9914120477546166}}, \"EndTime\": 1575906870.321887, \"Dimensions\": {\"model\": 5, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.321882}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904294032814049, \"sum\": 0.9904294032814049, \"min\": 0.9904294032814049}}, \"EndTime\": 1575906870.321907, \"Dimensions\": {\"model\": 6, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.321903}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9914120475225437, \"sum\": 0.9914120475225437, \"min\": 0.9914120475225437}}, \"EndTime\": 1575906870.321928, \"Dimensions\": {\"model\": 7, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.321923}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.990415146451683, \"sum\": 0.990415146451683, \"min\": 0.990415146451683}}, \"EndTime\": 1575906870.321948, \"Dimensions\": {\"model\": 8, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.321943}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904726842904529, \"sum\": 0.9904726842904529, \"min\": 0.9904726842904529}}, \"EndTime\": 1575906870.321968, \"Dimensions\": {\"model\": 9, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.321964}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.990429767322966, \"sum\": 0.990429767322966, \"min\": 0.990429767322966}}, \"EndTime\": 1575906870.321989, \"Dimensions\": {\"model\": 10, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.321984}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904726842904529, \"sum\": 0.9904726842904529, \"min\": 0.9904726842904529}}, \"EndTime\": 1575906870.322009, \"Dimensions\": {\"model\": 11, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322005}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9904151463810521, \"sum\": 0.9904151463810521, \"min\": 0.9904151463810521}}, \"EndTime\": 1575906870.32203, \"Dimensions\": {\"model\": 12, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322025}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9914100201645062, \"sum\": 0.9914100201645062, \"min\": 0.9914100201645062}}, \"EndTime\": 1575906870.32205, \"Dimensions\": {\"model\": 13, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322045}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.990429767322966, \"sum\": 0.990429767322966, \"min\": 0.990429767322966}}, \"EndTime\": 1575906870.322071, \"Dimensions\": {\"model\": 14, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322066}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9914100207699136, \"sum\": 0.9914100207699136, \"min\": 0.9914100207699136}}, \"EndTime\": 1575906870.322091, \"Dimensions\": {\"model\": 15, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322086}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9913502676546485, \"sum\": 0.9913502676546485, \"min\": 0.9913502676546485}}, \"EndTime\": 1575906870.32211, \"Dimensions\": {\"model\": 16, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322106}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9914020847365331, \"sum\": 0.9914020847365331, \"min\": 0.9914020847365331}}, \"EndTime\": 1575906870.32213, \"Dimensions\": {\"model\": 17, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322126}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9913628535296705, \"sum\": 0.9913628535296705, \"min\": 0.9913628535296705}}, \"EndTime\": 1575906870.32215, \"Dimensions\": {\"model\": 18, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322145}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9914020847365331, \"sum\": 0.9914020847365331, \"min\": 0.9914020847365331}}, \"EndTime\": 1575906870.32217, \"Dimensions\": {\"model\": 19, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322166}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9913502679169917, \"sum\": 0.9913502679169917, \"min\": 0.9913502679169917}}, \"EndTime\": 1575906870.32219, \"Dimensions\": {\"model\": 20, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322185}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9922361398533881, \"sum\": 0.9922361398533881, \"min\": 0.9922361398533881}}, \"EndTime\": 1575906870.32221, \"Dimensions\": {\"model\": 21, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322205}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9913628535296705, \"sum\": 0.9913628535296705, \"min\": 0.9913628535296705}}, \"EndTime\": 1575906870.322229, \"Dimensions\": {\"model\": 22, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322225}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.992236139520414, \"sum\": 0.992236139520414, \"min\": 0.992236139520414}}, \"EndTime\": 1575906870.322249, \"Dimensions\": {\"model\": 23, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322244}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938085020005082, \"sum\": 0.9938085020005082, \"min\": 0.9938085020005082}}, \"EndTime\": 1575906870.322269, \"Dimensions\": {\"model\": 24, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322264}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938558048620995, \"sum\": 0.9938558048620995, \"min\": 0.9938558048620995}}, \"EndTime\": 1575906870.322288, \"Dimensions\": {\"model\": 25, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322284}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938194405518991, \"sum\": 0.9938194405518991, \"min\": 0.9938194405518991}}, \"EndTime\": 1575906870.322308, \"Dimensions\": {\"model\": 26, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322303}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938558048620995, \"sum\": 0.9938558048620995, \"min\": 0.9938558048620995}}, \"EndTime\": 1575906870.322327, \"Dimensions\": {\"model\": 27, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322323}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938085020005082, \"sum\": 0.9938085020005082, \"min\": 0.9938085020005082}}, \"EndTime\": 1575906870.322347, \"Dimensions\": {\"model\": 28, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322342}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9946026332538016, \"sum\": 0.9946026332538016, \"min\": 0.9946026332538016}}, \"EndTime\": 1575906870.322366, \"Dimensions\": {\"model\": 29, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322361}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.9938194405518991, \"sum\": 0.9938194405518991, \"min\": 0.9938194405518991}}, \"EndTime\": 1575906870.322386, \"Dimensions\": {\"model\": 30, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322381}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"train_mse_objective\": {\"count\": 1, \"max\": 0.994602632870377, \"sum\": 0.994602632870377, \"min\": 0.994602632870377}}, \"EndTime\": 1575906870.322405, \"Dimensions\": {\"model\": 31, \"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906870.322401}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:30 INFO 140271297824576] #quality_metric: host=algo-1, epoch=4, train mse_objective <loss>=0.990414748739\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:30 INFO 140271297824576] #early_stopping_criteria_metric: host=algo-1, epoch=4, criteria=mse_objective, value=0.990414748406\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:30 INFO 140271297824576] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:30 INFO 140271297824576] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Total Batches Seen\": {\"count\": 1, \"max\": 30262, \"sum\": 30262.0, \"min\": 30262}, \"Total Records Seen\": {\"count\": 1, \"max\": 30257850, \"sum\": 30257850.0, \"min\": 30257850}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 6049170, \"sum\": 6049170.0, \"min\": 6049170}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1575906870.323889, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\", \"epoch\": 4}, \"StartTime\": 1575906808.697334}\n",
      "\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:30 INFO 140271297824576] #throughput_metric: host=algo-1, train throughput=98158.3756849 records/second\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:30 WARNING 140271297824576] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:30 WARNING 140271297824576] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[2019-12-09 15:54:30.329] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 13, \"duration\": 4, \"num_examples\": 1, \"num_bytes\": 52000}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2019-12-09 15:54:41.627] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 15, \"duration\": 11296, \"num_examples\": 6050, \"num_bytes\": 314556840}\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:41 INFO 140271297824576] #train_score (algo-1) : ('mse_objective', 0.23661111631920895)\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:41 INFO 140271297824576] #train_score (algo-1) : ('mse', 0.23661111631920895)\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:41 INFO 140271297824576] #train_score (algo-1) : ('absolute_loss', 0.4735741623044092)\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:41 INFO 140271297824576] #quality_metric: host=algo-1, train mse_objective <loss>=0.236611116319\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:41 INFO 140271297824576] #quality_metric: host=algo-1, train mse <loss>=0.236611116319\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:41 INFO 140271297824576] #quality_metric: host=algo-1, train absolute_loss <loss>=0.473574162304\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:41 INFO 140271297824576] Best model found for hyperparameters: {\"lr_scheduler_step\": 10, \"wd\": 0.0001, \"optimizer\": \"adam\", \"lr_scheduler_factor\": 0.99, \"l1\": 0.0, \"learning_rate\": 0.005, \"lr_scheduler_minimum_lr\": 1e-05}\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:41 INFO 140271297824576] Saved checkpoint to \"/tmp/tmpTvOF0z/mx-mod-0000.params\"\u001b[0m\n",
      "\u001b[34m[12/09/2019 15:54:41 INFO 140271297824576] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 306512.34102249146, \"sum\": 306512.34102249146, \"min\": 306512.34102249146}, \"finalize.time\": {\"count\": 1, \"max\": 11304.15391921997, \"sum\": 11304.15391921997, \"min\": 11304.15391921997}, \"initialize.time\": {\"count\": 1, \"max\": 122.12085723876953, \"sum\": 122.12085723876953, \"min\": 122.12085723876953}, \"check_early_stopping.time\": {\"count\": 5, \"max\": 0.5650520324707031, \"sum\": 1.0678768157958984, \"min\": 0.11801719665527344}, \"setuptime\": {\"count\": 1, \"max\": 23.514986038208008, \"sum\": 23.514986038208008, \"min\": 23.514986038208008}, \"update.time\": {\"count\": 5, \"max\": 61626.43384933472, \"sum\": 294986.0770702362, \"min\": 57184.261083602905}, \"epochs\": {\"count\": 1, \"max\": 15, \"sum\": 15.0, \"min\": 15}}, \"EndTime\": 1575906881.636053, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"Linear Learner\"}, \"StartTime\": 1575906575.199689}\n",
      "\u001b[0m\n",
      "\n",
      "2019-12-09 15:55:24 Uploading - Uploading generated training model\n",
      "2019-12-09 15:55:24 Completed - Training job completed\n",
      "Training seconds: 384\n",
      "Billable seconds: 384\n",
      "---------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# Squared Loss\n",
    "squared_loss_hyperparams = {\n",
    "    'feature_dim': 2,\n",
    "    'predictor_type': 'regressor',\n",
    "    'loss': 'squared_loss',\n",
    "}\n",
    "squared_loss_output_path = 's3://{}/{}/class_weights/output'.format(bucket, prefix)\n",
    "squared_loss_predictor = predictor_from_hyperparams(s3_train_path, squared_loss_hyperparams, squared_loss_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'predicted_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-7533586b833a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredictors_squared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Squared loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msquared_loss_predictor\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictors_squared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.float_format'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'%.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Recall'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Precision'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-153-7533586b833a>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredictors_squared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Squared loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msquared_loss_predictor\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictors_squared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.float_format'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'%.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Recall'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Precision'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-139-ca0176afdfc3>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(linear_predictor, test_features, test_labels, model_name, verbose)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprediction_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlinear_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# parse raw predictions json to exctract predicted label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction_batches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# calculate true positives, false positives, true negatives, false negatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-139-ca0176afdfc3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprediction_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlinear_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# parse raw predictions json to exctract predicted label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction_batches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# calculate true positives, false positives, true negatives, false negatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-139-ca0176afdfc3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprediction_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlinear_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# parse raw predictions json to exctract predicted label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction_batches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# calculate true positives, false positives, true negatives, false negatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predicted_label'"
     ]
    }
   ],
   "source": [
    "# predictors_squared = {'Squared loss': squared_loss_predictor}\n",
    "# metrics = {key: evaluate(predictor, test_features, test_labels, key, False) for key, predictor in predictors_squared.items()}\n",
    "# pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "# display(pd.DataFrame(list(metrics.values())).loc[:, ['Model', 'Recall', 'Precision', 'Accuracy', 'F1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it seems that squared loss also has an error while we are trying to examine its parameters.\n",
    "\n",
    "After examining these 3 models, it can be seen that the results from logistic regression with class weights and hinge loss are the same. This is due to the fact that the focus of our model is to gain better parameters (accuracy, recall, precision, f1), therefore either model will be fine for the purpose of this project alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we believe that either the logistic regression with class weights or the hinge loss method will does the job for predicting whether a taxi driver can overperfom or underperfom in terms of earning tips with regard to the average value. They are the best because as it can be seen from the evaluation part, both of them have a recall of 1.0, meaning that the false negative rate for either one is 0. This is a decent result since 0 means that the model predicts nothing that is supposed to be negative (underperforming) as positive (overperforming) with our current scenario, and that is a job well done.\n",
    "\n",
    "Moreover, the precision value lies around 0.38, meaning that model predicts about 38.8% of the positive observations that is supposed to be as positive, which does leave room for future refinement but is definitely a good start for a project at our level.\n",
    "\n",
    "Starting from here, we believe that these models can be used by taxi drivers to specifically target the optimal time range for working so that they can gain a tip amount that will perform above the average value. This is a good starting point for the rationale for this project since most taxi drivers are contracted to a platform service provider, which would extract commission based on percentages from each driver's payout, leaving them with only bare salaries as the pie gets larger and more and more people start to share it. There is no way a taxi driver can 'pick' any taxi trip that would definitely give them a big paycheck, therefore, any extra cent in the tip amount counts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
